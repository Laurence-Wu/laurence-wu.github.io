<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Blog - Laurence</title><meta name="description" content="Read my latest thoughts on technology, engineering, and various interests."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://laurence-wu.github.io/blog/"><meta property="og:title" content="Blog - Laurence"><meta property="og:description" content="Read my latest thoughts on technology, engineering, and various interests."><meta property="og:image" content="/assets/profile-image.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://laurence-wu.github.io/blog/"><meta property="twitter:title" content="Blog - Laurence"><meta property="twitter:description" content="Read my latest thoughts on technology, engineering, and various interests."><meta property="twitter:image" content="/assets/profile-image.jpg"><!-- KaTeX CSS for Math Rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous"><!-- Google Fonts - Preserved Typography System --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;500;600;700&family=Montserrat:wght@300;400;500;600;700&family=Cormorant+Garamond:wght@300;400;500;600;700&display=swap" rel="stylesheet"><!-- Fira Code for code blocks --><link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap" rel="stylesheet"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Global CSS Import --><link rel="stylesheet" href="/_astro/index.DHDHwiTa.css">
<link rel="stylesheet" href="/_astro/index.BRcr8Vd7.css"></head> <body data-astro-cid-37fxchfa> <nav class="navbar" data-astro-cid-pux6a34n> <div class="navbar-brand" data-astro-cid-pux6a34n>Xiaoyou Wu</div> <a href="/" class="navbar-link " data-astro-cid-pux6a34n>
Home
</a> <a href="/blog" class="navbar-link active" data-astro-cid-pux6a34n>
Blog
</a> <a href="/projects" class="navbar-link " data-astro-cid-pux6a34n>
Projects
</a> <a href="/about" class="navbar-link " data-astro-cid-pux6a34n>
About
</a> </nav>  <script type="module">function r(){const e=document.querySelector(".navbar");window.scrollY>50?e?.classList.add("scrolled"):e?.classList.remove("scrolled")}window.addEventListener("scroll",r);document.querySelector(".navbar")?.addEventListener("contextmenu",e=>{e.preventDefault()});document.querySelector(".navbar")?.addEventListener("dragstart",e=>{e.preventDefault()});</script> <div class="animated-background full" aria-hidden="true" data-astro-cid-5a2bynky></div>  <main class="main-content" data-astro-cid-37fxchfa>   <div class="blog-header" data-astro-cid-5tznm7mj> <div class="container" data-astro-cid-5tznm7mj> <h1 data-astro-cid-5tznm7mj>Blog</h1> <p data-astro-cid-5tznm7mj>My thoughts on technology, engineering, and life</p> </div> </div> <section class="blog-content" data-astro-cid-5tznm7mj> <div class="container" data-astro-cid-5tznm7mj> <div class="blog-sidebar" data-astro-cid-5tznm7mj> <div class="sidebar-section" data-astro-cid-5tznm7mj> <h3 data-astro-cid-5tznm7mj>Search</h3> <div class="search-container" data-astro-cid-mjrxwznw> <div class="search-input-wrapper" data-astro-cid-mjrxwznw> <input type="text" id="search-input" placeholder="Search blogs & projects..." autocomplete="off" data-astro-cid-mjrxwznw> <button id="search-icon" class="search-icon" type="button" aria-label="Search" data-astro-cid-mjrxwznw> <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" data-astro-cid-mjrxwznw> <circle cx="11" cy="11" r="8" data-astro-cid-mjrxwznw></circle> <path d="m21 21-4.35-4.35" data-astro-cid-mjrxwznw></path> </svg> </button> </div> <div id="search-results" class="search-results hidden" data-astro-cid-mjrxwznw> <div class="results-header" data-astro-cid-mjrxwznw> <span id="results-count" data-astro-cid-mjrxwznw>0 results</span> <button id="clear-search" class="clear-btn" data-astro-cid-mjrxwznw>√ó</button> </div> <div id="results-list" class="results-list" data-astro-cid-mjrxwznw></div> </div> </div>  <!-- GLOBAL STYLES TO OVERRIDE ASTRO SCOPING -->  <script type="module">const allBlogPosts = [{"id":"matrix-multiplication.md","data":{"title":"Intro to Triton with Matrix Multiplication","description":"Introduction to GPU programming with Triton and build the matrix multiplication along the way","pubDate":"2025-03-18T00:00:00.000Z","author":"Xiaoyou Wu","tags":["triton","GPU","CUDA","compiler","code","optimization"],"image":"./images/grouped_vs_row_major_ordering.png"},"body":"As the most important operation in GPU computation, matrix multiplication optimization is a must learn!\r\n\r\n**Bruh, let's dive right in ~**\r\n\r\n## From the architecture to the idea\r\n\r\n### Memory Architecture of GPU\r\n\r\n(research1)[https://stanfordasl.github.io/projects/]\r\n\r\n\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph \"CPU / Host\"\r\n        A[Host Code] --> B{Kernel Launch}\r\n    end\r\n\r\n    subgraph \"GPU / Device\"\r\n        C[Command Queue]\r\n        D(Thread Blocks)\r\n        E[Streaming Multiprocessors]\r\n        F[L1 Cache / Shared Memory]\r\n        G[CUDA Cores]\r\n        H[L2 Cache]\r\n        I[High-Bandwidth Memory]\r\n    end\r\n\r\n    %% --- Define Connections ---\r\n    B -- Command --> C\r\n    C --> D\r\n    D --> E\r\n    E -- Contains --> F\r\n    E -- Contains --> G\r\n\r\n    %% Memory Flow\r\n    G <--> F\r\n    F <--> H\r\n    H <--> I\r\n```\r\n\r\n1. Command from CPU: CPU will run a pointer to the compiled GPU function. _But how is it compiled, and what needs to be included? This will be discussed in the compilation part._\r\n2. Then, the command will undergo an asynchronous operation using a physical memory buffer FIFO. Compile instructions will be loaded.\r\n3. Here, the grid of blocks is simply a soft level abstraction. The information about the grid blocks will be compiled for each thread block, which is in the SMs (Streaming Multiprocessor). **_<u>This is incredible, frontend and backend separation techniques, I guess it helps the hardware security.</u>_**\r\n4. **<u>VRAM</u>** is basically a more general external memory, and **<u>HBM</u>** is just a high-performance type.\r\n5. Then, for the SM, it will manage warp, which manages 32 threads for executing and transporting information to other parts:\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph GPU Device\r\n        subgraph Streaming Multiprocessor 1\r\n            subgraph CUDA Core 1A\r\n                R1[Registers]\r\n            end\r\n            subgraph CUDA Core 1B\r\n                R2[Registers]\r\n            end\r\n            L1_1[L1 Cache / Shared Memory]\r\n        end\r\n\r\n        subgraph Streaming Multiprocessor 2\r\n            subgraph CUDA Core 2A\r\n                R3[Registers]\r\n            end\r\n            subgraph CUDA Core 2B\r\n                R4[Registers]\r\n            end\r\n            L1_2[L1 Cache / Shared Memory]\r\n        end\r\n\r\n        L2[L2 Cache]\r\n        HBM[High-Bandwidth Memory]\r\n    end\r\n\r\n    %% --- Data Access Paths ---\r\n    R1 <--> L1_1\r\n    R2 <--> L1_1\r\n    R3 <--> L1_2\r\n    R4 <--> L1_2\r\n    L1_1 <--> L2\r\n    L1_2 <--> L2\r\n    L2 <--> HBM\r\n\r\n    style R1 fill:#f9f,stroke:#333,stroke-width:2px\r\n    style R2 fill:#f9f,stroke:#333,stroke-width:2px\r\n    style R3 fill:#f9f,stroke:#333,stroke-width:2px\r\n    style R4 fill:#f9f,stroke:#333,stroke-width:2px\r\n    style L1_1 fill:#bbf,stroke:#333,stroke-width:2px\r\n    style L1_2 fill:#bbf,stroke:#333,stroke-width:2px\r\n    style L2 fill:#bdf,stroke:#333,stroke-width:2px\r\n    style HBM fill:#fb9,stroke:#333,stroke-width:2px\r\n```\r\n\r\nAND there are their properties:\r\n\r\n| Memory Hardware                        | Key Parameters                                                                                          | Primary Job                                                                                                        |\r\n| -------------------------------------- | ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |\r\n| **Registers**                          | **Size:** Tiniest (bytes per thread)<br />**Speed:** Instantaneous **Scope:** Private to **one thread** | Holds the immediate working data for a single thread's current instruction. The absolute fastest memory available. |\r\n| **L1 Cache / Shared Memory**           | **Size:** Small (~100 KB per SM) <br />**Speed:** Fastest <br />**Scope:** Per SM                       | A high-speed scratchpad for threads within a **block** to share data and cooperate.                                |\r\n| **L2 Cache**                           | **Size:** Medium (a few MB) <br />**Speed:** Fast<br />**Scope:** Shared by **all SMs**                 | A unified cache for the entire GPU, catching data requests that miss L1 to avoid slow HBM access.                  |\r\n| **High-Bandwidth Memory (HBM) / VRAM** | **Size:** Large (many GB) **Speed:** Slowest **Scope:** Global (entire GPU)                             | The main data storage for the GPU, holding all large datasets, textures, and models.                               |\r\n\r\n### Compilation architecture\r\n\r\n```mermaid\r\ngraph TD\r\n    %% --- Node Definitions ---\r\n\r\n    subgraph \"Stage 1: Compilation\"\r\n        A[\"üìÑ Source File .cu\\nContains both Host CPU and Device GPU code\"]\r\n        B[\"1. NVCC Compiler Driver\"]\r\n    end\r\n\r\n    subgraph \"Host CPU Path\"\r\n        C[\"Host Code C++\"]\r\n        D[\"Host Compiler\\ne.g., GCC, Clang, MSVC\"]\r\n        E[\"Host Object File .o\"]\r\n    end\r\n\r\n    subgraph \"Device GPU Path\"\r\n        F[\"Device Code CUDA C++\"]\r\n        G[\"CUDA Frontend Compiler\"]\r\n        H[\"PTX\\nParallel Thread Execution\\n<I>Intermediate Assembly</I>\"]\r\n        I[\"PTX Assembler\"]\r\n        J[\"SASS\\nStreaming Assembler\\n<B>GPU Machine Code</B>\"]\r\n        K[\"Device Object File .o\"]\r\n    end\r\n\r\n    subgraph \"Stage 2: Linking\"\r\n        L[\"Linker\"]\r\n        M[\"‚úÖ Final Executable\\nContains Host code + embedded GPU code\"]\r\n    end\r\n\r\n    subgraph \"Stage 3: Execution\"\r\n        N[\"Program runs on Host CPU\"]\r\n        O[\"CUDA Driver\"]\r\n        P{\"<B>JIT Compilation</B>\\nJust-In-Time\"}\r\n        Q[\"üöÄ GPU Executes Code\"]\r\n    end\r\n\r\n    %% --- Connection Definitions ---\r\n    A --> B\r\n    B -- Separates Code --> C & F\r\n    C --> D --> E\r\n    F --> G --> H --> I --> J --> K\r\n    E --> L\r\n    K --> L\r\n    L --> M\r\n    M --> N --> O --> P\r\n    P -- \"If embedded SASS doesn't match current GPU\" --> I\r\n    P -- \"If SASS is compatible\" --> Q\r\n\r\n    %% --- Styling ---\r\n    style B fill:#f9f,stroke:#333,stroke-width:2px\r\n    style L fill:#f9f,stroke:#333,stroke-width:2px\r\n    style N fill:#f9f,stroke:#333,stroke-width:2px\r\n    style O fill:#f9f,stroke:#333,stroke-width:2px\r\n```\r\n\r\n1. Command from CPU: The command will be compiled in the CPU. The kernel function should include the behavior of the Grid of Blocks and the number of threads per block.\r\n2. **<u>JIT (just in time)</u>** means it compiles the program, not before, but during the GPU's execution.\r\n3. **<u>Bank conflict:</u>** since multiple threads can potentially want to access the same shared memory bank (a column) simultaneously, we would have to serialize them / or arrange the data properly\r\n4. **<u>PTX</u>** provides forward compatibility, and **<u>SASS</u>** provides the maximum performance.\r\n5. **<u>Memory coalescing</u>**: When a warp accesses the GPU's main VRAM, the hardware checks the addresses they are requesting. If these addresses are close together and fall within a single, aligned memory segment, the GPU \"coalesces\" them. Instead of performing 32 small, separate memory fetches, it performs one single, large fetch that grabs all the requested data at once. Shared Memory/registers are fast enough, and L1/L2 also have this feature called locality.\r\n\r\n## Torch Implementation of Matrix Multiplication\r\n\r\n```python\r\ndef matrix_multilication(x,y):\r\n  # consider matrix x and y with dimention M,N,K\r\n  M,N = x.shape\r\n  N,K = y.shape\r\n  accumulator = torch.zeros(M,K)\r\n  for row_x in range(M):\r\n    for col_y in range(K):\r\n      dot_product = torch.dot(x[row_X,:],y[:,col_y])\r\n      accumulator[row_x][col_y] = dot_product\r\n\treturn accumulator\r\n# Basically just use\r\nresult = torch.matmul(x,y)\r\n```\r\n\r\n## High-Level Implementation of Matrix Multiplication\r\n\r\n```python\r\n#Implement the tiling in the Triton\r\ndef matrix_multiplication(x,y,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K):\r\n  M,N = x.shape\r\n  N,K = y.shape\r\n  z = torch.zeros(M,K)\r\n  for m in range(0,M,BLOCK_SIZE_M):\r\n  \tfor k in range(0,K,BLOCK_SIZE_K):\r\n      accumulator = zeros((BLOCK_SIZE_M,BLOCK_SIZE_K),dtype=float32)\r\n      #in the SM So we have to export the ACCU to the L2 first\r\n      for i in range(0,n,BLOCK_SIZE_N):\r\n        a = x[m:m+BLOCK_SIZE_M,n:n+BLOCK_SIZE_N]\r\n        b = y[n:n+BLOCK_SIZE_N,k:k+BLOCK_SIZE_K]\r\n        accumulator += dot(a,b)\r\n      z[m:m+BLOCK_SIZE,k:k+BLOCK_SIZE_K] = accumulator\r\n  return z\r\n```\r\n\r\nHere are some useful insights:\r\n\r\n- Why do we use the accumulator? Why can't we just add to the output matrix? A: Because of the coalescing the access to the information is faster\r\n- Why do we do a small matrix multiplication instead of a faster dot product? A: They are essentially the same because they are just 1D arrays in the memory space, and the difference between these two algorithms won't make much of a difference.\r\n\r\n## Triton Kernel for Matrix Multiplication\r\n\r\n```python\r\n@triton.jit\r\ndef kernel_maxmul(\r\n        # Pointers to matrices\r\n        a_ptr, b_ptr, c_ptr,\r\n        # Matrix dimensions\r\n        M, N, K,\r\n        # The stride variables represent how much to increase the ptr by when moving by 1\r\n        stride_am, stride_ak,\r\n        stride_bk, stride_bn,\r\n        stride_cm, stride_cn,\r\n        # Meta-parameters\r\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  # for tiling\r\n        GROUP_SIZE_M: tl.constexpr,  # for super band\r\n        ACTIVATION: tl.constexpr  )\r\n\r\n    # This is done in a grouped ordering to promote L2 data reuse.(exactly why we use the GROUP_SIZE_M)\r\n    pid = tl.program_id(axis=0)\r\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\r\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\r\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\r\n    group_id = pid // num_pid_in_group\r\n    first_pid_m = group_id * GROUP_SIZE_M\r\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\r\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\r\n    pid_n = (pid % num_pid_in_group) // group_size_m\r\n\r\n    # ------------------------------------------------------\r\n    # This helps to guide integer analysis in the backend to optimize load/store offset address calculation\r\n    tl.assume(pid_m >= 0)\r\n    tl.assume(pid_n >= 0)\r\n    tl.assume(stride_am > 0)\r\n    tl.assume(stride_ak > 0)\r\n    tl.assume(stride_bn > 0)\r\n    tl.assume(stride_bk > 0)\r\n    tl.assume(stride_cm > 0)\r\n    tl.assume(stride_cn > 0)\r\n\r\n    # ----------------------------------------------------------\r\n    # Create pointers for the first blocks of A and B.\r\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\r\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\r\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\r\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\r\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\r\n\r\n    # -----------------------------------------------------------\r\n    # Iterate to compute a block of the C matrix.\r\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\r\n    # of fp32 values for higher accuracy.\r\n    # `accumulator` will be converted back to fp16 after the loop.\r\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\r\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\r\n        # Load the next block of A and B, generate a mask by checking the K dimension.\r\n        # If it is out of bounds, set it to 0.\r\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\r\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\r\n        # We accumulate along the K dimension.\r\n        accumulator = tl.dot(a, b, accumulator)\r\n        # Advance the ptrs to the next K block.\r\n        a_ptrs += BLOCK_SIZE_K * stride_ak\r\n        b_ptrs += BLOCK_SIZE_K * stride_bk\r\n    # You can fuse arbitrary activation functions here\r\n    # while the accumulator is still in FP32!\r\n    if ACTIVATION == \"leaky_relu\":\r\n        accumulator = leaky_relu(accumulator)\r\n    c = accumulator.to(tl.float16)\r\n\r\n    # -----------------------------------------------------------\r\n    # Write back the block of the output matrix C with masks.\r\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\r\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\r\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\r\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\r\n    tl.store(c_ptrs, c, mask=c_mask)\r\n\r\n```\r\n\r\n- We used a novel parameter: GROUP_SIZE_M, which was meant to be the height of a super-block with several BLOCK_SIZE_M tall. The following picture shows this idea better. Let me use the quote from the original tutorial <u>_\"In the following matmul where each matrix is 9 blocks by 9 blocks, we can see that if we compute the output in row-major ordering, we need to load 90 blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped ordering, we only need to load 54 blocks.\"_</u>\r\n\r\n## ![grouped_vs_row_major_ordering](./images/grouped_vs_row_major_ordering.png)\r\n\r\n## Conclusion\r\n\r\nGPU performance is dictated by its memory hierarchy: extremely fast but small on-chip memory (Registers, L1 Cache/Shared Memory) and large but slow off-chip memory (VRAM/HBM). The primary optimization goal is to minimize traffic to slow VRAM by maximizing data reuse in the fast caches.\r\n\r\nThe key software strategy is **tiling** (or blocking), where large matrices are broken into smaller blocks that fit into fast on-chip memory. Computations are performed on these blocks using a local **accumulator** to sum results, ensuring only one final, slow write to VRAM per block. This algorithm must also account for hardware features, ensuring memory access is **coalesced** to maximize VRAM bandwidth and that it avoids **bank conflicts** in shared memory.\r\n\r\nBlock Tiling also works ! And it fosters the calculation speed.","filePath":"src/content/blogs/matrix-multiplication.md","assetImports":["./images/grouped_vs_row_major_ordering.png"],"digest":"0973c9a45cbdf650","rendered":{"html":"<p>As the most important operation in GPU computation, matrix multiplication optimization is a must learn!</p>\n<p><strong>Bruh, let‚Äôs dive right in ~</strong></p>\n<h2 id=\"from-the-architecture-to-the-idea\">From the architecture to the idea</h2>\n<h3 id=\"memory-architecture-of-gpu\">Memory Architecture of GPU</h3>\n<p>(research1)[<a href=\"https://stanfordasl.github.io/projects/\">https://stanfordasl.github.io/projects/</a>]</p>\n<figure class=\"mermaid-diagram\" id=\"mermaid-qmoeuhhri\">\n  <div class=\"mermaid-container\">\n    Loading diagram...\n  </div>\n  <details class=\"mermaid-source\">\n    <summary>Source</summary>\n    <pre class=\"mermaid-code\"><code>graph TD\n    subgraph \"CPU / Host\"\n        A[Host Code] --> B{Kernel Launch}\n    end\n\n    subgraph \"GPU / Device\"\n        C[Command Queue]\n        D(Thread Blocks)\n        E[Streaming Multiprocessors]\n        F[L1 Cache / Shared Memory]\n        G[CUDA Cores]\n        H[L2 Cache]\n        I[High-Bandwidth Memory]\n    end\n\n    %% --- Define Connections ---\n    B -- Command --> C\n    C --> D\n    D --> E\n    E -- Contains --> F\n    E -- Contains --> G\n\n    %% Memory Flow\n    G &#x3C;--> F\n    F &#x3C;--> H\n    H &#x3C;--> I</code></pre>\n  </details>\n</figure>\n<ol>\n<li>Command from CPU: CPU will run a pointer to the compiled GPU function. <em>But how is it compiled, and what needs to be included? This will be discussed in the compilation part.</em></li>\n<li>Then, the command will undergo an asynchronous operation using a physical memory buffer FIFO. Compile instructions will be loaded.</li>\n<li>Here, the grid of blocks is simply a soft level abstraction. The information about the grid blocks will be compiled for each thread block, which is in the SMs (Streaming Multiprocessor). <strong><em><u>This is incredible, frontend and backend separation techniques, I guess it helps the hardware security.</u></em></strong></li>\n<li><strong><u>VRAM</u></strong> is basically a more general external memory, and <strong><u>HBM</u></strong> is just a high-performance type.</li>\n<li>Then, for the SM, it will manage warp, which manages 32 threads for executing and transporting information to other parts:</li>\n</ol>\n<figure class=\"mermaid-diagram\" id=\"mermaid-i7nh30r51\">\n  <div class=\"mermaid-container\">\n    Loading diagram...\n  </div>\n  <details class=\"mermaid-source\">\n    <summary>Source</summary>\n    <pre class=\"mermaid-code\"><code>graph TD\n    subgraph GPU Device\n        subgraph Streaming Multiprocessor 1\n            subgraph CUDA Core 1A\n                R1[Registers]\n            end\n            subgraph CUDA Core 1B\n                R2[Registers]\n            end\n            L1_1[L1 Cache / Shared Memory]\n        end\n\n        subgraph Streaming Multiprocessor 2\n            subgraph CUDA Core 2A\n                R3[Registers]\n            end\n            subgraph CUDA Core 2B\n                R4[Registers]\n            end\n            L1_2[L1 Cache / Shared Memory]\n        end\n\n        L2[L2 Cache]\n        HBM[High-Bandwidth Memory]\n    end\n\n    %% --- Data Access Paths ---\n    R1 &#x3C;--> L1_1\n    R2 &#x3C;--> L1_1\n    R3 &#x3C;--> L1_2\n    R4 &#x3C;--> L1_2\n    L1_1 &#x3C;--> L2\n    L1_2 &#x3C;--> L2\n    L2 &#x3C;--> HBM\n\n    style R1 fill:#f9f,stroke:#333,stroke-width:2px\n    style R2 fill:#f9f,stroke:#333,stroke-width:2px\n    style R3 fill:#f9f,stroke:#333,stroke-width:2px\n    style R4 fill:#f9f,stroke:#333,stroke-width:2px\n    style L1_1 fill:#bbf,stroke:#333,stroke-width:2px\n    style L1_2 fill:#bbf,stroke:#333,stroke-width:2px\n    style L2 fill:#bdf,stroke:#333,stroke-width:2px\n    style HBM fill:#fb9,stroke:#333,stroke-width:2px</code></pre>\n  </details>\n</figure>\n<p>AND there are their properties:</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Memory Hardware</th><th>Key Parameters</th><th>Primary Job</th></tr></thead><tbody><tr><td><strong>Registers</strong></td><td><strong>Size:</strong> Tiniest (bytes per thread)<br><strong>Speed:</strong> Instantaneous <strong>Scope:</strong> Private to <strong>one thread</strong></td><td>Holds the immediate working data for a single thread‚Äôs current instruction. The absolute fastest memory available.</td></tr><tr><td><strong>L1 Cache / Shared Memory</strong></td><td><strong>Size:</strong> Small (~100 KB per SM) <br><strong>Speed:</strong> Fastest <br><strong>Scope:</strong> Per SM</td><td>A high-speed scratchpad for threads within a <strong>block</strong> to share data and cooperate.</td></tr><tr><td><strong>L2 Cache</strong></td><td><strong>Size:</strong> Medium (a few MB) <br><strong>Speed:</strong> Fast<br><strong>Scope:</strong> Shared by <strong>all SMs</strong></td><td>A unified cache for the entire GPU, catching data requests that miss L1 to avoid slow HBM access.</td></tr><tr><td><strong>High-Bandwidth Memory (HBM) / VRAM</strong></td><td><strong>Size:</strong> Large (many GB) <strong>Speed:</strong> Slowest <strong>Scope:</strong> Global (entire GPU)</td><td>The main data storage for the GPU, holding all large datasets, textures, and models.</td></tr></tbody></table>\n<h3 id=\"compilation-architecture\">Compilation architecture</h3>\n<figure class=\"mermaid-diagram\" id=\"mermaid-nqdutft9l\">\n  <div class=\"mermaid-container\">\n    Loading diagram...\n  </div>\n  <details class=\"mermaid-source\">\n    <summary>Source</summary>\n    <pre class=\"mermaid-code\"><code>graph TD\n    %% --- Node Definitions ---\n\n    subgraph \"Stage 1: Compilation\"\n        A[\"üìÑ Source File .cu\\nContains both Host CPU and Device GPU code\"]\n        B[\"1. NVCC Compiler Driver\"]\n    end\n\n    subgraph \"Host CPU Path\"\n        C[\"Host Code C++\"]\n        D[\"Host Compiler\\ne.g., GCC, Clang, MSVC\"]\n        E[\"Host Object File .o\"]\n    end\n\n    subgraph \"Device GPU Path\"\n        F[\"Device Code CUDA C++\"]\n        G[\"CUDA Frontend Compiler\"]\n        H[\"PTX\\nParallel Thread Execution\\n&#x3C;I>Intermediate Assembly&#x3C;/I>\"]\n        I[\"PTX Assembler\"]\n        J[\"SASS\\nStreaming Assembler\\n&#x3C;B>GPU Machine Code&#x3C;/B>\"]\n        K[\"Device Object File .o\"]\n    end\n\n    subgraph \"Stage 2: Linking\"\n        L[\"Linker\"]\n        M[\"‚úÖ Final Executable\\nContains Host code + embedded GPU code\"]\n    end\n\n    subgraph \"Stage 3: Execution\"\n        N[\"Program runs on Host CPU\"]\n        O[\"CUDA Driver\"]\n        P{\"&#x3C;B>JIT Compilation&#x3C;/B>\\nJust-In-Time\"}\n        Q[\"üöÄ GPU Executes Code\"]\n    end\n\n    %% --- Connection Definitions ---\n    A --> B\n    B -- Separates Code --> C &#x26; F\n    C --> D --> E\n    F --> G --> H --> I --> J --> K\n    E --> L\n    K --> L\n    L --> M\n    M --> N --> O --> P\n    P -- \"If embedded SASS doesn't match current GPU\" --> I\n    P -- \"If SASS is compatible\" --> Q\n\n    %% --- Styling ---\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style L fill:#f9f,stroke:#333,stroke-width:2px\n    style N fill:#f9f,stroke:#333,stroke-width:2px\n    style O fill:#f9f,stroke:#333,stroke-width:2px</code></pre>\n  </details>\n</figure>\n<ol>\n<li>Command from CPU: The command will be compiled in the CPU. The kernel function should include the behavior of the Grid of Blocks and the number of threads per block.</li>\n<li><strong><u>JIT (just in time)</u></strong> means it compiles the program, not before, but during the GPU‚Äôs execution.</li>\n<li><strong><u>Bank conflict:</u></strong> since multiple threads can potentially want to access the same shared memory bank (a column) simultaneously, we would have to serialize them / or arrange the data properly</li>\n<li><strong><u>PTX</u></strong> provides forward compatibility, and <strong><u>SASS</u></strong> provides the maximum performance.</li>\n<li><strong><u>Memory coalescing</u></strong>: When a warp accesses the GPU‚Äôs main VRAM, the hardware checks the addresses they are requesting. If these addresses are close together and fall within a single, aligned memory segment, the GPU ‚Äúcoalesces‚Äù them. Instead of performing 32 small, separate memory fetches, it performs one single, large fetch that grabs all the requested data at once. Shared Memory/registers are fast enough, and L1/L2 also have this feature called locality.</li>\n</ol>\n<h2 id=\"torch-implementation-of-matrix-multiplication\">Torch Implementation of Matrix Multiplication</h2>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> matrix_multilication</span><span style=\"color:#E1E4E8\">(x,y):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">  # consider matrix x and y with dimention M,N,K</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  M,N </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  N,K </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.zeros(M,K)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  for</span><span style=\"color:#E1E4E8\"> row_x </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(M):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> col_y </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(K):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      dot_product </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.dot(x[row_X,:],y[:,col_y])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      accumulator[row_x][col_y] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dot_product</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> accumulator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Basically just use</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.matmul(x,y)</span></span></code></pre>\n<h2 id=\"high-level-implementation-of-matrix-multiplication\">High-Level Implementation of Matrix Multiplication</h2>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#6A737D\">#Implement the tiling in the Triton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> matrix_multiplication</span><span style=\"color:#E1E4E8\">(x,y,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  M,N </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  N,K </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  z </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.zeros(M,K)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,M,</span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  \tfor</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,K,</span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> zeros((</span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">),</span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">float32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">      #in the SM So we have to export the ACCU to the L2 first</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,n,</span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x[m:m</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">,n:n</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y[n:n</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">,k:k</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        accumulator </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> dot(a,b)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      z[m:m</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE</span><span style=\"color:#E1E4E8\">,k:k</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> accumulator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  return</span><span style=\"color:#E1E4E8\"> z</span></span></code></pre>\n<p>Here are some useful insights:</p>\n<ul>\n<li>Why do we use the accumulator? Why can‚Äôt we just add to the output matrix? A: Because of the coalescing the access to the information is faster</li>\n<li>Why do we do a small matrix multiplication instead of a faster dot product? A: They are essentially the same because they are just 1D arrays in the memory space, and the difference between these two algorithms won‚Äôt make much of a difference.</li>\n</ul>\n<h2 id=\"triton-kernel-for-matrix-multiplication\">Triton Kernel for Matrix Multiplication</h2>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#B392F0\">@triton.jit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> kernel_maxmul</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Pointers to matrices</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a_ptr, b_ptr, c_ptr,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Matrix dimensions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        M, N, K,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # The stride variables represent how much to increase the ptr by when moving by 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stride_am, stride_ak,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stride_bk, stride_bn,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stride_cm, stride_cn,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Meta-parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  </span><span style=\"color:#6A737D\"># for tiling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        GROUP_SIZE_M: tl.constexpr,  </span><span style=\"color:#6A737D\"># for super band</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ACTIVATION: tl.constexpr  )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This is done in a grouped ordering to promote L2 data reuse.(exactly why we use the GROUP_SIZE_M)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pid </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.program_id(</span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_pid_m </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.cdiv(M, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_pid_n </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.cdiv(N, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_pid_in_group </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> GROUP_SIZE_M</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> num_pid_n</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    group_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pid </span><span style=\"color:#F97583\">//</span><span style=\"color:#E1E4E8\"> num_pid_in_group</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    first_pid_m </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> group_id </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> GROUP_SIZE_M</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    group_size_m </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(num_pid_m </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> first_pid_m, </span><span style=\"color:#79B8FF\">GROUP_SIZE_M</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pid_m </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> first_pid_m </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> ((pid </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> num_pid_in_group) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> group_size_m)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pid_n </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (pid </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> num_pid_in_group) </span><span style=\"color:#F97583\">//</span><span style=\"color:#E1E4E8\"> group_size_m</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ------------------------------------------------------</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This helps to guide integer analysis in the backend to optimize load/store offset address calculation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(pid_m </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(pid_n </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_am </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_ak </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_bn </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_bk </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_cm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_cn </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ----------------------------------------------------------</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create pointers for the first blocks of A and B.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_am </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (pid_m </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_M</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> M</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_bn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (pid_n </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_N</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> N</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a_ptrs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a_ptr </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (offs_am[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stride_am </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> offs_k[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stride_ak)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b_ptrs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> b_ptr </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (offs_k[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stride_bk </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> offs_bn[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stride_bn)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # -----------------------------------------------------------</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Iterate to compute a block of the C matrix.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # of fp32 values for higher accuracy.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # `accumulator` will be converted back to fp16 after the loop.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.zeros((</span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tl.float32)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, tl.cdiv(K, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">)):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Load the next block of A and B, generate a mask by checking the K dimension.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If it is out of bounds, set it to 0.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.load(a_ptrs, </span><span style=\"color:#FFAB70\">mask</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">offs_k[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> K </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">other</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.load(b_ptrs, </span><span style=\"color:#FFAB70\">mask</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">offs_k[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> K </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">other</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # We accumulate along the K dimension.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.dot(a, b, accumulator)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Advance the ptrs to the next K block.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a_ptrs </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_K</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> stride_ak</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b_ptrs </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_K</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> stride_bk</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # You can fuse arbitrary activation functions here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # while the accumulator is still in FP32!</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> ACTIVATION</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"leaky_relu\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> leaky_relu(accumulator)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> accumulator.to(tl.float16)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # -----------------------------------------------------------</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Write back the block of the output matrix C with masks.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_cm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pid_m </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_M</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_cn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pid_n </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_N</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c_ptrs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> c_ptr </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> stride_cm </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> offs_cm[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> stride_cn </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> offs_cn[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (offs_cm[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> M) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> (offs_cn[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> N)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.store(c_ptrs, c, </span><span style=\"color:#FFAB70\">mask</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">c_mask)</span></span>\n<span class=\"line\"></span></code></pre>\n<ul>\n<li>We used a novel parameter: GROUP_SIZE_M, which was meant to be the height of a super-block with several BLOCK_SIZE_M tall. The following picture shows this idea better. Let me use the quote from the original tutorial <u><em>‚ÄúIn the following matmul where each matrix is 9 blocks by 9 blocks, we can see that if we compute the output in row-major ordering, we need to load 90 blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped ordering, we only need to load 54 blocks.‚Äù</em></u></li>\n</ul>\n<h2 id=\"\"><img __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;./images/grouped_vs_row_major_ordering.png&#x22;,&#x22;alt&#x22;:&#x22;grouped_vs_row_major_ordering&#x22;,&#x22;index&#x22;:0}\"></h2>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>GPU performance is dictated by its memory hierarchy: extremely fast but small on-chip memory (Registers, L1 Cache/Shared Memory) and large but slow off-chip memory (VRAM/HBM). The primary optimization goal is to minimize traffic to slow VRAM by maximizing data reuse in the fast caches.</p>\n<p>The key software strategy is <strong>tiling</strong> (or blocking), where large matrices are broken into smaller blocks that fit into fast on-chip memory. Computations are performed on these blocks using a local <strong>accumulator</strong> to sum results, ensuring only one final, slow write to VRAM per block. This algorithm must also account for hardware features, ensuring memory access is <strong>coalesced</strong> to maximize VRAM bandwidth and that it avoids <strong>bank conflicts</strong> in shared memory.</p>\n<p>Block Tiling also works ! And it fosters the calculation speed.</p>","metadata":{"headings":[{"depth":2,"slug":"from-the-architecture-to-the-idea","text":"From the architecture to the idea"},{"depth":3,"slug":"memory-architecture-of-gpu","text":"Memory Architecture of GPU"},{"depth":3,"slug":"compilation-architecture","text":"Compilation architecture"},{"depth":2,"slug":"torch-implementation-of-matrix-multiplication","text":"Torch Implementation of Matrix Multiplication"},{"depth":2,"slug":"high-level-implementation-of-matrix-multiplication","text":"High-Level Implementation of Matrix Multiplication"},{"depth":2,"slug":"triton-kernel-for-matrix-multiplication","text":"Triton Kernel for Matrix Multiplication"},{"depth":2,"slug":"","text":""},{"depth":2,"slug":"conclusion","text":"Conclusion"}],"localImagePaths":["./images/grouped_vs_row_major_ordering.png"],"remoteImagePaths":[],"frontmatter":{"title":"Intro to Triton with Matrix Multiplication","description":"Introduction to GPU programming with Triton and build the matrix multiplication along the way","pubDate":"2025-03-18T00:00:00.000Z","author":"Xiaoyou Wu","tags":["triton","GPU","CUDA","compiler","code","optimization"],"image":"./images/grouped_vs_row_major_ordering.png"},"imagePaths":["./images/grouped_vs_row_major_ordering.png"]}},"collection":"blogs","slug":"matrix-multiplication"},{"id":"QAT-Implementation.md","data":{"title":"Quantization Aware Training Implementation Guide","description":"Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies","pubDate":"2025-01-15T00:00:00.000Z","author":"Xiaoyou Wu","tags":["machine-learning","quantization","QAT","LoRA","GPU","optimization","pytorch"]},"body":"# Quantization Aware Training\r\n\r\n# Basic information\r\n\r\n### LoRA:\r\n\r\n- Old method to improve domain specific fine-tuning faces disadvantages like sequential processing bottleneck. So LoRA changes the feed forward layer to self.linear(x) + (x @ self.lora_A @ self.lora_B) *\r\n    self.scaling .\r\n\r\n**PRT (Precision Range Test)**\r\n\r\n- Start from the lowest bit and detect the predetermined threshold to record the B_min. Then the B_max should be determined by the max precision you will be need to experiment with.\r\n\r\n## Implementation details\r\n\r\n### Loading weights\r\n\r\nFor model from the transformer, you should remember to import the weight of the projection layer\r\n\r\n### Initializing the model\r\n\r\nUse apply function in torch to optimize the initialization of different layers. In the current case, you should initialize transformer layer with its final projection layer and also two layers of ffn.\r\n\r\n### Inheritance of  nn.Module and torch.autograd.Function \r\n\r\n\r\n\r\n# APPENDIX ON torch and transformer usage\r\n\r\n## Important functions in torch\r\n\r\n1. torch.save: uses the pickle for operations. For tensors, its raw data, size information, gradient requirements. For models, it mainly store the state_dict which is an orderedDict that maps each layer or params name to its tensor values.\r\n2. torch.amp.GradScaler('cuda'): Automatic Mixed Precision\r\n3. torch.nn.module: its handy to inherit this class for your customized model. You can just do model(input_params) to call the \"\\__call__\" function inherited to perform ffn.\r\n4. For dataset objects imported with the load_dataset from the datasets library. Its handy to call the .feature property  or you can call the \\__dict\\__ method to understand the structure.\r\n5. return_tensor = \"pt\" adds a dimension so remember to do the [0] for the tensor.\r\n6. While loading information from a dataset, pay attention to the dataset padding token and eos token, if their choice is the same, you should change the padding token to be something else\r\n\r\n## Important functions in transformer\r\n\r\n1. GPT2Config return an json with all those configurations and then could be utilized by other tuning methods.\r\n2. model.eval() turn on the evaluation mode and disable those dropout, \r\n\r\n# APPENDIX ON GPU RAM usage\r\n\r\n# GPU Memory Hierarchy & Parameter Impact Guide\r\n\r\n## Memory Hierarchy Overview\r\n\r\n| Memory Type | Size (A100) | Bandwidth | Latency | What's Stored |\r\n| --- | --- | --- | --- | --- |\r\n| **Registers** | 256 KB/SM | ~19 TB/s | 1 cycle | Active thread variables, loop counters |\r\n| **L1 Cache/SMEM** | 192 KB/SM | ~19 TB/s | ~30 cycles | Shared memory, frequently accessed data |\r\n| **L2 Cache** | 40 MB | ~4 TB/s | ~200 cycles | Recently accessed data from HBM |\r\n| **HBM (VRAM)** | 40-80 GB | ~2 TB/s | ~400 cycles | Model weights, activations, optimizer states |\r\n\r\n## Parameter Impact on Memory Usage\r\n\r\n| Parameter | HBM Usage | L1/SMEM Usage | Register Usage | Impact Description |\r\n| --- | --- | --- | --- | --- |\r\n| **batch_size** | High üî¥ | Medium üü° | Low üü¢ | Multiplies activation memory linearly |\r\n| **model_size** | High üî¥ | Low üü¢ | Low üü¢ | Weights stored entirely in HBM |\r\n| **sequence_length** | High üî¥ | Medium üü° | Low üü¢ | Quadratic for attention (seq_len¬≤) |\r\n| **hidden_dim** | High üî¥ | Medium üü° | Low üü¢ | Affects weight matrices & activations |\r\n| **num_layers** | High üî¥ | Low üü¢ | Low üü¢ | Linear increase in weights |\r\n| **precision (FP32/16/8)** | High üî¥ | Medium üü° | Medium üü° | Halves memory per precision drop |\r\n| **gradient_accumulation** | Low üü¢ | Low üü¢ | Low üü¢ | Reduces batch memory requirement |\r\n| **optimizer (SGD/Adam)** | High üî¥ | Low üü¢ | Low üü¢ | Adam uses 3x memory (m, v states) |\r\n\r\n## Detailed HBM Storage Breakdown\r\n\r\n| Component | Formula | FP32 Memory | FP16 Memory | Stored Location |\r\n| --- | --- | --- | --- | --- |\r\n| **Model Weights** | `num_params √ó precision` | 4 bytes/param | 2 bytes/param | HBM |\r\n| **Gradients** | `num_params √ó precision` | 4 bytes/param | 2 bytes/param | HBM |\r\n| **Adam Optimizer** | `2 √ó num_params √ó FP32` | 8 bytes/param | 8 bytes/param* | HBM |\r\n| **Activations** | `batch √ó seq_len √ó hidden √ó layers` | Variable | Variable/2 | HBM |\r\n| **KV Cache (LLMs)** | `batch √ó heads √ó seq_len √ó dim √ó layers √ó 2` | Large | Large/2 | HBM |\r\n| **Temp Buffers** | `workspace for ops` | ~1-2 GB | ~0.5-1 GB | HBM |\r\n\r\n*Adam states typically stay FP32 even in mixed precision\r\n\r\n## Kernel-Level Memory Usage\r\n\r\n| Operation | Register Pressure | L1/SMEM Usage | HBM Access Pattern |\r\n| --- | --- | --- | --- |\r\n| **GEMM (MatMul)** | High üî¥ | High üî¥ | Tiled access |\r\n| **Element-wise** | Medium üü° | Low üü¢ | Sequential streaming |\r\n| **Softmax** | Medium üü° | Medium üü° | Row-wise access |\r\n| **LayerNorm** | Medium üü° | Medium üü° | Channel-wise access |\r\n| **Attention** | High üî¥ | High üî¥ | Complex tiling |\r\n| **Conv2D** | High üî¥ | High üî¥ | Im2col or tiled |\r\n\r\n## Optimization Strategies by Memory Type\r\n\r\n| Memory Type | Optimization Strategy | Impact |\r\n| --- | --- | --- |\r\n| **HBM** | Gradient checkpointing, model sharding, mixed precision | Reduce total storage |\r\n| **L2 Cache** | Increase arithmetic intensity, kernel fusion | Reduce HBM traffic |\r\n| **L1/SMEM** | Tile size tuning, shared memory allocation | Better data reuse |\r\n| **Registers** | Loop unrolling, reduce live variables | Higher throughput |\r\n\r\n## Practical Example: GPT-2 Medium (345M Parameters)\r\n\r\n### Memory Breakdown\r\n\r\n| Component | Calculation | Memory Usage |\r\n| --- | --- | --- |\r\n| **Parameters** | 345M params √ó 4 bytes | 1.4 GB (FP32) |\r\n| **Gradients** | 345M params √ó 4 bytes | 1.4 GB (FP32) |\r\n| **Adam States** | 345M √ó 2 √ó 4 bytes | 2.8 GB (FP32) |\r\n| **Activations** | batch=8, seq=1024, ~20 layers | ~4 GB |\r\n| **Total Training** | Sum of above | **~9.6 GB** |\r\n| **Inference Only** | Parameters only | **~1.4 GB** |\r\n\r\n### Memory Usage by Precision\r\n\r\n| Precision | Weights | Gradients | Adam | Activations | Total Training |\r\n| --- | --- | --- | --- | --- | --- |\r\n| **FP32** | 1.4 GB | 1.4 GB | 2.8 GB | 4 GB | 9.6 GB |\r\n| **FP16 Mixed** | 0.7 GB | 0.7 GB | 2.8 GB | 2 GB | 6.2 GB |\r\n| **INT8** | 0.35 GB | N/A | N/A | 1 GB | 1.35 GB (Inference) |\r\n\r\n## Memory Calculation Formulas\r\n\r\n### Training Memory\r\n```\r\nTotal_Memory = Model_Weights + Gradients + Optimizer_States + Activations + Temp_Buffers\r\n```\r\n\r\n### Model Weights\r\n```\r\nModel_Memory = num_parameters √ó bytes_per_param\r\n```\r\n\r\n### Activation Memory (Transformer)\r\n```\r\nActivation_Memory = batch_size √ó seq_length √ó hidden_dim √ó num_layers √ó \r\n                   (attention_heads + mlp_ratio + norm_layers)\r\n```\r\n\r\n### Attention Memory\r\n```\r\nAttention_Memory = batch_size √ó num_heads √ó seq_length¬≤ √ó head_dim √ó num_layers\r\n```\r\n\r\n## Common Memory Bottlenecks\r\n\r\n| Bottleneck | Symptoms | Solution |\r\n| --- | --- | --- |\r\n| **OOM on forward pass** | Crashes during model(input) | Reduce batch size or model size |\r\n| **OOM on backward pass** | Crashes during loss.backward() | Enable gradient checkpointing |\r\n| **OOM on optimizer step** | Crashes during optimizer.step() | Use gradient accumulation or efficient optimizer |\r\n| **Slow training** | Low GPU utilization | Increase batch size or arithmetic intensity |\r\n| **Memory fragmentation** | OOM with available memory | Clear cache: `torch.cuda.empty_cache()` |","filePath":"src/content/blogs/QAT-Implementation.md","digest":"90f13013df671c7e","rendered":{"html":"<h1 id=\"quantization-aware-training\">Quantization Aware Training</h1>\n<h1 id=\"basic-information\">Basic information</h1>\n<h3 id=\"lora\">LoRA:</h3>\n<ul>\n<li>Old method to improve domain specific fine-tuning faces disadvantages like sequential processing bottleneck. So LoRA changes the feed forward layer to self.linear(x) + (x @ self.lora_A @ self.lora_B) *\r\nself.scaling .</li>\n</ul>\n<p><strong>PRT (Precision Range Test)</strong></p>\n<ul>\n<li>Start from the lowest bit and detect the predetermined threshold to record the B_min. Then the B_max should be determined by the max precision you will be need to experiment with.</li>\n</ul>\n<h2 id=\"implementation-details\">Implementation details</h2>\n<h3 id=\"loading-weights\">Loading weights</h3>\n<p>For model from the transformer, you should remember to import the weight of the projection layer</p>\n<h3 id=\"initializing-the-model\">Initializing the model</h3>\n<p>Use apply function in torch to optimize the initialization of different layers. In the current case, you should initialize transformer layer with its final projection layer and also two layers of ffn.</p>\n<h3 id=\"inheritance-of--nnmodule-and-torchautogradfunction\">Inheritance of  nn.Module and torch.autograd.Function</h3>\n<h1 id=\"appendix-on-torch-and-transformer-usage\">APPENDIX ON torch and transformer usage</h1>\n<h2 id=\"important-functions-in-torch\">Important functions in torch</h2>\n<ol>\n<li>torch.save: uses the pickle for operations. For tensors, its raw data, size information, gradient requirements. For models, it mainly store the state_dict which is an orderedDict that maps each layer or params name to its tensor values.</li>\n<li>torch.amp.GradScaler(‚Äòcuda‚Äô): Automatic Mixed Precision</li>\n<li>torch.nn.module: its handy to inherit this class for your customized model. You can just do model(input_params) to call the ‚Äú_<em>call</em>_‚Äù function inherited to perform ffn.</li>\n<li>For dataset objects imported with the load_dataset from the datasets library. Its handy to call the .feature property  or you can call the _<em>dict_</em> method to understand the structure.</li>\n<li>return_tensor = ‚Äúpt‚Äù adds a dimension so remember to do the [0] for the tensor.</li>\n<li>While loading information from a dataset, pay attention to the dataset padding token and eos token, if their choice is the same, you should change the padding token to be something else</li>\n</ol>\n<h2 id=\"important-functions-in-transformer\">Important functions in transformer</h2>\n<ol>\n<li>GPT2Config return an json with all those configurations and then could be utilized by other tuning methods.</li>\n<li>model.eval() turn on the evaluation mode and disable those dropout,</li>\n</ol>\n<h1 id=\"appendix-on-gpu-ram-usage\">APPENDIX ON GPU RAM usage</h1>\n<h1 id=\"gpu-memory-hierarchy--parameter-impact-guide\">GPU Memory Hierarchy &#x26; Parameter Impact Guide</h1>\n<h2 id=\"memory-hierarchy-overview\">Memory Hierarchy Overview</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Memory Type</th><th>Size (A100)</th><th>Bandwidth</th><th>Latency</th><th>What‚Äôs Stored</th></tr></thead><tbody><tr><td><strong>Registers</strong></td><td>256 KB/SM</td><td>~19 TB/s</td><td>1 cycle</td><td>Active thread variables, loop counters</td></tr><tr><td><strong>L1 Cache/SMEM</strong></td><td>192 KB/SM</td><td>~19 TB/s</td><td>~30 cycles</td><td>Shared memory, frequently accessed data</td></tr><tr><td><strong>L2 Cache</strong></td><td>40 MB</td><td>~4 TB/s</td><td>~200 cycles</td><td>Recently accessed data from HBM</td></tr><tr><td><strong>HBM (VRAM)</strong></td><td>40-80 GB</td><td>~2 TB/s</td><td>~400 cycles</td><td>Model weights, activations, optimizer states</td></tr></tbody></table>\n<h2 id=\"parameter-impact-on-memory-usage\">Parameter Impact on Memory Usage</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Parameter</th><th>HBM Usage</th><th>L1/SMEM Usage</th><th>Register Usage</th><th>Impact Description</th></tr></thead><tbody><tr><td><strong>batch_size</strong></td><td>High üî¥</td><td>Medium üü°</td><td>Low üü¢</td><td>Multiplies activation memory linearly</td></tr><tr><td><strong>model_size</strong></td><td>High üî¥</td><td>Low üü¢</td><td>Low üü¢</td><td>Weights stored entirely in HBM</td></tr><tr><td><strong>sequence_length</strong></td><td>High üî¥</td><td>Medium üü°</td><td>Low üü¢</td><td>Quadratic for attention (seq_len¬≤)</td></tr><tr><td><strong>hidden_dim</strong></td><td>High üî¥</td><td>Medium üü°</td><td>Low üü¢</td><td>Affects weight matrices &#x26; activations</td></tr><tr><td><strong>num_layers</strong></td><td>High üî¥</td><td>Low üü¢</td><td>Low üü¢</td><td>Linear increase in weights</td></tr><tr><td><strong>precision (FP32/16/8)</strong></td><td>High üî¥</td><td>Medium üü°</td><td>Medium üü°</td><td>Halves memory per precision drop</td></tr><tr><td><strong>gradient_accumulation</strong></td><td>Low üü¢</td><td>Low üü¢</td><td>Low üü¢</td><td>Reduces batch memory requirement</td></tr><tr><td><strong>optimizer (SGD/Adam)</strong></td><td>High üî¥</td><td>Low üü¢</td><td>Low üü¢</td><td>Adam uses 3x memory (m, v states)</td></tr></tbody></table>\n<h2 id=\"detailed-hbm-storage-breakdown\">Detailed HBM Storage Breakdown</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Component</th><th>Formula</th><th>FP32 Memory</th><th>FP16 Memory</th><th>Stored Location</th></tr></thead><tbody><tr><td><strong>Model Weights</strong></td><td><code>num_params √ó precision</code></td><td>4 bytes/param</td><td>2 bytes/param</td><td>HBM</td></tr><tr><td><strong>Gradients</strong></td><td><code>num_params √ó precision</code></td><td>4 bytes/param</td><td>2 bytes/param</td><td>HBM</td></tr><tr><td><strong>Adam Optimizer</strong></td><td><code>2 √ó num_params √ó FP32</code></td><td>8 bytes/param</td><td>8 bytes/param*</td><td>HBM</td></tr><tr><td><strong>Activations</strong></td><td><code>batch √ó seq_len √ó hidden √ó layers</code></td><td>Variable</td><td>Variable/2</td><td>HBM</td></tr><tr><td><strong>KV Cache (LLMs)</strong></td><td><code>batch √ó heads √ó seq_len √ó dim √ó layers √ó 2</code></td><td>Large</td><td>Large/2</td><td>HBM</td></tr><tr><td><strong>Temp Buffers</strong></td><td><code>workspace for ops</code></td><td>~1-2 GB</td><td>~0.5-1 GB</td><td>HBM</td></tr></tbody></table>\n<p>*Adam states typically stay FP32 even in mixed precision</p>\n<h2 id=\"kernel-level-memory-usage\">Kernel-Level Memory Usage</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Operation</th><th>Register Pressure</th><th>L1/SMEM Usage</th><th>HBM Access Pattern</th></tr></thead><tbody><tr><td><strong>GEMM (MatMul)</strong></td><td>High üî¥</td><td>High üî¥</td><td>Tiled access</td></tr><tr><td><strong>Element-wise</strong></td><td>Medium üü°</td><td>Low üü¢</td><td>Sequential streaming</td></tr><tr><td><strong>Softmax</strong></td><td>Medium üü°</td><td>Medium üü°</td><td>Row-wise access</td></tr><tr><td><strong>LayerNorm</strong></td><td>Medium üü°</td><td>Medium üü°</td><td>Channel-wise access</td></tr><tr><td><strong>Attention</strong></td><td>High üî¥</td><td>High üî¥</td><td>Complex tiling</td></tr><tr><td><strong>Conv2D</strong></td><td>High üî¥</td><td>High üî¥</td><td>Im2col or tiled</td></tr></tbody></table>\n<h2 id=\"optimization-strategies-by-memory-type\">Optimization Strategies by Memory Type</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Memory Type</th><th>Optimization Strategy</th><th>Impact</th></tr></thead><tbody><tr><td><strong>HBM</strong></td><td>Gradient checkpointing, model sharding, mixed precision</td><td>Reduce total storage</td></tr><tr><td><strong>L2 Cache</strong></td><td>Increase arithmetic intensity, kernel fusion</td><td>Reduce HBM traffic</td></tr><tr><td><strong>L1/SMEM</strong></td><td>Tile size tuning, shared memory allocation</td><td>Better data reuse</td></tr><tr><td><strong>Registers</strong></td><td>Loop unrolling, reduce live variables</td><td>Higher throughput</td></tr></tbody></table>\n<h2 id=\"practical-example-gpt-2-medium-345m-parameters\">Practical Example: GPT-2 Medium (345M Parameters)</h2>\n<h3 id=\"memory-breakdown\">Memory Breakdown</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Component</th><th>Calculation</th><th>Memory Usage</th></tr></thead><tbody><tr><td><strong>Parameters</strong></td><td>345M params √ó 4 bytes</td><td>1.4 GB (FP32)</td></tr><tr><td><strong>Gradients</strong></td><td>345M params √ó 4 bytes</td><td>1.4 GB (FP32)</td></tr><tr><td><strong>Adam States</strong></td><td>345M √ó 2 √ó 4 bytes</td><td>2.8 GB (FP32)</td></tr><tr><td><strong>Activations</strong></td><td>batch=8, seq=1024, ~20 layers</td><td>~4 GB</td></tr><tr><td><strong>Total Training</strong></td><td>Sum of above</td><td><strong>~9.6 GB</strong></td></tr><tr><td><strong>Inference Only</strong></td><td>Parameters only</td><td><strong>~1.4 GB</strong></td></tr></tbody></table>\n<h3 id=\"memory-usage-by-precision\">Memory Usage by Precision</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Precision</th><th>Weights</th><th>Gradients</th><th>Adam</th><th>Activations</th><th>Total Training</th></tr></thead><tbody><tr><td><strong>FP32</strong></td><td>1.4 GB</td><td>1.4 GB</td><td>2.8 GB</td><td>4 GB</td><td>9.6 GB</td></tr><tr><td><strong>FP16 Mixed</strong></td><td>0.7 GB</td><td>0.7 GB</td><td>2.8 GB</td><td>2 GB</td><td>6.2 GB</td></tr><tr><td><strong>INT8</strong></td><td>0.35 GB</td><td>N/A</td><td>N/A</td><td>1 GB</td><td>1.35 GB (Inference)</td></tr></tbody></table>\n<h2 id=\"memory-calculation-formulas\">Memory Calculation Formulas</h2>\n<h3 id=\"training-memory\">Training Memory</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>Total_Memory = Model_Weights + Gradients + Optimizer_States + Activations + Temp_Buffers</span></span></code></pre>\n<h3 id=\"model-weights\">Model Weights</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>Model_Memory = num_parameters √ó bytes_per_param</span></span></code></pre>\n<h3 id=\"activation-memory-transformer\">Activation Memory (Transformer)</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>Activation_Memory = batch_size √ó seq_length √ó hidden_dim √ó num_layers √ó </span></span>\n<span class=\"line\"><span>                   (attention_heads + mlp_ratio + norm_layers)</span></span></code></pre>\n<h3 id=\"attention-memory\">Attention Memory</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>Attention_Memory = batch_size √ó num_heads √ó seq_length¬≤ √ó head_dim √ó num_layers</span></span></code></pre>\n<h2 id=\"common-memory-bottlenecks\">Common Memory Bottlenecks</h2>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Bottleneck</th><th>Symptoms</th><th>Solution</th></tr></thead><tbody><tr><td><strong>OOM on forward pass</strong></td><td>Crashes during model(input)</td><td>Reduce batch size or model size</td></tr><tr><td><strong>OOM on backward pass</strong></td><td>Crashes during loss.backward()</td><td>Enable gradient checkpointing</td></tr><tr><td><strong>OOM on optimizer step</strong></td><td>Crashes during optimizer.step()</td><td>Use gradient accumulation or efficient optimizer</td></tr><tr><td><strong>Slow training</strong></td><td>Low GPU utilization</td><td>Increase batch size or arithmetic intensity</td></tr><tr><td><strong>Memory fragmentation</strong></td><td>OOM with available memory</td><td>Clear cache: <code>torch.cuda.empty_cache()</code></td></tr></tbody></table>","metadata":{"headings":[{"depth":1,"slug":"quantization-aware-training","text":"Quantization Aware Training"},{"depth":1,"slug":"basic-information","text":"Basic information"},{"depth":3,"slug":"lora","text":"LoRA:"},{"depth":2,"slug":"implementation-details","text":"Implementation details"},{"depth":3,"slug":"loading-weights","text":"Loading weights"},{"depth":3,"slug":"initializing-the-model","text":"Initializing the model"},{"depth":3,"slug":"inheritance-of--nnmodule-and-torchautogradfunction","text":"Inheritance of  nn.Module and torch.autograd.Function"},{"depth":1,"slug":"appendix-on-torch-and-transformer-usage","text":"APPENDIX ON torch and transformer usage"},{"depth":2,"slug":"important-functions-in-torch","text":"Important functions in torch"},{"depth":2,"slug":"important-functions-in-transformer","text":"Important functions in transformer"},{"depth":1,"slug":"appendix-on-gpu-ram-usage","text":"APPENDIX ON GPU RAM usage"},{"depth":1,"slug":"gpu-memory-hierarchy--parameter-impact-guide","text":"GPU Memory Hierarchy & Parameter Impact Guide"},{"depth":2,"slug":"memory-hierarchy-overview","text":"Memory Hierarchy Overview"},{"depth":2,"slug":"parameter-impact-on-memory-usage","text":"Parameter Impact on Memory Usage"},{"depth":2,"slug":"detailed-hbm-storage-breakdown","text":"Detailed HBM Storage Breakdown"},{"depth":2,"slug":"kernel-level-memory-usage","text":"Kernel-Level Memory Usage"},{"depth":2,"slug":"optimization-strategies-by-memory-type","text":"Optimization Strategies by Memory Type"},{"depth":2,"slug":"practical-example-gpt-2-medium-345m-parameters","text":"Practical Example: GPT-2 Medium (345M Parameters)"},{"depth":3,"slug":"memory-breakdown","text":"Memory Breakdown"},{"depth":3,"slug":"memory-usage-by-precision","text":"Memory Usage by Precision"},{"depth":2,"slug":"memory-calculation-formulas","text":"Memory Calculation Formulas"},{"depth":3,"slug":"training-memory","text":"Training Memory"},{"depth":3,"slug":"model-weights","text":"Model Weights"},{"depth":3,"slug":"activation-memory-transformer","text":"Activation Memory (Transformer)"},{"depth":3,"slug":"attention-memory","text":"Attention Memory"},{"depth":2,"slug":"common-memory-bottlenecks","text":"Common Memory Bottlenecks"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Quantization Aware Training Implementation Guide","description":"Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies","pubDate":"2025-01-15T00:00:00.000Z","author":"Xiaoyou Wu","tags":["machine-learning","quantization","QAT","LoRA","GPU","optimization","pytorch"]},"imagePaths":[]}},"collection":"blogs","slug":"qat-implementation"},{"id":"EIC_interview_report.md","data":{"title":"Quantization Strategy Report: Switchable and Cyclic Precision Training","description":"Comprehensive report on quantization strategies including switchable precision and cyclic precision training applied to WikiText-103 dataset","pubDate":"2025-01-10T00:00:00.000Z","author":"Xiaoyou Wu","tags":["quantization","machine-learning","precision-training","wikitext","model-compression"]},"body":"# Report on Quantization Strategy on Switchable Precision and Cyclic Precision Training\r\n\r\n*// Because the link to the squad in the pdf can't find anything: https://huggingface.co/Squad/datasets, therefore, I switched to a text generative dataset wikitext 103 for its ability on quantization*\r\n\r\n# 0. Performance achieved after applying various quantization bit-width configurations to the wikitext-103 dataset.\r\n\r\n|        | WikiText103 perplexity | BoolQ | HellaSwag | WinoGrande | Accuracy  Average |\r\n| ------ | ---------------------- | ----- | --------- | ---------- | ----------------- |\r\n| 4-bit  | 54.5                   | 61.2  | 30.2      | 53.2       | 47.8              |\r\n| 5-bit  | 30.8                   | 61.8  | 33.4      | 48.2       | 47.8              |\r\n| 16-bit | 26.9                   | 58.8  | 33.2      | 51.6       | 47.9              |\r\n| 32-bit | 26.9                   | 59.0  | 32.2      | 53.2       | 48.1              |\r\n\r\nAll of them are trained with gradual bit width configurations.\r\n\r\n\r\n\r\n## 1. **Questions: How did you determine the optimal quantization bit-width configurations? Have you gleaned any insights from your observations that could guide future work to further enhance performance? ** \r\n\r\nDetermining the **optimal configuration** requires clearly defining what \"optimal\" means in relation to specific **evaluation benchmarks** and performance metrics. For this report, we define optimal performance based on two primary criteria: perplexity scores and zero-shot accuracy. Several critical factors influence the optimization of model performance:\r\n\r\n### 1.1 **Choice of quantizer base on the current precision**: \r\n\r\nThe choice of quantization method significantly impacts model performance and varies with the target bit width:\r\n\r\n- **For 4-bit or lower precision**: MinMax quantization delivers the best results\r\n- **For 5-bit or higher precision**: Advanced methods like log quantization and tanh quantization provide superior performance\r\n\r\nMy experiments reveal that log quantization reduces 8-bit model degradation to just 2%, compared to 15% degradation with MinMax quantization. This improvement likely occurs because GPT-2 Small has relatively limited information content and less pronounced outlier effects, allowing log quantization to outperform the MinMax clipping function in case of higher precisions.  Here's my experiments results on different approaches of changing the quantization strategies.\r\n\r\n|                           | 4 bit with Minmax Quantizer | 4 bit with Log Quantizer | 5 bit with Minmax Quantizer | 5 bit with Log Quantizer |\r\n| ------------------------- | --------------------------- | ------------------------ | --------------------------- | ------------------------ |\r\n| perplexity on WikiText2   | 53.2                        | 56.5                     | 40.5                        | 35.1                     |\r\n| perplexity on WikiText103 | 44.5                        | 48.4                     | 33.6                        | 29.3                     |\r\n| BoolQ                     | 57.0                        | 59.0                     | 52.6                        | 57.4                     |\r\n| HellaSwag                 | 32.0                        | 30.2                     | 32.8                        | 33.4                     |\r\n| WinoGrande                | 54.6                        | 48.2                     | 51.0                        | 56.4                     |\r\n| Zeroshot average          | 47.9                        | 45.8                     | 45.5                        | 49.1                     |\r\n\r\n**Important Notes:**\r\n\r\n- C4 perplexity metrics were excluded from this analysis since the models were fine-tuned specifically on the WikiText dataset, making C4 comparisons less meaningful\r\n\r\n\r\n\r\n### 1.2  Training precision distribution\r\n\r\n### 1.2.1 On accumulative temperature scaled soft max distillation method\r\n\r\nDuring the training process of switchable precision model, the self distillation formula is as follows: \r\n$$\r\nL_q = \\alpha_1 D_{KL}(SG(p_r)||p_{qq}) + \\alpha_2 \\sum_{i\\ in\\ feature} \\mathcal||f_{ir} - f_{iq}||_2^2\r\n$$\r\nWhere $p_{r}$ is the unquantized output distribution, $p_{qq}$ is the quantized output distribution.\r\n\r\n- |                           | 4 bit model | 5 bit model |\r\n  | ------------------------- | ----------- | ----------- |\r\n  | perplexity on WikiText2   | 56.5        | 35.1        |\r\n  | perplexity on WikiText103 | 48.4        | 29.3        |\r\n  | BoolQ                     | 59.0        | 57.4        |\r\n  | HellaSwag                 | 30.2        | 33.4        |\r\n  | WinoGrande                | 48.2        | 56.4        |\r\n  | Zeroshot average          | 45.8        | 49.1        |\r\n\r\n‚Äã\t**All of them are using the log quantizer.**\t\r\n\r\n- Activation bits changes accordingly to the current bit width during the training.\r\n\r\n\r\n\r\n### 1.2.2 On non-accumulative temperature scaled soft max distillation method (used by the switchable precision paper)\r\n\r\n$$\r\nL_q = \\alpha_1 D_{KL}(SG(p_r)||p_{qq}) + \\alpha_2 \\mathcal||f_r - f_{q}||_2^2\r\n$$\r\n\r\nI also investigated a stochastic distillation variant where feature layers are randomly selected from the hidden states during training. Surprisingly, despite receiving guidance from only a subset of attention layers rather than all layers, the model's accuracy improved rather than deteriorated, while the perplexity decreases as expected:\r\n\r\n|                           | 4 bit self distillation method with teaching from all the hidden states | 4 bit self distillation method with teaching from a random hidden state |\r\n| ------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\r\n| perplexity on WikiText2   | 56.5                                                         | 67.6                                                         |\r\n| perplexity on WikiText103 | 48.4                                                         | dc ..68.8                                                    |\r\n| BoolQ                     | 59.0                                                         | 60.0                                                         |\r\n| HellaSwag                 | 30.2                                                         | 31.8                                                         |\r\n| WinoGrande                | 48.2                                                         | 49.8                                                         |\r\n| Zeroshot average          | 45.8                                                         | 47.2                                                         |\r\n\r\nThese results raise an important question: which specific information within the hidden layers contributes most significantly to accuracy improvements? Understanding this could help optimize the distillation process further.\r\n\r\n## 2. Interesting insight on optimizing the model performance\r\n\r\n### 2.1 16 bits is extremely similar to the 32 bits\r\nCalibration logic need to be recalibrated after series of grad accumulation. With no calibration, **16 bit has the same perplexity(0.8% degradation) as 32 bit**, the degradation from not quantization is minimal. \r\n\r\n### 2.2 Merge then quant or the other way around? \r\nFor lora, would it be better performing if we apply the AB and then do the quantization or do the quantization after AB? [Experiment from a blogger](https://kaitchup.substack.com/p/training-loading-and-merging-qdora)  The inference time is so much longer, which is due to the huge tensor for dequantization. Meanwhile, I would think that it would take longer to train since the big tensor is harder to calibrate. \r\n\r\n|                   | Merge after Quant: Q(W) + Q(A @ B) | Merge after Quant: Q(W) + Q(A )@ Q(B) |\r\n| ----------------- | ---------------------------------- | ------------------------------------- |\r\n| Inference Speed   | Faster                             | Slower                                |\r\n| Accuracy          | Same                               | Same                                  |\r\n| Calibration speed | Faster ( by a bit)                 | Slower                                |\r\n\r\n### 2.3 Per channel , per feature or per token ? \r\nMy hypothesis is that quantization calibration should align with the **semantic boundaries** of tensor dimensions. Different layers encode information differently, and quantization should preserve these natural groupings. (But from the testing experiences, it seems that it actually not matter as much tho.) For Embedded layers, per channel calibration should be optimal, and for other layers like projection layers and those attention layers, per token calibration should be more optimal.\r\n\r\n### 2.4 **Unexpected minimal impact during debugging**: \r\n\r\n- It just incredible that by changing the FP32 weight bit quantization for 32 bits to the matching precision reduce the perplexity from about 120 to 30 (approximately, since I was debugging the code).\r\n- During the evaluation with input activation quantization, using pretrained input scale and zero point or recalibrate the input actually makes minimal perplexity and accuracy differences. Input activation distributions are stable across different datasets - pretrained calibration generalizes well.\r\n\r\n### 2.5 The root cause for the difference CPT and progressive precision training\r\nThe Reason why Progressive training works, might be similar to the distillation effect: we train with higher bit-width to understand the structure better and then pass the more precise understanding to the next lower bit-width. But what the reason why CPT is better than progressive training in lower bit quantization is that \r\n\r\n- **Exploration matters more** than exploitation at low precision\r\n- Cycling provides **escape mechanisms** from bad local minima\r\n\r\n\r\n## 4.**Does this phenomenon align with the observations in CPT (ICLR‚Äô21)? If not, what could be the potential reasons?**\r\n\r\n- The CPT model does help with reducing the cost. Since traditional method required distillation from teacher bit, these model will store and process at least two times the number of lora layer of a single model. But for CPT model, we would only need to store the lora layer of a model once and change precision during the flight. : \r\n\r\n  | Num of params               | Cyclic Precision Training | Switchable Precision of |\r\n  | --------------------------- | ------------------------- | ----------------------- |\r\n  | Total parameters            | 166,212,880               | 257,097,984             |\r\n  | Frozen parameters           | 162,998,784 (98.1%)       | 124,977,408 (48.6%)     |\r\n  | Trainable (LoRA) parameters | 3,214,096 (1.9%)          | 132,120,576 (51.4%)     |\r\n\r\n\r\n- The CPT does helps with training to gain a higher accuracy, even though CPT's perplexity greatly degraded.\r\n\r\n|            | 5 bit model trained with CPT | 5 bit model trained with Switchable precision |\r\n| ---------- | ---------------------------- | --------------------------------------------- |\r\n| BoolQ      | 63.0                         | 57.4                                          |\r\n| HellaSwag  | 32.4                         | 33.4                                          |\r\n| WinoGrande | 52.4                         | 56.4                                          |\r\n| Average    | 49.3                         | 49.1                                          |\r\n\r\n\r\n\r\n## 5.**[Step 6] Does this phenomenon align with the observations in Double-Win Quant (ICML‚Äô21)? If not, what could be the potential reasons?**\r\n\r\nIt does show that \r\n\r\n- Poor Adversarial transferability\r\n- Random Precision Inference does works to increase the robustness\r\n\r\n### Model Metrics\r\n\r\n| Metric               | Value  |\r\n| -------------------- | ------ |\r\n| **Clean Accuracy**   | 27.66% |\r\n| **Clean Perplexity** | 74.01  |\r\n| **Clean Loss**       | 4.304  |\r\n\r\n### Attack Effectiveness\r\n\r\n| Attack Type     | Success Rate | Original Acc. | Adversarial Acc. | Accuracy Drop | Perturbation |\r\n| --------------- | ------------ | ------------- | ---------------- | ------------- | ------------ |\r\n| **TextFooler**  | 63.33%       | 26.95%        | 20.53%           | -6.41%        | 11.86%       |\r\n| **BERT-Attack** | 83.33%       | 26.95%        | 16.77%           | -10.18%       | 15.83%       |\r\n\r\n### RPI Implementation:\r\n\r\n| Switching Probability | TextFooler Defense | BERT-Attack Defense | Average |\r\n| --------------------- | ------------------ | ------------------- | ------- |\r\n| **0%** (Baseline)     | 0.0%               | 0.0%                | 0.0%    |\r\n| **30%**               | +36.8%             | +48.0%              | +42.4%  |\r\n| **50%**               | +47.4%             | +60.0%              | +53.7%  |\r\n| **70%**               | +47.4%             | +60.0%              | +53.7%  |\r\n\r\n\r\n\r\n## 6. *Based on your explorations of switchable and dynamic quantization, could you propose some promising research directions or questions for further integrating them with LLMs?**\r\n\r\n### 6. 1 Intentionally use **lower-precision layer normalization** than the current training bit-width to inject beneficial exploration noise, or maybe employ a cyclic schedule for precision switching.\r\n\r\nWould it be possible to use different precision batch norm for the current back propagating bit width? IN the CPT paper, the reason that lower bit width training actually improve on the accuracy is because *a lower precision that leads to a short-term poor accuracy might actually help the DNN exploration during training thanks to its associated larger quantization noise*. Then would it be possible to use a lower-than-current bit layer norm base on the CPT training (i.e. 4 bit layer norm for WA-6 / GE-8 and WA-7 / GE-8)\r\n\r\n### 6.2 Since we're not deploying yet (just training with fake quantization), why limit ourselves to **integer bit-widths**? Use **continuous, real-valued bit-widths** (e.g., 4.7 bits, 5.3 bits). Would it make CPT more powerful? Would it helps us to understand what happen on the boundary from 4 to 5 bits?\r\n\r\nFor the cyclic training method, since the quantization is essentially fake, and we are not talking about deployment right now, so can't we use real number quantization, instead of integer? What might a more continuous training schedule looks like?Precision-LR Coupling:** Is there an optimal mathematical relationship? **Robustness Boundaries:** How does training near precision boundaries affect model resilience?\r\n\r\n### 6.3 Equavalence of Learning Rate Schedule and the Precision Schedule\r\nLearning rate schedule is adjusting the step for each optimization, and think about the way CPT works, switching precisions between epochs. They looks quite different at the first glance, but the thing is that they are both changing optimizing steps. For LR schedule, it might be changing the step in a linear way, but precision backward basically integrate part of the sematic meaning (per Channel calibration ) into the optimizing stepping. Would there's be a map from the precision switching schedule to the LR schedule, if so, can we literally replace the LR schedule and replace this with the CPT with better sematic interpretation?\r\n\r\nCould it be something like\r\n- 4-bit training ‚âà high_lr * gradient + large_noise\r\n- 8-bit training ‚âà medium_lr * gradient + medium_noise  \r\n- 16-bit training ‚âà low_lr * gradient + small_noise","filePath":"src/content/blogs/EIC_interview_report.md","digest":"5747cced2a2eec8c","rendered":{"html":"<h1 id=\"report-on-quantization-strategy-on-switchable-precision-and-cyclic-precision-training\">Report on Quantization Strategy on Switchable Precision and Cyclic Precision Training</h1>\n<p><em>// Because the link to the squad in the pdf can‚Äôt find anything: <a href=\"https://huggingface.co/Squad/datasets\">https://huggingface.co/Squad/datasets</a>, therefore, I switched to a text generative dataset wikitext 103 for its ability on quantization</em></p>\n<h1 id=\"0-performance-achieved-after-applying-various-quantization-bit-width-configurations-to-the-wikitext-103-dataset\">0. Performance achieved after applying various quantization bit-width configurations to the wikitext-103 dataset.</h1>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th></th><th>WikiText103 perplexity</th><th>BoolQ</th><th>HellaSwag</th><th>WinoGrande</th><th>Accuracy  Average</th></tr></thead><tbody><tr><td>4-bit</td><td>54.5</td><td>61.2</td><td>30.2</td><td>53.2</td><td>47.8</td></tr><tr><td>5-bit</td><td>30.8</td><td>61.8</td><td>33.4</td><td>48.2</td><td>47.8</td></tr><tr><td>16-bit</td><td>26.9</td><td>58.8</td><td>33.2</td><td>51.6</td><td>47.9</td></tr><tr><td>32-bit</td><td>26.9</td><td>59.0</td><td>32.2</td><td>53.2</td><td>48.1</td></tr></tbody></table>\n<p>All of them are trained with gradual bit width configurations.</p>\n<h2 id=\"1-questions-how-did-you-determine-the-optimal-quantization-bit-width-configurations-have-you-gleaned-any-insights-from-your-observations-that-could-guide-future-work-to-further-enhance-performance\">1. **Questions: How did you determine the optimal quantization bit-width configurations? Have you gleaned any insights from your observations that could guide future work to further enhance performance? **</h2>\n<p>Determining the <strong>optimal configuration</strong> requires clearly defining what ‚Äúoptimal‚Äù means in relation to specific <strong>evaluation benchmarks</strong> and performance metrics. For this report, we define optimal performance based on two primary criteria: perplexity scores and zero-shot accuracy. Several critical factors influence the optimization of model performance:</p>\n<h3 id=\"11-choice-of-quantizer-base-on-the-current-precision\">1.1 <strong>Choice of quantizer base on the current precision</strong>:</h3>\n<p>The choice of quantization method significantly impacts model performance and varies with the target bit width:</p>\n<ul>\n<li><strong>For 4-bit or lower precision</strong>: MinMax quantization delivers the best results</li>\n<li><strong>For 5-bit or higher precision</strong>: Advanced methods like log quantization and tanh quantization provide superior performance</li>\n</ul>\n<p>My experiments reveal that log quantization reduces 8-bit model degradation to just 2%, compared to 15% degradation with MinMax quantization. This improvement likely occurs because GPT-2 Small has relatively limited information content and less pronounced outlier effects, allowing log quantization to outperform the MinMax clipping function in case of higher precisions.  Here‚Äôs my experiments results on different approaches of changing the quantization strategies.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th></th><th>4 bit with Minmax Quantizer</th><th>4 bit with Log Quantizer</th><th>5 bit with Minmax Quantizer</th><th>5 bit with Log Quantizer</th></tr></thead><tbody><tr><td>perplexity on WikiText2</td><td>53.2</td><td>56.5</td><td>40.5</td><td>35.1</td></tr><tr><td>perplexity on WikiText103</td><td>44.5</td><td>48.4</td><td>33.6</td><td>29.3</td></tr><tr><td>BoolQ</td><td>57.0</td><td>59.0</td><td>52.6</td><td>57.4</td></tr><tr><td>HellaSwag</td><td>32.0</td><td>30.2</td><td>32.8</td><td>33.4</td></tr><tr><td>WinoGrande</td><td>54.6</td><td>48.2</td><td>51.0</td><td>56.4</td></tr><tr><td>Zeroshot average</td><td>47.9</td><td>45.8</td><td>45.5</td><td>49.1</td></tr></tbody></table>\n<p><strong>Important Notes:</strong></p>\n<ul>\n<li>C4 perplexity metrics were excluded from this analysis since the models were fine-tuned specifically on the WikiText dataset, making C4 comparisons less meaningful</li>\n</ul>\n<h3 id=\"12--training-precision-distribution\">1.2  Training precision distribution</h3>\n<h3 id=\"121-on-accumulative-temperature-scaled-soft-max-distillation-method\">1.2.1 On accumulative temperature scaled soft max distillation method</h3>\n<p>During the training process of switchable precision model, the self distillation formula is as follows:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>L</mi><mi>q</mi></msub><mo>=</mo><msub><mi>Œ±</mi><mn>1</mn></msub><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>S</mi><mi>G</mi><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">‚à£</mi><mi mathvariant=\"normal\">‚à£</mi><msub><mi>p</mi><mrow><mi>q</mi><mi>q</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>+</mo><msub><mi>Œ±</mi><mn>2</mn></msub><munder><mo>‚àë</mo><mrow><mi>i</mi><mtext>¬†</mtext><mi>i</mi><mi>n</mi><mtext>¬†</mtext><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi></mrow></munder><mi mathvariant=\"script\">‚à£</mi><mi mathvariant=\"normal\">‚à£</mi><msub><mi>f</mi><mrow><mi>i</mi><mi>r</mi></mrow></msub><mo>‚àí</mo><msub><mi>f</mi><mrow><mi>i</mi><mi>q</mi></mrow></msub><mi mathvariant=\"normal\">‚à£</mi><msubsup><mi mathvariant=\"normal\">‚à£</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">L_q = \\alpha_1 D_{KL}(SG(p_r)||p_{qq}) + \\alpha_2 \\sum_{i\\ in\\ feature} \\mathcal||f_{ir} - f_{iq}||_2^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">Œ±</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">SG</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">‚à£‚à£</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">qq</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4882em;vertical-align:-1.4382em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">Œ±</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.05em;\"><span style=\"top:-1.8479em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mspace mtight\"><span class=\"mtight\">¬†</span></span><span class=\"mord mathnormal mtight\">in</span><span class=\"mspace mtight\"><span class=\"mtight\">¬†</span></span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathnormal mtight\">e</span><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">t</span><span class=\"mord mathnormal mtight\">u</span><span class=\"mord mathnormal mtight\">re</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">‚àë</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.4382em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">‚à£‚à£</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">‚àí</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1502em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\">‚à£</span><span class=\"mord\"><span class=\"mord\">‚à£</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641em;\"><span style=\"top:-2.453em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>p</mi><mi>r</mi></msub></mrow><annotation encoding=\"application/x-tex\">p_{r}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> is the unquantized output distribution, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>p</mi><mrow><mi>q</mi><mi>q</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">p_{qq}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">qq</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span> is the quantized output distribution.</p>\n<ul>\n<li>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th></th><th>4 bit model</th><th>5 bit model</th></tr></thead><tbody><tr><td>perplexity on WikiText2</td><td>56.5</td><td>35.1</td></tr><tr><td>perplexity on WikiText103</td><td>48.4</td><td>29.3</td></tr><tr><td>BoolQ</td><td>59.0</td><td>57.4</td></tr><tr><td>HellaSwag</td><td>30.2</td><td>33.4</td></tr><tr><td>WinoGrande</td><td>48.2</td><td>56.4</td></tr><tr><td>Zeroshot average</td><td>45.8</td><td>49.1</td></tr></tbody></table>\n</li>\n</ul>\n<p>‚Äã\t<strong>All of them are using the log quantizer.</strong></p>\n<ul>\n<li>Activation bits changes accordingly to the current bit width during the training.</li>\n</ul>\n<h3 id=\"122-on-non-accumulative-temperature-scaled-soft-max-distillation-method-used-by-the-switchable-precision-paper\">1.2.2 On non-accumulative temperature scaled soft max distillation method (used by the switchable precision paper)</h3>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>L</mi><mi>q</mi></msub><mo>=</mo><msub><mi>Œ±</mi><mn>1</mn></msub><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>S</mi><mi>G</mi><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">‚à£</mi><mi mathvariant=\"normal\">‚à£</mi><msub><mi>p</mi><mrow><mi>q</mi><mi>q</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>+</mo><msub><mi>Œ±</mi><mn>2</mn></msub><mi mathvariant=\"script\">‚à£</mi><mi mathvariant=\"normal\">‚à£</mi><msub><mi>f</mi><mi>r</mi></msub><mo>‚àí</mo><msub><mi>f</mi><mi>q</mi></msub><mi mathvariant=\"normal\">‚à£</mi><msubsup><mi mathvariant=\"normal\">‚à£</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">L_q = \\alpha_1 D_{KL}(SG(p_r)||p_{qq}) + \\alpha_2 \\mathcal||f_r - f_{q}||_2^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">Œ±</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal mtight\">L</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">SG</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">‚à£‚à£</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">qq</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">Œ±</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">‚à£‚à£</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">‚àí</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1502em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\">‚à£</span><span class=\"mord\"><span class=\"mord\">‚à£</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641em;\"><span style=\"top:-2.453em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">‚Äã</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>I also investigated a stochastic distillation variant where feature layers are randomly selected from the hidden states during training. Surprisingly, despite receiving guidance from only a subset of attention layers rather than all layers, the model‚Äôs accuracy improved rather than deteriorated, while the perplexity decreases as expected:</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th></th><th>4 bit self distillation method with teaching from all the hidden states</th><th>4 bit self distillation method with teaching from a random hidden state</th></tr></thead><tbody><tr><td>perplexity on WikiText2</td><td>56.5</td><td>67.6</td></tr><tr><td>perplexity on WikiText103</td><td>48.4</td><td>dc ..68.8</td></tr><tr><td>BoolQ</td><td>59.0</td><td>60.0</td></tr><tr><td>HellaSwag</td><td>30.2</td><td>31.8</td></tr><tr><td>WinoGrande</td><td>48.2</td><td>49.8</td></tr><tr><td>Zeroshot average</td><td>45.8</td><td>47.2</td></tr></tbody></table>\n<p>These results raise an important question: which specific information within the hidden layers contributes most significantly to accuracy improvements? Understanding this could help optimize the distillation process further.</p>\n<h2 id=\"2-interesting-insight-on-optimizing-the-model-performance\">2. Interesting insight on optimizing the model performance</h2>\n<h3 id=\"21-16-bits-is-extremely-similar-to-the-32-bits\">2.1 16 bits is extremely similar to the 32 bits</h3>\n<p>Calibration logic need to be recalibrated after series of grad accumulation. With no calibration, <strong>16 bit has the same perplexity(0.8% degradation) as 32 bit</strong>, the degradation from not quantization is minimal.</p>\n<h3 id=\"22-merge-then-quant-or-the-other-way-around\">2.2 Merge then quant or the other way around?</h3>\n<p>For lora, would it be better performing if we apply the AB and then do the quantization or do the quantization after AB? <a href=\"https://kaitchup.substack.com/p/training-loading-and-merging-qdora\">Experiment from a blogger</a>  The inference time is so much longer, which is due to the huge tensor for dequantization. Meanwhile, I would think that it would take longer to train since the big tensor is harder to calibrate.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th></th><th>Merge after Quant: Q(W) + Q(A @ B)</th><th>Merge after Quant: Q(W) + Q(A )@ Q(B)</th></tr></thead><tbody><tr><td>Inference Speed</td><td>Faster</td><td>Slower</td></tr><tr><td>Accuracy</td><td>Same</td><td>Same</td></tr><tr><td>Calibration speed</td><td>Faster ( by a bit)</td><td>Slower</td></tr></tbody></table>\n<h3 id=\"23-per-channel--per-feature-or-per-token\">2.3 Per channel , per feature or per token ?</h3>\n<p>My hypothesis is that quantization calibration should align with the <strong>semantic boundaries</strong> of tensor dimensions. Different layers encode information differently, and quantization should preserve these natural groupings. (But from the testing experiences, it seems that it actually not matter as much tho.) For Embedded layers, per channel calibration should be optimal, and for other layers like projection layers and those attention layers, per token calibration should be more optimal.</p>\n<h3 id=\"24-unexpected-minimal-impact-during-debugging\">2.4 <strong>Unexpected minimal impact during debugging</strong>:</h3>\n<ul>\n<li>It just incredible that by changing the FP32 weight bit quantization for 32 bits to the matching precision reduce the perplexity from about 120 to 30 (approximately, since I was debugging the code).</li>\n<li>During the evaluation with input activation quantization, using pretrained input scale and zero point or recalibrate the input actually makes minimal perplexity and accuracy differences. Input activation distributions are stable across different datasets - pretrained calibration generalizes well.</li>\n</ul>\n<h3 id=\"25-the-root-cause-for-the-difference-cpt-and-progressive-precision-training\">2.5 The root cause for the difference CPT and progressive precision training</h3>\n<p>The Reason why Progressive training works, might be similar to the distillation effect: we train with higher bit-width to understand the structure better and then pass the more precise understanding to the next lower bit-width. But what the reason why CPT is better than progressive training in lower bit quantization is that</p>\n<ul>\n<li><strong>Exploration matters more</strong> than exploitation at low precision</li>\n<li>Cycling provides <strong>escape mechanisms</strong> from bad local minima</li>\n</ul>\n<h2 id=\"4does-this-phenomenon-align-with-the-observations-in-cpt-iclr21-if-not-what-could-be-the-potential-reasons\">4.<strong>Does this phenomenon align with the observations in CPT (ICLR‚Äô21)? If not, what could be the potential reasons?</strong></h2>\n<ul>\n<li>\n<p>The CPT model does help with reducing the cost. Since traditional method required distillation from teacher bit, these model will store and process at least two times the number of lora layer of a single model. But for CPT model, we would only need to store the lora layer of a model once and change precision during the flight. :</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Num of params</th><th>Cyclic Precision Training</th><th>Switchable Precision of</th></tr></thead><tbody><tr><td>Total parameters</td><td>166,212,880</td><td>257,097,984</td></tr><tr><td>Frozen parameters</td><td>162,998,784 (98.1%)</td><td>124,977,408 (48.6%)</td></tr><tr><td>Trainable (LoRA) parameters</td><td>3,214,096 (1.9%)</td><td>132,120,576 (51.4%)</td></tr></tbody></table>\n</li>\n<li>\n<p>The CPT does helps with training to gain a higher accuracy, even though CPT‚Äôs perplexity greatly degraded.</p>\n</li>\n</ul>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th></th><th>5 bit model trained with CPT</th><th>5 bit model trained with Switchable precision</th></tr></thead><tbody><tr><td>BoolQ</td><td>63.0</td><td>57.4</td></tr><tr><td>HellaSwag</td><td>32.4</td><td>33.4</td></tr><tr><td>WinoGrande</td><td>52.4</td><td>56.4</td></tr><tr><td>Average</td><td>49.3</td><td>49.1</td></tr></tbody></table>\n<h2 id=\"5step-6-does-this-phenomenon-align-with-the-observations-in-double-win-quant-icml21-if-not-what-could-be-the-potential-reasons\">5.<strong>[Step 6] Does this phenomenon align with the observations in Double-Win Quant (ICML‚Äô21)? If not, what could be the potential reasons?</strong></h2>\n<p>It does show that</p>\n<ul>\n<li>Poor Adversarial transferability</li>\n<li>Random Precision Inference does works to increase the robustness</li>\n</ul>\n<h3 id=\"model-metrics\">Model Metrics</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody><tr><td><strong>Clean Accuracy</strong></td><td>27.66%</td></tr><tr><td><strong>Clean Perplexity</strong></td><td>74.01</td></tr><tr><td><strong>Clean Loss</strong></td><td>4.304</td></tr></tbody></table>\n<h3 id=\"attack-effectiveness\">Attack Effectiveness</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Attack Type</th><th>Success Rate</th><th>Original Acc.</th><th>Adversarial Acc.</th><th>Accuracy Drop</th><th>Perturbation</th></tr></thead><tbody><tr><td><strong>TextFooler</strong></td><td>63.33%</td><td>26.95%</td><td>20.53%</td><td>-6.41%</td><td>11.86%</td></tr><tr><td><strong>BERT-Attack</strong></td><td>83.33%</td><td>26.95%</td><td>16.77%</td><td>-10.18%</td><td>15.83%</td></tr></tbody></table>\n<h3 id=\"rpi-implementation\">RPI Implementation:</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Switching Probability</th><th>TextFooler Defense</th><th>BERT-Attack Defense</th><th>Average</th></tr></thead><tbody><tr><td><strong>0%</strong> (Baseline)</td><td>0.0%</td><td>0.0%</td><td>0.0%</td></tr><tr><td><strong>30%</strong></td><td>+36.8%</td><td>+48.0%</td><td>+42.4%</td></tr><tr><td><strong>50%</strong></td><td>+47.4%</td><td>+60.0%</td><td>+53.7%</td></tr><tr><td><strong>70%</strong></td><td>+47.4%</td><td>+60.0%</td><td>+53.7%</td></tr></tbody></table>\n<h2 id=\"6-based-on-your-explorations-of-switchable-and-dynamic-quantization-could-you-propose-some-promising-research-directions-or-questions-for-further-integrating-them-with-llms\">6. <em>Based on your explorations of switchable and dynamic quantization, could you propose some promising research directions or questions for further integrating them with LLMs?</em>*</h2>\n<h3 id=\"6-1-intentionally-use-lower-precision-layer-normalization-than-the-current-training-bit-width-to-inject-beneficial-exploration-noise-or-maybe-employ-a-cyclic-schedule-for-precision-switching\">6. 1 Intentionally use <strong>lower-precision layer normalization</strong> than the current training bit-width to inject beneficial exploration noise, or maybe employ a cyclic schedule for precision switching.</h3>\n<p>Would it be possible to use different precision batch norm for the current back propagating bit width? IN the CPT paper, the reason that lower bit width training actually improve on the accuracy is because <em>a lower precision that leads to a short-term poor accuracy might actually help the DNN exploration during training thanks to its associated larger quantization noise</em>. Then would it be possible to use a lower-than-current bit layer norm base on the CPT training (i.e. 4 bit layer norm for WA-6 / GE-8 and WA-7 / GE-8)</p>\n<h3 id=\"62-since-were-not-deploying-yet-just-training-with-fake-quantization-why-limit-ourselves-to-integer-bit-widths-use-continuous-real-valued-bit-widths-eg-47-bits-53-bits-would-it-make-cpt-more-powerful-would-it-helps-us-to-understand-what-happen-on-the-boundary-from-4-to-5-bits\">6.2 Since we‚Äôre not deploying yet (just training with fake quantization), why limit ourselves to <strong>integer bit-widths</strong>? Use <strong>continuous, real-valued bit-widths</strong> (e.g., 4.7 bits, 5.3 bits). Would it make CPT more powerful? Would it helps us to understand what happen on the boundary from 4 to 5 bits?</h3>\n<p>For the cyclic training method, since the quantization is essentially fake, and we are not talking about deployment right now, so can‚Äôt we use real number quantization, instead of integer? What might a more continuous training schedule looks like?Precision-LR Coupling:** Is there an optimal mathematical relationship? <strong>Robustness Boundaries:</strong> How does training near precision boundaries affect model resilience?</p>\n<h3 id=\"63-equavalence-of-learning-rate-schedule-and-the-precision-schedule\">6.3 Equavalence of Learning Rate Schedule and the Precision Schedule</h3>\n<p>Learning rate schedule is adjusting the step for each optimization, and think about the way CPT works, switching precisions between epochs. They looks quite different at the first glance, but the thing is that they are both changing optimizing steps. For LR schedule, it might be changing the step in a linear way, but precision backward basically integrate part of the sematic meaning (per Channel calibration ) into the optimizing stepping. Would there‚Äôs be a map from the precision switching schedule to the LR schedule, if so, can we literally replace the LR schedule and replace this with the CPT with better sematic interpretation?</p>\n<p>Could it be something like</p>\n<ul>\n<li>4-bit training ‚âà high_lr * gradient + large_noise</li>\n<li>8-bit training ‚âà medium_lr * gradient + medium_noise</li>\n<li>16-bit training ‚âà low_lr * gradient + small_noise</li>\n</ul>","metadata":{"headings":[{"depth":1,"slug":"report-on-quantization-strategy-on-switchable-precision-and-cyclic-precision-training","text":"Report on Quantization Strategy on Switchable Precision and Cyclic Precision Training"},{"depth":1,"slug":"0-performance-achieved-after-applying-various-quantization-bit-width-configurations-to-the-wikitext-103-dataset","text":"0. Performance achieved after applying various quantization bit-width configurations to the wikitext-103 dataset."},{"depth":2,"slug":"1-questions-how-did-you-determine-the-optimal-quantization-bit-width-configurations-have-you-gleaned-any-insights-from-your-observations-that-could-guide-future-work-to-further-enhance-performance","text":"1. **Questions: How did you determine the optimal quantization bit-width configurations? Have you gleaned any insights from your observations that could guide future work to further enhance performance? **"},{"depth":3,"slug":"11-choice-of-quantizer-base-on-the-current-precision","text":"1.1 Choice of quantizer base on the current precision:"},{"depth":3,"slug":"12--training-precision-distribution","text":"1.2  Training precision distribution"},{"depth":3,"slug":"121-on-accumulative-temperature-scaled-soft-max-distillation-method","text":"1.2.1 On accumulative temperature scaled soft max distillation method"},{"depth":3,"slug":"122-on-non-accumulative-temperature-scaled-soft-max-distillation-method-used-by-the-switchable-precision-paper","text":"1.2.2 On non-accumulative temperature scaled soft max distillation method (used by the switchable precision paper)"},{"depth":2,"slug":"2-interesting-insight-on-optimizing-the-model-performance","text":"2. Interesting insight on optimizing the model performance"},{"depth":3,"slug":"21-16-bits-is-extremely-similar-to-the-32-bits","text":"2.1 16 bits is extremely similar to the 32 bits"},{"depth":3,"slug":"22-merge-then-quant-or-the-other-way-around","text":"2.2 Merge then quant or the other way around?"},{"depth":3,"slug":"23-per-channel--per-feature-or-per-token","text":"2.3 Per channel , per feature or per token ?"},{"depth":3,"slug":"24-unexpected-minimal-impact-during-debugging","text":"2.4 Unexpected minimal impact during debugging:"},{"depth":3,"slug":"25-the-root-cause-for-the-difference-cpt-and-progressive-precision-training","text":"2.5 The root cause for the difference CPT and progressive precision training"},{"depth":2,"slug":"4does-this-phenomenon-align-with-the-observations-in-cpt-iclr21-if-not-what-could-be-the-potential-reasons","text":"4.Does this phenomenon align with the observations in CPT (ICLR‚Äô21)? If not, what could be the potential reasons?"},{"depth":2,"slug":"5step-6-does-this-phenomenon-align-with-the-observations-in-double-win-quant-icml21-if-not-what-could-be-the-potential-reasons","text":"5.[Step 6] Does this phenomenon align with the observations in Double-Win Quant (ICML‚Äô21)? If not, what could be the potential reasons?"},{"depth":3,"slug":"model-metrics","text":"Model Metrics"},{"depth":3,"slug":"attack-effectiveness","text":"Attack Effectiveness"},{"depth":3,"slug":"rpi-implementation","text":"RPI Implementation:"},{"depth":2,"slug":"6-based-on-your-explorations-of-switchable-and-dynamic-quantization-could-you-propose-some-promising-research-directions-or-questions-for-further-integrating-them-with-llms","text":"6. Based on your explorations of switchable and dynamic quantization, could you propose some promising research directions or questions for further integrating them with LLMs?*"},{"depth":3,"slug":"6-1-intentionally-use-lower-precision-layer-normalization-than-the-current-training-bit-width-to-inject-beneficial-exploration-noise-or-maybe-employ-a-cyclic-schedule-for-precision-switching","text":"6. 1 Intentionally use lower-precision layer normalization than the current training bit-width to inject beneficial exploration noise, or maybe employ a cyclic schedule for precision switching."},{"depth":3,"slug":"62-since-were-not-deploying-yet-just-training-with-fake-quantization-why-limit-ourselves-to-integer-bit-widths-use-continuous-real-valued-bit-widths-eg-47-bits-53-bits-would-it-make-cpt-more-powerful-would-it-helps-us-to-understand-what-happen-on-the-boundary-from-4-to-5-bits","text":"6.2 Since we‚Äôre not deploying yet (just training with fake quantization), why limit ourselves to integer bit-widths? Use continuous, real-valued bit-widths (e.g., 4.7 bits, 5.3 bits). Would it make CPT more powerful? Would it helps us to understand what happen on the boundary from 4 to 5 bits?"},{"depth":3,"slug":"63-equavalence-of-learning-rate-schedule-and-the-precision-schedule","text":"6.3 Equavalence of Learning Rate Schedule and the Precision Schedule"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Quantization Strategy Report: Switchable and Cyclic Precision Training","description":"Comprehensive report on quantization strategies including switchable precision and cyclic precision training applied to WikiText-103 dataset","pubDate":"2025-01-10T00:00:00.000Z","author":"Xiaoyou Wu","tags":["quantization","machine-learning","precision-training","wikitext","model-compression"]},"imagePaths":[]}},"collection":"blogs","slug":"eic_interview_report"},{"id":"VIP_Progress.md","data":{"title":"VIP Progress Journal - SystemVerilog Learning","description":"Weekly progress journal documenting my journey learning SystemVerilog, including module structures, always blocks, and FSM implementations","pubDate":"2024-08-25T00:00:00.000Z","author":"Xiaoyou Wu","tags":["SystemVerilog","hardware","FPGA","digital-design","learning-journal"]},"body":"## Week 1: Aug 25 - Aug 31\r\n\r\n### Foundation & Basic Structures\r\n\r\n#### Module Structure\r\n```systemverilog\r\nmodule ModuleName (\r\n    // Input signals\r\n    input logic signal_name,\r\n    input logic [31:0] bus_signal,\r\n\r\n    // Output signals\r\n    output logic output_signal,\r\n    output logic [4:0] output_bus\r\n);\r\n    // Module body\r\nendmodule\r\n```\r\n\r\n**Remember:**\r\n- `logic` replaces wire/reg\r\n- Bus width: `[MSB:LSB]` before signal name\r\n- Semicolon after port list\r\n\r\n#### Data Types\r\n```systemverilog\r\n// Enumerated types with explicit width\r\ntypedef enum logic [2:0] {\r\n    NO_WRITEBACK = 3'b000,\r\n    READ_ALU_RESULT = 3'b001,\r\n    READ_MEM_RESULT = 3'b010\r\n} write_back_mux_selector;\r\n\r\n// Localparam for compile-time constants\r\nlocalparam INSTR_START_PC = 0;\r\nlocalparam DATA_START_PC = 127;\r\n\r\n// Parameter extraction using bit slicing\r\nlocalparam REG_S1_MSB = 19;\r\nlocalparam REG_S1_LSB = 15;\r\n```\r\n\r\n**Number Formats:**\r\n```systemverilog\r\n7'h23    // 7-bit hex value\r\n3'b001   // 3-bit binary value\r\n32'bz    // 32-bit high-impedance\r\n8'h00    // 8-bit hex (for byte operations)\r\n```\r\n\r\n#### Procedural Blocks\r\n\r\n**Combinational - always_comb**\r\n```systemverilog\r\nalways_comb begin\r\n    case(alu_operator_ip)\r\n        ALU_ADD: begin\r\n            alu_result_op = alu_operand_a_ip + alu_operand_b_ip;\r\n            alu_valid_op = 1;\r\n        end\r\n        ALU_SUB: begin\r\n            alu_result_op = alu_operand_a_ip - alu_operand_b_ip;\r\n            alu_valid_op = 1;\r\n        end\r\n        default: begin\r\n            alu_result_op = 0;\r\n            alu_valid_op = 0;\r\n        end\r\n    endcase\r\nend\r\n```\r\n\r\n**Sequential - always @(posedge clk)**\r\n```systemverilog\r\nalways @(posedge clk) begin\r\n    if (~reset)\r\n        cycle_count <= cycle_count + 1;\r\nend\r\n```\r\n\r\n---\r\n\r\n## Week 2: Sep 1 - Sep 7\r\n### Advanced Patterns & Pipeline Design\r\n\r\n#### Packages\r\n```systemverilog\r\npackage CORE_PKG;\r\n    // Parameters\r\n    parameter OPCODE_STORE = 7'h23;\r\n\r\n    // Enumerations\r\n    typedef enum logic [6:0] {\r\n        ALU_NOP = 7'b0000000,\r\n        ALU_ADD = 7'b0011000\r\n    } alu_opcode_e;\r\n\r\n    // Custom types\r\n    typedef enum {\r\n        REG_A, PC, OPA_NOP\r\n    } operand_a_mux;\r\nendpackage\r\n```\r\n\r\n**Import:**\r\n```systemverilog\r\nimport CORE_PKG::*;\r\n```\r\n\r\n#### Pipeline Patterns\r\n\r\n**Signal Naming Convention**\r\n```systemverilog\r\n// Pass-through signals use _pt suffix\r\nlogic [31:0] id_uimmd_pt;\r\nlogic [31:0] ex_uimmd_pt;\r\nlogic [31:0] lsu_uimmd_pt;\r\n\r\n// Stage outputs use _op suffix\r\noutput logic [31:0] alu_result_op;\r\noutput logic alu_valid_op;\r\n\r\n// Stage inputs use _ip suffix\r\ninput logic [31:0] alu_operand_a_ip;\r\ninput logic alu_enable_ip;\r\n```\r\n\r\n**Pipeline Buffers**\r\n```systemverilog\r\n// ID to EX pipeline buffer\r\nlogic [31:0] id_instr_pc_addr_pt;\r\nlogic [31:0] ex_instr_pc_addr_pt;\r\n\r\n// EX to MEM pipeline buffer\r\nlogic [31:0] ex_alu_result_pt;\r\nlogic ex_alu_result_valid_pt;\r\n```\r\n\r\n**Pipeline Control**\r\n```systemverilog\r\ninput logic flush_en;   // Branch flush\r\noutput logic stall_op;  // Hazard stall\r\n```\r\n\r\n#### RISC-V Patterns\r\n\r\n**Opcodes**\r\n```systemverilog\r\nparameter OPCODE_STORE = 7'h23;   // S-type\r\nparameter OPCODE_LOAD = 7'h03;    // I-type\r\nparameter OPCODE_BRANCH = 7'h63;  // B-type\r\nparameter OPCODE_JAL = 7'h6f;     // J-type\r\nparameter OPCODE_JALR = 7'h67;\r\nparameter OPCODE_LUI = 7'h37;     // U-type\r\nparameter OPCODE_AUIPC = 7'h17;\r\nparameter OPCODE_OP = 7'h33;      // R-type\r\nparameter OPCODE_OPIMM = 7'h13;\r\n```\r\n\r\n**Field Extraction**\r\n```systemverilog\r\nlogic [6:0] opcode = instruction[6:0];\r\nlogic [4:0] rd = instruction[11:7];\r\nlogic [2:0] funct3 = instruction[14:12];\r\nlogic [4:0] rs1 = instruction[19:15];\r\nlogic [4:0] rs2 = instruction[24:20];\r\nlogic [6:0] funct7 = instruction[31:25];\r\n```\r\n\r\n---\r\n\r\n### Interesting Knowledge base on my previous project\r\n\r\n## Week 3: Sep 8 - Sep 14\r\n\r\n### Testing, Synthesis & Optimization\r\n\r\n#### Testbench\r\n\r\n**Clock & Timing**\r\n```systemverilog\r\n`timescale 1ns / 1ns\r\n\r\nmodule Core_tb;\r\n    logic clk = 1;\r\n    always #1 clk <= clk + 1;\r\n\r\n    initial begin\r\n        reset = 1'b1;\r\n        #6 reset = 1'b0;\r\n        #50 $finish;\r\n    end\r\n```\r\n\r\n**Memory Init**\r\n```systemverilog\r\ncore_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[0] = 8'h00;\r\ncore_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[1] = 8'h00;\r\ncore_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[2] = 8'h00;\r\ncore_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[3] = 8'h00;\r\n```\r\n\r\n**Waveform Dump**\r\n```systemverilog\r\n$dumpfile(\"Core_Simulation.vcd\");\r\n$dumpvars(0, Core_tb);\r\n```\r\n\r\n#### Tricks\r\n\r\n**1. Ternary Operator**\r\n```systemverilog\r\nassign valid_instr_to_decode = instr_data_valid_ip ? instr_data_ip : 32'bz;\r\nalu_result_op = ($signed(alu_operand_a_ip) < $signed(alu_operand_b_ip)) ? 1 : 0;\r\n```\r\n\r\n**2. Prevent Latches**\r\n```systemverilog\r\nalways_comb begin\r\n    // Set defaults first\r\n    alu_operator = ALU_NOP;\r\n    operand_a_select = OPA_NOP;\r\n    writeback_mux = NO_WRITEBACK;\r\n\r\n    // Then override based on conditions\r\n    case(opcode)\r\n        // ...\r\n    endcase\r\nend\r\n```\r\n\r\n**3. Hierarchical Access**\r\n```systemverilog\r\ncore_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[0]\r\n```\r\n\r\n**4. Module Instance**\r\n```systemverilog\r\nCore core_proc(\r\n    .clock(clk),\r\n    .reset(reset),\r\n    .mem_en(mem_enable)\r\n);\r\n```\r\n\r\n**5. Case with Default**\r\n```systemverilog\r\ncase(selector)\r\n    VALUE1: action1();\r\n    VALUE2: action2();\r\n    default: begin\r\n        default_action();\r\n    end\r\nendcase\r\n```\r\n\r\n#### Synthesis Rules\r\n\r\n**1. Assignment Types**\r\n```systemverilog\r\nalways @(posedge clk) begin\r\n    signal <= new_value;  // Non-blocking in sequential\r\nend\r\n\r\nalways_comb begin\r\n    signal = input_value;  // Blocking in combinational\r\nend\r\n```\r\n\r\n**2. Sensitivity Lists**\r\n```systemverilog\r\nalways_comb begin  // Auto-sensitive to all RHS\r\nend\r\n```\r\n\r\n**3. No Latches**\r\n- Assign in all branches\r\n- Default case mandatory\r\n- Initialize outputs\r\n\r\n#### Debug\r\n\r\n**1. Display Messages**\r\n```systemverilog\r\n$display(\"Time: %0t, PC: %h, Instruction: %h\", $time, pc, instruction);\r\n```\r\n\r\n**2. Assertions**\r\n```systemverilog\r\nassert(condition) else $error(\"Assertion failed at time %0t\", $time);\r\n```\r\n\r\n**3. Generate Loops**\r\n```systemverilog\r\ngenvar i;\r\ngenerate\r\n    for (i = 0; i < 32; i++) begin : gen_loop\r\n        assign bit_out[i] = bit_in[i] & enable;\r\n    end\r\nendgenerate\r\n```\r\n\r\n**4. Parameterized Width**\r\n```systemverilog\r\nmodule GenericModule #(\r\n    parameter WIDTH = 32\r\n)(\r\n    input logic [WIDTH-1:0] data_in,\r\n    output logic [WIDTH-1:0] data_out\r\n);\r\n```\r\n\r\n**5. Width-Specified Constants**\r\n```systemverilog\r\nlogic [31:0] data = 32'h0000_0000;  // Underscores for readability\r\nlogic [6:0] opcode = 7'b011_0011;\r\n```\r\n\r\n**6. Forward Declaration**\r\n\r\n```systemverilog\r\nlogic [31:0] internal_signal;\r\nlogic control_signal;\r\n\r\nalways_comb begin\r\n    internal_signal = input_signal + offset;\r\nend\r\n```\r\n\r\n**7. Auto-sensitivity - always @(*)**\r\n\r\n```systemverilog\r\nalways @(*) begin\r\n    alu_operator = ALU_NOP; // Default assignment\r\n    operand_a_select = OPA_NOP;\r\nend\r\n```\r\n\r\n**8. Signed Compare**\r\n\r\n```systemverilog\r\n$signed(operand_a) < $signed(operand_b)  // Treat as signed values\r\n```\r\n\r\n## Week 3: Sep 15 - Sep 17\r\n\r\nWe decide upon the part for the add-on board\r\n\r\nFor this current board, I think I might design a four layer board. I have to correctly consider the WIFI module SPI and the level shifter, and do some entry level impedance matching.\r\n\r\n### Common terms in selecting the part choice:\r\n\r\n- **RoHS** stands for the **Restriction of Hazardous Substances**. It's a directive from the European Union that restricts the use of specific hazardous materials in electrical and electronic products\r\n\r\n### Part choice\r\n\r\n1. UJC-HP-3-SMT-TR\r\n\r\n   - **UJC**\r\n\r\n     - **Meaning:** This is the product series prefix. It stands for **USB Jack Connector**.\r\n\r\n     **HP**\r\n\r\n     - **Meaning:** This often specifies a particular feature, like **High Power** or **High Performance**. For a modern USB-C jack, this likely indicates it's designed to handle the higher power delivery standards for fast charging.\r\n\r\n     **3**\r\n\r\n     - **Meaning:** This number typically refers to the **USB standard** the connector is designed for. In this case, it indicates compatibility with **USB 3.x** specifications (like USB 3.1 or 3.2), which support higher data transfer speeds than older USB 2.0.\r\n\r\n     **TR**\r\n\r\n     - **Meaning:** This is a packaging designator that stands for **Tape and Reel**. The components are packaged in a long, pocketed tape wound onto a reel. \r\n\r\n2. ATWINC1500 \r\n\r\n   - **AT:** This prefix often indicates the part originated from **Atmel**, a company that was acquired by Microchip.\r\n\r\n     **WINC:** This designates the product family, which is Microchip's **Wireless Network Controller** lineup.\r\n\r\n     **1500:** This is the specific model number within that family.\r\n\r\n3. TSW-110-08-G-S\r\n\r\n   - **TSW:** This is the series name. It stands for \"**T**hrough-hole **S**trip, .025\" S**q**uare Post\". This tells you it's a standard pin header designed to go through holes in a PCB.\r\n\r\n     **110:** This segment defines the number of pins per row. `1` indicates it's a single-row header, and `10` means it has **10 pins** in that row.\r\n\r\n     **08:** This code specifies the lead style and plating. `08` is a common style for standard through-hole headers.\r\n\r\n     **G:** This indicates the plating material, which is **gold**. Specifically, it's 10 ¬µ\" of gold on the contacts, which provides good conductivity and corrosion resistance.\r\n\r\n     **S:** This last part defines the row configuration. `S` means it is a **single-row** header. If it were a dual-row header, it would have a `D`.\r\n\r\n4. TXS0108EPWR\r\n\r\n   - **TXS:** This prefix identifies the product family, which are voltage-level **translators** with auto-direction sensing.\r\n\r\n     **01:** This often indicates the technology generation or a specific feature set within the family.\r\n\r\n     **08:** This number specifies that the chip has **8 channels**, meaning it can translate eight separate signal lines at once.\r\n\r\n     **E:** This letter indicates that the part is an \"enhanced\" or updated product.\r\n\r\n     **PWR:** This suffix describes the packaging. `PW` specifies the **TSSOP** (Thin Shrink Small Outline Package), and `R` means it's supplied on **Tape & Reel** for automated manufacturing.\r\n\r\n5. CP2102N-A02-GQFN24R\r\n\r\n   - **CP2102N:** This is the base model number. The 'N' signifies it's part of the newer, more advanced generation of this popular chip.\r\n\r\n     **A02:** This indicates a specific revision of the chip, which often includes minor bug fixes or feature updates from the original `A01` version.\r\n\r\n     **GQFN24:** This describes the physical package of the chip. **QFN** (Quad Flat No-leads) is a very compact, square package with 24 pins that are pads on the bottom, not traditional \"legs\" sticking out.\r\n\r\n     **R:** This final letter indicates the part is supplied on **Tape & Reel**, which is standard for automated manufacturing.\r\n\r\n6. KUSBX-AS1N-B\r\n\r\n   - **KUSBX:** This is the series name from Kycon for their line of USB connectors.\r\n\r\n     **A:** Specifies that it's a **USB Type-A** connector.\r\n\r\n     **S:** Indicates that it's a **plug** (the male half).\r\n\r\n     **1N:** Defines the mounting style, which is **through-hole** with a standard horizontal orientation.\r\n\r\n     **B:** Specifies the shell type, which in this case is a standard one-piece metal shell.\r\n\r\n### Design experiences from my previous PCB project\r\n\r\n1. Route the WIFI first, and the UART, for its the high speed route.\r\n2. Modify the Rule checks for proper impedance matching\r\n3. Make sure the layout first.","filePath":"src/content/blogs/VIP_Progress.md","digest":"502da275d25260bf","rendered":{"html":"<h2 id=\"week-1-aug-25---aug-31\">Week 1: Aug 25 - Aug 31</h2>\n<h3 id=\"foundation--basic-structures\">Foundation &#x26; Basic Structures</h3>\n<h4 id=\"module-structure\">Module Structure</h4>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>module ModuleName (</span></span>\n<span class=\"line\"><span>    // Input signals</span></span>\n<span class=\"line\"><span>    input logic signal_name,</span></span>\n<span class=\"line\"><span>    input logic [31:0] bus_signal,</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    // Output signals</span></span>\n<span class=\"line\"><span>    output logic output_signal,</span></span>\n<span class=\"line\"><span>    output logic [4:0] output_bus</span></span>\n<span class=\"line\"><span>);</span></span>\n<span class=\"line\"><span>    // Module body</span></span>\n<span class=\"line\"><span>endmodule</span></span></code></pre>\n<p><strong>Remember:</strong></p>\n<ul>\n<li><code>logic</code> replaces wire/reg</li>\n<li>Bus width: <code>[MSB:LSB]</code> before signal name</li>\n<li>Semicolon after port list</li>\n</ul>\n<h4 id=\"data-types\">Data Types</h4>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>// Enumerated types with explicit width</span></span>\n<span class=\"line\"><span>typedef enum logic [2:0] {</span></span>\n<span class=\"line\"><span>    NO_WRITEBACK = 3'b000,</span></span>\n<span class=\"line\"><span>    READ_ALU_RESULT = 3'b001,</span></span>\n<span class=\"line\"><span>    READ_MEM_RESULT = 3'b010</span></span>\n<span class=\"line\"><span>} write_back_mux_selector;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>// Localparam for compile-time constants</span></span>\n<span class=\"line\"><span>localparam INSTR_START_PC = 0;</span></span>\n<span class=\"line\"><span>localparam DATA_START_PC = 127;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>// Parameter extraction using bit slicing</span></span>\n<span class=\"line\"><span>localparam REG_S1_MSB = 19;</span></span>\n<span class=\"line\"><span>localparam REG_S1_LSB = 15;</span></span></code></pre>\n<p><strong>Number Formats:</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>7'h23    // 7-bit hex value</span></span>\n<span class=\"line\"><span>3'b001   // 3-bit binary value</span></span>\n<span class=\"line\"><span>32'bz    // 32-bit high-impedance</span></span>\n<span class=\"line\"><span>8'h00    // 8-bit hex (for byte operations)</span></span></code></pre>\n<h4 id=\"procedural-blocks\">Procedural Blocks</h4>\n<p><strong>Combinational - always_comb</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>always_comb begin</span></span>\n<span class=\"line\"><span>    case(alu_operator_ip)</span></span>\n<span class=\"line\"><span>        ALU_ADD: begin</span></span>\n<span class=\"line\"><span>            alu_result_op = alu_operand_a_ip + alu_operand_b_ip;</span></span>\n<span class=\"line\"><span>            alu_valid_op = 1;</span></span>\n<span class=\"line\"><span>        end</span></span>\n<span class=\"line\"><span>        ALU_SUB: begin</span></span>\n<span class=\"line\"><span>            alu_result_op = alu_operand_a_ip - alu_operand_b_ip;</span></span>\n<span class=\"line\"><span>            alu_valid_op = 1;</span></span>\n<span class=\"line\"><span>        end</span></span>\n<span class=\"line\"><span>        default: begin</span></span>\n<span class=\"line\"><span>            alu_result_op = 0;</span></span>\n<span class=\"line\"><span>            alu_valid_op = 0;</span></span>\n<span class=\"line\"><span>        end</span></span>\n<span class=\"line\"><span>    endcase</span></span>\n<span class=\"line\"><span>end</span></span></code></pre>\n<p><strong>Sequential - always @(posedge clk)</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>always @(posedge clk) begin</span></span>\n<span class=\"line\"><span>    if (~reset)</span></span>\n<span class=\"line\"><span>        cycle_count &#x3C;= cycle_count + 1;</span></span>\n<span class=\"line\"><span>end</span></span></code></pre>\n<hr>\n<h2 id=\"week-2-sep-1---sep-7\">Week 2: Sep 1 - Sep 7</h2>\n<h3 id=\"advanced-patterns--pipeline-design\">Advanced Patterns &#x26; Pipeline Design</h3>\n<h4 id=\"packages\">Packages</h4>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>package CORE_PKG;</span></span>\n<span class=\"line\"><span>    // Parameters</span></span>\n<span class=\"line\"><span>    parameter OPCODE_STORE = 7'h23;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    // Enumerations</span></span>\n<span class=\"line\"><span>    typedef enum logic [6:0] {</span></span>\n<span class=\"line\"><span>        ALU_NOP = 7'b0000000,</span></span>\n<span class=\"line\"><span>        ALU_ADD = 7'b0011000</span></span>\n<span class=\"line\"><span>    } alu_opcode_e;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    // Custom types</span></span>\n<span class=\"line\"><span>    typedef enum {</span></span>\n<span class=\"line\"><span>        REG_A, PC, OPA_NOP</span></span>\n<span class=\"line\"><span>    } operand_a_mux;</span></span>\n<span class=\"line\"><span>endpackage</span></span></code></pre>\n<p><strong>Import:</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>import CORE_PKG::*;</span></span></code></pre>\n<h4 id=\"pipeline-patterns\">Pipeline Patterns</h4>\n<p><strong>Signal Naming Convention</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>// Pass-through signals use _pt suffix</span></span>\n<span class=\"line\"><span>logic [31:0] id_uimmd_pt;</span></span>\n<span class=\"line\"><span>logic [31:0] ex_uimmd_pt;</span></span>\n<span class=\"line\"><span>logic [31:0] lsu_uimmd_pt;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>// Stage outputs use _op suffix</span></span>\n<span class=\"line\"><span>output logic [31:0] alu_result_op;</span></span>\n<span class=\"line\"><span>output logic alu_valid_op;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>// Stage inputs use _ip suffix</span></span>\n<span class=\"line\"><span>input logic [31:0] alu_operand_a_ip;</span></span>\n<span class=\"line\"><span>input logic alu_enable_ip;</span></span></code></pre>\n<p><strong>Pipeline Buffers</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>// ID to EX pipeline buffer</span></span>\n<span class=\"line\"><span>logic [31:0] id_instr_pc_addr_pt;</span></span>\n<span class=\"line\"><span>logic [31:0] ex_instr_pc_addr_pt;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>// EX to MEM pipeline buffer</span></span>\n<span class=\"line\"><span>logic [31:0] ex_alu_result_pt;</span></span>\n<span class=\"line\"><span>logic ex_alu_result_valid_pt;</span></span></code></pre>\n<p><strong>Pipeline Control</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>input logic flush_en;   // Branch flush</span></span>\n<span class=\"line\"><span>output logic stall_op;  // Hazard stall</span></span></code></pre>\n<h4 id=\"risc-v-patterns\">RISC-V Patterns</h4>\n<p><strong>Opcodes</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>parameter OPCODE_STORE = 7'h23;   // S-type</span></span>\n<span class=\"line\"><span>parameter OPCODE_LOAD = 7'h03;    // I-type</span></span>\n<span class=\"line\"><span>parameter OPCODE_BRANCH = 7'h63;  // B-type</span></span>\n<span class=\"line\"><span>parameter OPCODE_JAL = 7'h6f;     // J-type</span></span>\n<span class=\"line\"><span>parameter OPCODE_JALR = 7'h67;</span></span>\n<span class=\"line\"><span>parameter OPCODE_LUI = 7'h37;     // U-type</span></span>\n<span class=\"line\"><span>parameter OPCODE_AUIPC = 7'h17;</span></span>\n<span class=\"line\"><span>parameter OPCODE_OP = 7'h33;      // R-type</span></span>\n<span class=\"line\"><span>parameter OPCODE_OPIMM = 7'h13;</span></span></code></pre>\n<p><strong>Field Extraction</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>logic [6:0] opcode = instruction[6:0];</span></span>\n<span class=\"line\"><span>logic [4:0] rd = instruction[11:7];</span></span>\n<span class=\"line\"><span>logic [2:0] funct3 = instruction[14:12];</span></span>\n<span class=\"line\"><span>logic [4:0] rs1 = instruction[19:15];</span></span>\n<span class=\"line\"><span>logic [4:0] rs2 = instruction[24:20];</span></span>\n<span class=\"line\"><span>logic [6:0] funct7 = instruction[31:25];</span></span></code></pre>\n<hr>\n<h3 id=\"interesting-knowledge-base-on-my-previous-project\">Interesting Knowledge base on my previous project</h3>\n<h2 id=\"week-3-sep-8---sep-14\">Week 3: Sep 8 - Sep 14</h2>\n<h3 id=\"testing-synthesis--optimization\">Testing, Synthesis &#x26; Optimization</h3>\n<h4 id=\"testbench\">Testbench</h4>\n<p><strong>Clock &#x26; Timing</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>`timescale 1ns / 1ns</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>module Core_tb;</span></span>\n<span class=\"line\"><span>    logic clk = 1;</span></span>\n<span class=\"line\"><span>    always #1 clk &#x3C;= clk + 1;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    initial begin</span></span>\n<span class=\"line\"><span>        reset = 1'b1;</span></span>\n<span class=\"line\"><span>        #6 reset = 1'b0;</span></span>\n<span class=\"line\"><span>        #50 $finish;</span></span>\n<span class=\"line\"><span>    end</span></span></code></pre>\n<p><strong>Memory Init</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>core_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[0] = 8'h00;</span></span>\n<span class=\"line\"><span>core_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[1] = 8'h00;</span></span>\n<span class=\"line\"><span>core_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[2] = 8'h00;</span></span>\n<span class=\"line\"><span>core_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[3] = 8'h00;</span></span></code></pre>\n<p><strong>Waveform Dump</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>$dumpfile(\"Core_Simulation.vcd\");</span></span>\n<span class=\"line\"><span>$dumpvars(0, Core_tb);</span></span></code></pre>\n<h4 id=\"tricks\">Tricks</h4>\n<p><strong>1. Ternary Operator</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>assign valid_instr_to_decode = instr_data_valid_ip ? instr_data_ip : 32'bz;</span></span>\n<span class=\"line\"><span>alu_result_op = ($signed(alu_operand_a_ip) &#x3C; $signed(alu_operand_b_ip)) ? 1 : 0;</span></span></code></pre>\n<p><strong>2. Prevent Latches</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>always_comb begin</span></span>\n<span class=\"line\"><span>    // Set defaults first</span></span>\n<span class=\"line\"><span>    alu_operator = ALU_NOP;</span></span>\n<span class=\"line\"><span>    operand_a_select = OPA_NOP;</span></span>\n<span class=\"line\"><span>    writeback_mux = NO_WRITEBACK;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    // Then override based on conditions</span></span>\n<span class=\"line\"><span>    case(opcode)</span></span>\n<span class=\"line\"><span>        // ...</span></span>\n<span class=\"line\"><span>    endcase</span></span>\n<span class=\"line\"><span>end</span></span></code></pre>\n<p><strong>3. Hierarchical Access</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>core_proc.InstructionFetch_Module.InstructionMemory.instr_RAM[0]</span></span></code></pre>\n<p><strong>4. Module Instance</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>Core core_proc(</span></span>\n<span class=\"line\"><span>    .clock(clk),</span></span>\n<span class=\"line\"><span>    .reset(reset),</span></span>\n<span class=\"line\"><span>    .mem_en(mem_enable)</span></span>\n<span class=\"line\"><span>);</span></span></code></pre>\n<p><strong>5. Case with Default</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>case(selector)</span></span>\n<span class=\"line\"><span>    VALUE1: action1();</span></span>\n<span class=\"line\"><span>    VALUE2: action2();</span></span>\n<span class=\"line\"><span>    default: begin</span></span>\n<span class=\"line\"><span>        default_action();</span></span>\n<span class=\"line\"><span>    end</span></span>\n<span class=\"line\"><span>endcase</span></span></code></pre>\n<h4 id=\"synthesis-rules\">Synthesis Rules</h4>\n<p><strong>1. Assignment Types</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>always @(posedge clk) begin</span></span>\n<span class=\"line\"><span>    signal &#x3C;= new_value;  // Non-blocking in sequential</span></span>\n<span class=\"line\"><span>end</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>always_comb begin</span></span>\n<span class=\"line\"><span>    signal = input_value;  // Blocking in combinational</span></span>\n<span class=\"line\"><span>end</span></span></code></pre>\n<p><strong>2. Sensitivity Lists</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>always_comb begin  // Auto-sensitive to all RHS</span></span>\n<span class=\"line\"><span>end</span></span></code></pre>\n<p><strong>3. No Latches</strong></p>\n<ul>\n<li>Assign in all branches</li>\n<li>Default case mandatory</li>\n<li>Initialize outputs</li>\n</ul>\n<h4 id=\"debug\">Debug</h4>\n<p><strong>1. Display Messages</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>$display(\"Time: %0t, PC: %h, Instruction: %h\", $time, pc, instruction);</span></span></code></pre>\n<p><strong>2. Assertions</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>assert(condition) else $error(\"Assertion failed at time %0t\", $time);</span></span></code></pre>\n<p><strong>3. Generate Loops</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>genvar i;</span></span>\n<span class=\"line\"><span>generate</span></span>\n<span class=\"line\"><span>    for (i = 0; i &#x3C; 32; i++) begin : gen_loop</span></span>\n<span class=\"line\"><span>        assign bit_out[i] = bit_in[i] &#x26; enable;</span></span>\n<span class=\"line\"><span>    end</span></span>\n<span class=\"line\"><span>endgenerate</span></span></code></pre>\n<p><strong>4. Parameterized Width</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>module GenericModule #(</span></span>\n<span class=\"line\"><span>    parameter WIDTH = 32</span></span>\n<span class=\"line\"><span>)(</span></span>\n<span class=\"line\"><span>    input logic [WIDTH-1:0] data_in,</span></span>\n<span class=\"line\"><span>    output logic [WIDTH-1:0] data_out</span></span>\n<span class=\"line\"><span>);</span></span></code></pre>\n<p><strong>5. Width-Specified Constants</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>logic [31:0] data = 32'h0000_0000;  // Underscores for readability</span></span>\n<span class=\"line\"><span>logic [6:0] opcode = 7'b011_0011;</span></span></code></pre>\n<p><strong>6. Forward Declaration</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>logic [31:0] internal_signal;</span></span>\n<span class=\"line\"><span>logic control_signal;</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>always_comb begin</span></span>\n<span class=\"line\"><span>    internal_signal = input_signal + offset;</span></span>\n<span class=\"line\"><span>end</span></span></code></pre>\n<p><strong>7. Auto-sensitivity - always @(*)</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>always @(*) begin</span></span>\n<span class=\"line\"><span>    alu_operator = ALU_NOP; // Default assignment</span></span>\n<span class=\"line\"><span>    operand_a_select = OPA_NOP;</span></span>\n<span class=\"line\"><span>end</span></span></code></pre>\n<p><strong>8. Signed Compare</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>$signed(operand_a) &#x3C; $signed(operand_b)  // Treat as signed values</span></span></code></pre>\n<h2 id=\"week-3-sep-15---sep-17\">Week 3: Sep 15 - Sep 17</h2>\n<p>We decide upon the part for the add-on board</p>\n<p>For this current board, I think I might design a four layer board. I have to correctly consider the WIFI module SPI and the level shifter, and do some entry level impedance matching.</p>\n<h3 id=\"common-terms-in-selecting-the-part-choice\">Common terms in selecting the part choice:</h3>\n<ul>\n<li><strong>RoHS</strong> stands for the <strong>Restriction of Hazardous Substances</strong>. It‚Äôs a directive from the European Union that restricts the use of specific hazardous materials in electrical and electronic products</li>\n</ul>\n<h3 id=\"part-choice\">Part choice</h3>\n<ol>\n<li>\n<p>UJC-HP-3-SMT-TR</p>\n<ul>\n<li>\n<p><strong>UJC</strong></p>\n<ul>\n<li><strong>Meaning:</strong> This is the product series prefix. It stands for <strong>USB Jack Connector</strong>.</li>\n</ul>\n<p><strong>HP</strong></p>\n<ul>\n<li><strong>Meaning:</strong> This often specifies a particular feature, like <strong>High Power</strong> or <strong>High Performance</strong>. For a modern USB-C jack, this likely indicates it‚Äôs designed to handle the higher power delivery standards for fast charging.</li>\n</ul>\n<p><strong>3</strong></p>\n<ul>\n<li><strong>Meaning:</strong> This number typically refers to the <strong>USB standard</strong> the connector is designed for. In this case, it indicates compatibility with <strong>USB 3.x</strong> specifications (like USB 3.1 or 3.2), which support higher data transfer speeds than older USB 2.0.</li>\n</ul>\n<p><strong>TR</strong></p>\n<ul>\n<li><strong>Meaning:</strong> This is a packaging designator that stands for <strong>Tape and Reel</strong>. The components are packaged in a long, pocketed tape wound onto a reel.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>ATWINC1500</p>\n<ul>\n<li>\n<p><strong>AT:</strong> This prefix often indicates the part originated from <strong>Atmel</strong>, a company that was acquired by Microchip.</p>\n<p><strong>WINC:</strong> This designates the product family, which is Microchip‚Äôs <strong>Wireless Network Controller</strong> lineup.</p>\n<p><strong>1500:</strong> This is the specific model number within that family.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>TSW-110-08-G-S</p>\n<ul>\n<li>\n<p><strong>TSW:</strong> This is the series name. It stands for ‚Äú<strong>T</strong>hrough-hole <strong>S</strong>trip, .025‚Äù S<strong>q</strong>uare Post‚Äù. This tells you it‚Äôs a standard pin header designed to go through holes in a PCB.</p>\n<p><strong>110:</strong> This segment defines the number of pins per row. <code>1</code> indicates it‚Äôs a single-row header, and <code>10</code> means it has <strong>10 pins</strong> in that row.</p>\n<p><strong>08:</strong> This code specifies the lead style and plating. <code>08</code> is a common style for standard through-hole headers.</p>\n<p><strong>G:</strong> This indicates the plating material, which is <strong>gold</strong>. Specifically, it‚Äôs 10 ¬µ‚Äù of gold on the contacts, which provides good conductivity and corrosion resistance.</p>\n<p><strong>S:</strong> This last part defines the row configuration. <code>S</code> means it is a <strong>single-row</strong> header. If it were a dual-row header, it would have a <code>D</code>.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>TXS0108EPWR</p>\n<ul>\n<li>\n<p><strong>TXS:</strong> This prefix identifies the product family, which are voltage-level <strong>translators</strong> with auto-direction sensing.</p>\n<p><strong>01:</strong> This often indicates the technology generation or a specific feature set within the family.</p>\n<p><strong>08:</strong> This number specifies that the chip has <strong>8 channels</strong>, meaning it can translate eight separate signal lines at once.</p>\n<p><strong>E:</strong> This letter indicates that the part is an ‚Äúenhanced‚Äù or updated product.</p>\n<p><strong>PWR:</strong> This suffix describes the packaging. <code>PW</code> specifies the <strong>TSSOP</strong> (Thin Shrink Small Outline Package), and <code>R</code> means it‚Äôs supplied on <strong>Tape &#x26; Reel</strong> for automated manufacturing.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>CP2102N-A02-GQFN24R</p>\n<ul>\n<li>\n<p><strong>CP2102N:</strong> This is the base model number. The ‚ÄòN‚Äô signifies it‚Äôs part of the newer, more advanced generation of this popular chip.</p>\n<p><strong>A02:</strong> This indicates a specific revision of the chip, which often includes minor bug fixes or feature updates from the original <code>A01</code> version.</p>\n<p><strong>GQFN24:</strong> This describes the physical package of the chip. <strong>QFN</strong> (Quad Flat No-leads) is a very compact, square package with 24 pins that are pads on the bottom, not traditional ‚Äúlegs‚Äù sticking out.</p>\n<p><strong>R:</strong> This final letter indicates the part is supplied on <strong>Tape &#x26; Reel</strong>, which is standard for automated manufacturing.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>KUSBX-AS1N-B</p>\n<ul>\n<li>\n<p><strong>KUSBX:</strong> This is the series name from Kycon for their line of USB connectors.</p>\n<p><strong>A:</strong> Specifies that it‚Äôs a <strong>USB Type-A</strong> connector.</p>\n<p><strong>S:</strong> Indicates that it‚Äôs a <strong>plug</strong> (the male half).</p>\n<p><strong>1N:</strong> Defines the mounting style, which is <strong>through-hole</strong> with a standard horizontal orientation.</p>\n<p><strong>B:</strong> Specifies the shell type, which in this case is a standard one-piece metal shell.</p>\n</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"design-experiences-from-my-previous-pcb-project\">Design experiences from my previous PCB project</h3>\n<ol>\n<li>Route the WIFI first, and the UART, for its the high speed route.</li>\n<li>Modify the Rule checks for proper impedance matching</li>\n<li>Make sure the layout first.</li>\n</ol>","metadata":{"headings":[{"depth":2,"slug":"week-1-aug-25---aug-31","text":"Week 1: Aug 25 - Aug 31"},{"depth":3,"slug":"foundation--basic-structures","text":"Foundation & Basic Structures"},{"depth":4,"slug":"module-structure","text":"Module Structure"},{"depth":4,"slug":"data-types","text":"Data Types"},{"depth":4,"slug":"procedural-blocks","text":"Procedural Blocks"},{"depth":2,"slug":"week-2-sep-1---sep-7","text":"Week 2: Sep 1 - Sep 7"},{"depth":3,"slug":"advanced-patterns--pipeline-design","text":"Advanced Patterns & Pipeline Design"},{"depth":4,"slug":"packages","text":"Packages"},{"depth":4,"slug":"pipeline-patterns","text":"Pipeline Patterns"},{"depth":4,"slug":"risc-v-patterns","text":"RISC-V Patterns"},{"depth":3,"slug":"interesting-knowledge-base-on-my-previous-project","text":"Interesting Knowledge base on my previous project"},{"depth":2,"slug":"week-3-sep-8---sep-14","text":"Week 3: Sep 8 - Sep 14"},{"depth":3,"slug":"testing-synthesis--optimization","text":"Testing, Synthesis & Optimization"},{"depth":4,"slug":"testbench","text":"Testbench"},{"depth":4,"slug":"tricks","text":"Tricks"},{"depth":4,"slug":"synthesis-rules","text":"Synthesis Rules"},{"depth":4,"slug":"debug","text":"Debug"},{"depth":2,"slug":"week-3-sep-15---sep-17","text":"Week 3: Sep 15 - Sep 17"},{"depth":3,"slug":"common-terms-in-selecting-the-part-choice","text":"Common terms in selecting the part choice:"},{"depth":3,"slug":"part-choice","text":"Part choice"},{"depth":3,"slug":"design-experiences-from-my-previous-pcb-project","text":"Design experiences from my previous PCB project"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"VIP Progress Journal - SystemVerilog Learning","description":"Weekly progress journal documenting my journey learning SystemVerilog, including module structures, always blocks, and FSM implementations","pubDate":"2024-08-25T00:00:00.000Z","author":"Xiaoyou Wu","tags":["SystemVerilog","hardware","FPGA","digital-design","learning-journal"]},"imagePaths":[]}},"collection":"blogs","slug":"vip_progress"}];
const allProjects = [{"id":"ai-translation/AI_translation.md","data":{"title":"AI Translation Startup (Seek Hub)","description":"A smart, AI-powered platform for perfect PDF-to-Word and multi-format document translation, preserving layout and formatting.","pubDate":"2025-07-31T00:00:00.000Z","tags":["AI","Translation","PDF","DOCX","Next.js","Python","Startup"],"githubLink":"https://laurence-wu.github.io/projects/"},"body":"## My feelings to this project\r\n\r\nThis is like buiding a child of my own. Every feature properly tested to integrated, and all those annoying configuration on the google cloud. This is my first serious startup project.\r\n\r\n## Introduction\r\n\r\nWe all know how frustrating translating documents can be, especially complex PDFs. You lose your formatting, the context gets mixed up, and you spend hours fixing everything manually. Seek Hub is our solution: a smart, AI-powered platform designed to make the entire process effortless and deliver a perfect translation every time.\r\n\r\n## Why We're Building This and What It Does\r\n\r\nWe started this project because we saw a clear need for a simpler, more reliable translation tool. In a world of powerful AI agents, the process of translating a document should be easy. Our goal is to handle all the complex and tedious work for you, so you can get a high-quality translation without the headache.\r\n\r\nHere‚Äôs how simple we‚Äôve made it:\r\n\r\n1. You start by uploading your PDF document.\r\n2. Our AI agent immediately gets to work, automatically translating the entire file. The most important part is that it **perfectly preserves the original layout and formatting**‚Äîno more broken tables or misplaced images.\r\n3. In just a few moments, you‚Äôre presented with a ready-to-use, fully translated document.\r\n\r\nThe AI is smart enough to understand the document's context, but if you want to make a quick change, you can easily review the translation and accept smart suggestions for alternative wording. The goal is that you receive a document that is 99% of the way there, or completely finished, without you having to do any of the heavy lifting.\r\n\r\n## The Technical Details\r\n\r\nThe convenience you experience on the frontend is made possible by some serious engineering on the backend.\r\n\r\n- **Architecture:** We use a modern stack with a **Next.js** frontend and a **Python** backend. Our database is a NoSQL solution, currently **Firebase**, for smooth and reliable file uploads.\r\n- **The Core Engine:** Our platform's real power comes from our custom-built backend pipeline.\r\n  1. **Format Preservation Engine:** When you upload a PDF, our proprietary **PDF-to-Docs engine** is the first thing it touches. Its single most important job is to map out your document's structure. This allows us to reassemble the translated document perfectly, so you don't waste any time rebuilding tables, realigning images, or fixing layouts.\r\n  2. **High-Speed Translation:** To deliver your translation quickly, our backend uses a sophisticated **asynchronous pipeline**. This system breaks down large documents and processes them in parallel using a **connection pool** and a **thread pool**. All this complexity is handled completely behind the scenes. For you, it just means you get your translated file back incredibly fast, even if it's a very large document.\r\n  3. **Automatic Reassembly:** Once the translation is complete, our system automatically rebuilds the PDF, delivering a final product that looks just like your original.\r\n\r\n## Project Summary\r\n\r\nTo put it simply, **Seek Hub is designed to make high-quality document translation effortless.** It removes the biggest headaches from the process: losing your formatting and doing tedious manual work. Our platform's core promise is to deliver a perfectly formatted, accurately translated PDF back to you in a fraction of the time it would normally take. By leveraging a powerful AI agent and a smart backend pipeline, we've created a tool that just works, letting you focus on your content, not the complex process of translation.","filePath":"src/content/projects/ai-translation/AI_translation.md","digest":"41022e05c2de1e79","rendered":{"html":"<h2 id=\"my-feelings-to-this-project\">My feelings to this project</h2>\n<p>This is like buiding a child of my own. Every feature properly tested to integrated, and all those annoying configuration on the google cloud. This is my first serious startup project.</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>We all know how frustrating translating documents can be, especially complex PDFs. You lose your formatting, the context gets mixed up, and you spend hours fixing everything manually. Seek Hub is our solution: a smart, AI-powered platform designed to make the entire process effortless and deliver a perfect translation every time.</p>\n<h2 id=\"why-were-building-this-and-what-it-does\">Why We‚Äôre Building This and What It Does</h2>\n<p>We started this project because we saw a clear need for a simpler, more reliable translation tool. In a world of powerful AI agents, the process of translating a document should be easy. Our goal is to handle all the complex and tedious work for you, so you can get a high-quality translation without the headache.</p>\n<p>Here‚Äôs how simple we‚Äôve made it:</p>\n<ol>\n<li>You start by uploading your PDF document.</li>\n<li>Our AI agent immediately gets to work, automatically translating the entire file. The most important part is that it <strong>perfectly preserves the original layout and formatting</strong>‚Äîno more broken tables or misplaced images.</li>\n<li>In just a few moments, you‚Äôre presented with a ready-to-use, fully translated document.</li>\n</ol>\n<p>The AI is smart enough to understand the document‚Äôs context, but if you want to make a quick change, you can easily review the translation and accept smart suggestions for alternative wording. The goal is that you receive a document that is 99% of the way there, or completely finished, without you having to do any of the heavy lifting.</p>\n<h2 id=\"the-technical-details\">The Technical Details</h2>\n<p>The convenience you experience on the frontend is made possible by some serious engineering on the backend.</p>\n<ul>\n<li><strong>Architecture:</strong> We use a modern stack with a <strong>Next.js</strong> frontend and a <strong>Python</strong> backend. Our database is a NoSQL solution, currently <strong>Firebase</strong>, for smooth and reliable file uploads.</li>\n<li><strong>The Core Engine:</strong> Our platform‚Äôs real power comes from our custom-built backend pipeline.\n<ol>\n<li><strong>Format Preservation Engine:</strong> When you upload a PDF, our proprietary <strong>PDF-to-Docs engine</strong> is the first thing it touches. Its single most important job is to map out your document‚Äôs structure. This allows us to reassemble the translated document perfectly, so you don‚Äôt waste any time rebuilding tables, realigning images, or fixing layouts.</li>\n<li><strong>High-Speed Translation:</strong> To deliver your translation quickly, our backend uses a sophisticated <strong>asynchronous pipeline</strong>. This system breaks down large documents and processes them in parallel using a <strong>connection pool</strong> and a <strong>thread pool</strong>. All this complexity is handled completely behind the scenes. For you, it just means you get your translated file back incredibly fast, even if it‚Äôs a very large document.</li>\n<li><strong>Automatic Reassembly:</strong> Once the translation is complete, our system automatically rebuilds the PDF, delivering a final product that looks just like your original.</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"project-summary\">Project Summary</h2>\n<p>To put it simply, <strong>Seek Hub is designed to make high-quality document translation effortless.</strong> It removes the biggest headaches from the process: losing your formatting and doing tedious manual work. Our platform‚Äôs core promise is to deliver a perfectly formatted, accurately translated PDF back to you in a fraction of the time it would normally take. By leveraging a powerful AI agent and a smart backend pipeline, we‚Äôve created a tool that just works, letting you focus on your content, not the complex process of translation.</p>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-to-this-project","text":"My feelings to this project"},{"depth":2,"slug":"introduction","text":"Introduction"},{"depth":2,"slug":"why-were-building-this-and-what-it-does","text":"Why We‚Äôre Building This and What It Does"},{"depth":2,"slug":"the-technical-details","text":"The Technical Details"},{"depth":2,"slug":"project-summary","text":"Project Summary"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"AI Translation Startup (Seek Hub)","description":"A smart, AI-powered platform for perfect PDF-to-Word and multi-format document translation, preserving layout and formatting.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://laurence-wu.github.io/projects/","tags":["AI","Translation","PDF","DOCX","Next.js","Python","Startup"]},"imagePaths":[]}},"collection":"projects","slug":"ai-translation/ai_translation"},{"id":"buzzcar/BuzzCar.md","data":{"title":"BuzzRacer V2 Autonomous Car Project","description":"A custom-designed autonomous vehicle with advanced power management, LiDAR sensing, and compact PCB integration for algorithm development.","pubDate":"2025-07-31T00:00:00.000Z","tags":["Autonomous Vehicles","Arduino","LiDAR","PCB Design","Power Management","Embedded Systems"],"githubLink":"https://github.com/Laurence-Wu/BuzzRacer-V2.git"},"body":"## My feelings about this project\r\n\r\nThis is a project that I have always wanted to do, which is to self-design a small car. Thanks to Nick and the lab he's affiliated with, I got the chance to really design such a thing.\r\n\r\n## Technical Implementation: How It Works\r\n\r\n- **Central Controller**: An **Arduino Nano 33 IoT** serves as the brain of the vehicle, processing sensor data and controlling the actuators.\r\n- **Power Management**: This is a critical feature of the design.\r\n  - The system is powered by a **2s LiPo battery** (6.5V - 8.4V).\r\n  - It includes a crucial **hot-plug management** feature, allowing a wired DC power supply to be connected or disconnected for debugging without interrupting power to the Arduino.\r\n  - Onboard converters step the battery voltage down to a stable 5V to power the various components.\r\n  - The board also integrates **voltage sensing** to monitor battery levels for the control algorithm.\r\n- **Sensors and Actuators**:\r\n  - **Sensing**: Four **Benewake TF-Mini S Lidar** modules are used for environment detection, all communicating over a shared I2C bus.\r\n  - **Driving**: A brushed DC motor is controlled by a **TB67H451FNG motor driver IC**, with traces designed to handle up to 3A of current.\r\n  - **Steering**: A standard hobby servo handles steering, controlled directly by the Arduino.\r\n- **PCB Design**:\r\n  - The board is a **two-layer PCB** with dimensions under 100x100mm to fit the car chassis and manage manufacturing costs.\r\n  - It's designed using primarily **0805 SMT components**, which are small enough for a compact design but large enough to be hand-soldered with a heat gun.\r\n\r\n------\r\n\r\n## Conclusion\r\n\r\nIn summary, the BuzzRacer V2 project is a comprehensive engineering task to create a custom, all-in-one solution for a small-scale autonomous car. It combines mechanical design, by conforming to a specific chassis, with complex electronics design. The key technical challenges are the sophisticated **power management system** with hot-plug capability and the efficient **spatial integration** of all necessary components onto a compact, dual-layer PCB. The final result will be a durable and streamlined platform, ideal for developing and testing autonomous driving algorithms.","filePath":"src/content/projects/buzzcar/BuzzCar.md","digest":"fe2998d0b9c1ad23","rendered":{"html":"<h2 id=\"my-feelings-about-this-project\">My feelings about this project</h2>\n<p>This is a project that I have always wanted to do, which is to self-design a small car. Thanks to Nick and the lab he‚Äôs affiliated with, I got the chance to really design such a thing.</p>\n<h2 id=\"technical-implementation-how-it-works\">Technical Implementation: How It Works</h2>\n<ul>\n<li><strong>Central Controller</strong>: An <strong>Arduino Nano 33 IoT</strong> serves as the brain of the vehicle, processing sensor data and controlling the actuators.</li>\n<li><strong>Power Management</strong>: This is a critical feature of the design.\n<ul>\n<li>The system is powered by a <strong>2s LiPo battery</strong> (6.5V - 8.4V).</li>\n<li>It includes a crucial <strong>hot-plug management</strong> feature, allowing a wired DC power supply to be connected or disconnected for debugging without interrupting power to the Arduino.</li>\n<li>Onboard converters step the battery voltage down to a stable 5V to power the various components.</li>\n<li>The board also integrates <strong>voltage sensing</strong> to monitor battery levels for the control algorithm.</li>\n</ul>\n</li>\n<li><strong>Sensors and Actuators</strong>:\n<ul>\n<li><strong>Sensing</strong>: Four <strong>Benewake TF-Mini S Lidar</strong> modules are used for environment detection, all communicating over a shared I2C bus.</li>\n<li><strong>Driving</strong>: A brushed DC motor is controlled by a <strong>TB67H451FNG motor driver IC</strong>, with traces designed to handle up to 3A of current.</li>\n<li><strong>Steering</strong>: A standard hobby servo handles steering, controlled directly by the Arduino.</li>\n</ul>\n</li>\n<li><strong>PCB Design</strong>:\n<ul>\n<li>The board is a <strong>two-layer PCB</strong> with dimensions under 100x100mm to fit the car chassis and manage manufacturing costs.</li>\n<li>It‚Äôs designed using primarily <strong>0805 SMT components</strong>, which are small enough for a compact design but large enough to be hand-soldered with a heat gun.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In summary, the BuzzRacer V2 project is a comprehensive engineering task to create a custom, all-in-one solution for a small-scale autonomous car. It combines mechanical design, by conforming to a specific chassis, with complex electronics design. The key technical challenges are the sophisticated <strong>power management system</strong> with hot-plug capability and the efficient <strong>spatial integration</strong> of all necessary components onto a compact, dual-layer PCB. The final result will be a durable and streamlined platform, ideal for developing and testing autonomous driving algorithms.</p>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-about-this-project","text":"My feelings about this project"},{"depth":2,"slug":"technical-implementation-how-it-works","text":"Technical Implementation: How It Works"},{"depth":2,"slug":"conclusion","text":"Conclusion"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"BuzzRacer V2 Autonomous Car Project","description":"A custom-designed autonomous vehicle with advanced power management, LiDAR sensing, and compact PCB integration for algorithm development.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/BuzzRacer-V2.git","tags":["Autonomous Vehicles","Arduino","LiDAR","PCB Design","Power Management","Embedded Systems"]},"imagePaths":[]}},"collection":"projects","slug":"buzzcar/buzzcar"},{"id":"lips-to-speech/Lips_to_speech.md","data":{"title":"Lip-to-Speech Communication Device","description":"An assistive technology device that converts lip movements to speech using computer vision, AI models, and vibration feedback.","pubDate":"2025-07-31T00:00:00.000Z","tags":["Assistive Technology","Computer Vision","AI/ML","ESP32","Speech Synthesis","Accessibility"],"githubLink":"https://github.com/Laurence-Wu/LipToSpeech.git"},"body":"## Technical Implementation: \r\n\r\nThe project uses a distributed system where the PCB handles data capture and a laptop performs the heavy computational tasks.\r\n\r\n- **Hardware and Power Design**:\r\n  - **Power Source**: The device is powered by two LiPo batteries in series, providing **7.4V**.\r\n  - **Voltage Regulation**: A clever two-stage power system is used. First, an efficient **buck converter** steps the voltage down to 5V. Then, a **Low-Dropout (LDO) regulator** provides a very stable 3.3V output to power the sensitive main processor (ESP32-S3) and the camera. This design combines the high efficiency of a buck converter with the clean, stable power of an LDO, which is critical for the processor's performance.\r\n  - **Core Components**: The PCB is built around an **ESP32-S3 microcontroller** and an **OV5640 camera module**.\r\n- **Data Processing and Workflow**:\r\n  1. The **OV5640 camera** captures images of the user's lips.\r\n  2. The **ESP32-S3** processor takes these images and uses its built-in Wi-Fi to stream the data to a connected laptop.\r\n  3. All the intensive AI processing happens on the **laptop**:\r\n     - An open-source **lip-to-text model** analyzes the video stream and generates text.\r\n     - An open-source **text-to-speech model** converts that text into an audio signal.\r\n  4. The final audio signal is sent back to the ESP32-S3.\r\n  5. The ESP32-S3 outputs this as a **vibration signal** through a connected sound module.","filePath":"src/content/projects/lips-to-speech/Lips_to_speech.md","digest":"aeaf39cd9f57c147","rendered":{"html":"<h2 id=\"technical-implementation\">Technical Implementation:</h2>\n<p>The project uses a distributed system where the PCB handles data capture and a laptop performs the heavy computational tasks.</p>\n<ul>\n<li><strong>Hardware and Power Design</strong>:\n<ul>\n<li><strong>Power Source</strong>: The device is powered by two LiPo batteries in series, providing <strong>7.4V</strong>.</li>\n<li><strong>Voltage Regulation</strong>: A clever two-stage power system is used. First, an efficient <strong>buck converter</strong> steps the voltage down to 5V. Then, a <strong>Low-Dropout (LDO) regulator</strong> provides a very stable 3.3V output to power the sensitive main processor (ESP32-S3) and the camera. This design combines the high efficiency of a buck converter with the clean, stable power of an LDO, which is critical for the processor‚Äôs performance.</li>\n<li><strong>Core Components</strong>: The PCB is built around an <strong>ESP32-S3 microcontroller</strong> and an <strong>OV5640 camera module</strong>.</li>\n</ul>\n</li>\n<li><strong>Data Processing and Workflow</strong>:\n<ol>\n<li>The <strong>OV5640 camera</strong> captures images of the user‚Äôs lips.</li>\n<li>The <strong>ESP32-S3</strong> processor takes these images and uses its built-in Wi-Fi to stream the data to a connected laptop.</li>\n<li>All the intensive AI processing happens on the <strong>laptop</strong>:\n<ul>\n<li>An open-source <strong>lip-to-text model</strong> analyzes the video stream and generates text.</li>\n<li>An open-source <strong>text-to-speech model</strong> converts that text into an audio signal.</li>\n</ul>\n</li>\n<li>The final audio signal is sent back to the ESP32-S3.</li>\n<li>The ESP32-S3 outputs this as a <strong>vibration signal</strong> through a connected sound module.</li>\n</ol>\n</li>\n</ul>","metadata":{"headings":[{"depth":2,"slug":"technical-implementation","text":"Technical Implementation:"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Lip-to-Speech Communication Device","description":"An assistive technology device that converts lip movements to speech using computer vision, AI models, and vibration feedback.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/LipToSpeech.git","tags":["Assistive Technology","Computer Vision","AI/ML","ESP32","Speech Synthesis","Accessibility"]},"imagePaths":[]}},"collection":"projects","slug":"lips-to-speech/lips_to_speech"},{"id":"do-not-disturb/DoNotDistrub.md","data":{"title":"Do Not Disturb Robotics Arm Project","description":"A brain-computer interface and computer vision powered robotic arm that intelligently prevents interruptions during deep focus.","pubDate":"2025-07-31T00:00:00.000Z","tags":["Robotics","BCI","Computer Vision","Raspberry Pi","PID Control","Python"],"githubLink":"https://github.com/Laurence-Wu/RoboticsArmControl.git"},"body":"## My feelings to this project\r\n\r\nThis project means everything to me. I built it solo during a four-day hackathon, fueled by passion and just six hours of sleep. I was so exhausted afterward that I bombed my finals in statistics and signal processing. You could say I traded my GPA for this project, but it was worth it.\r\n\r\nThat was the moment I knew I had to be in robotics‚Äîit just clicked. It was the perfect fusion of computer vision, embedded systems, control theory, and PCB design, all working together to bring a complex robotic arm to life. I still vividly remember the late nights: writing error handling / testing scripts, recording and adding safety angular constraints,troubleshooting communication failures on the shared serial port, and fixing the burned out motors from overcurrent.\r\n\r\nThe more I thought about it, the more I understood. This world may not be a world of science; it might be a complex world that are engineered with 1000 more preprocessing steps.\r\n\r\n## Introduction\r\n\r\nThe \"Do Not Disturb\" project is an integrated robotics system designed to intelligently prevent interruptions based on a user's mental state. The project was conceived from the personal experience of being disrupted while in a state of deep focus, or \"flow.\" By combining Brain-Computer Interface (BCI) technology with computer vision and a robotic arm, the system serves as an automated guardian that only activates when the user is genuinely concentrating, solving the nuanced problem of distinguishing between welcome and unwelcome interruptions.\r\n\r\n## Why This Project and What It Does\r\n\r\nThe primary motivation behind this project was to create a system that could understand the user's level of focus and act accordingly. The creator identified that not all interruptions are undesirable; sometimes, a distraction is welcome when one is not deeply engaged in a task. The challenge was to build a system that could differentiate between a focused state and an unfocused one.\r\n\r\nThe system operates through the collaboration of two main components:\r\n\r\n1. **Brainwave Analysis System:** A user wears a BCI headset that captures EEG (electroencephalogram) data. This raw data is first processed by the manufacturer's API and then fed into a custom deep learning network. This network, trained on public data, performs a two-category classification to determine if the user is currently \"focusing\" or \"not focusing.\"\r\n2. **Robotic Guardian System:** The classification result is sent over the cloud to a **Raspberry Pi** that controls a robotic arm. If the system detects the user is in a state of focus, it activates the arm. Using an onboard camera, the arm employs a visual tracking algorithm to detect and follow the face of any person who approaches, acting as a clear, non-verbal \"Do Not Disturb\" signal.\r\n\r\n## Technical Details\r\n\r\nThe project's architecture integrates complex software and hardware components, primarily developed in **Python** for its rapid prototyping capabilities and SDK availability.\r\n\r\n- **Computer Vision:** The visual tracking system uses a two-stage approach for robust performance. It first uses **Haar Cascades** for general human face detection. Then, a deep learning model is used for more precise identification and tracking. To prevent erratic arm movements, a **\"lockdown period\"** was implemented; if the target face is lost, the system waits a few seconds and predicts the face's next likely position before re-engaging, ensuring smoother motion.\r\n- **Robotic Arm Control:** The robotic arm has **2 degrees of freedom ** and is controlled using a **PID (Proportional-Integral-Derivative)** algorithm. This required the meticulous tuning of six distinct PID parameters (three for each axis) to accurately translate the 2D coordinates from the camera feed into physical movement. This process was a significant challenge, involving extensive debugging of signs and values to correct initial bugs that caused the arm to lock up or behave erratically.\r\n- **System Integration and Challenges:** The Raspberry Pi communicated with the robotic arm over a **single-threaded serial port**. This limitation made it difficult to run the PID control loop concurrently with other logic, such as returning the arm to a home position. The solution was to create a routine that could rapidly send velocity commands to override the PID data flow when necessary. Other challenges included creating an accurate mapping between the camera's pixels and the arm's real-world distance and dealing with hardware failures, such as two servo motors breaking from over-current before software constraints were added.\r\n\r\n## Project Summary\r\n\r\nThe \"Do Not Disturb\" project is a successful proof-of-concept that effectively integrates BCI, deep learning, and robotics to solve a nuanced real-world problem. The development process provided a significant learning experience in complex system integration, PID control algorithms, hardware interfacing with a Raspberry Pi, and robust debugging. The project culminated in winning first place in the \"Brain Interaction\" robotics category at a competition. The complete codebase, including testing scripts and error handling, is available on GitHub for further review.","filePath":"src/content/projects/do-not-disturb/DoNotDistrub.md","digest":"4ea3874fb2188451","rendered":{"html":"<h2 id=\"my-feelings-to-this-project\">My feelings to this project</h2>\n<p>This project means everything to me. I built it solo during a four-day hackathon, fueled by passion and just six hours of sleep. I was so exhausted afterward that I bombed my finals in statistics and signal processing. You could say I traded my GPA for this project, but it was worth it.</p>\n<p>That was the moment I knew I had to be in robotics‚Äîit just clicked. It was the perfect fusion of computer vision, embedded systems, control theory, and PCB design, all working together to bring a complex robotic arm to life. I still vividly remember the late nights: writing error handling / testing scripts, recording and adding safety angular constraints,troubleshooting communication failures on the shared serial port, and fixing the burned out motors from overcurrent.</p>\n<p>The more I thought about it, the more I understood. This world may not be a world of science; it might be a complex world that are engineered with 1000 more preprocessing steps.</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>The ‚ÄúDo Not Disturb‚Äù project is an integrated robotics system designed to intelligently prevent interruptions based on a user‚Äôs mental state. The project was conceived from the personal experience of being disrupted while in a state of deep focus, or ‚Äúflow.‚Äù By combining Brain-Computer Interface (BCI) technology with computer vision and a robotic arm, the system serves as an automated guardian that only activates when the user is genuinely concentrating, solving the nuanced problem of distinguishing between welcome and unwelcome interruptions.</p>\n<h2 id=\"why-this-project-and-what-it-does\">Why This Project and What It Does</h2>\n<p>The primary motivation behind this project was to create a system that could understand the user‚Äôs level of focus and act accordingly. The creator identified that not all interruptions are undesirable; sometimes, a distraction is welcome when one is not deeply engaged in a task. The challenge was to build a system that could differentiate between a focused state and an unfocused one.</p>\n<p>The system operates through the collaboration of two main components:</p>\n<ol>\n<li><strong>Brainwave Analysis System:</strong> A user wears a BCI headset that captures EEG (electroencephalogram) data. This raw data is first processed by the manufacturer‚Äôs API and then fed into a custom deep learning network. This network, trained on public data, performs a two-category classification to determine if the user is currently ‚Äúfocusing‚Äù or ‚Äúnot focusing.‚Äù</li>\n<li><strong>Robotic Guardian System:</strong> The classification result is sent over the cloud to a <strong>Raspberry Pi</strong> that controls a robotic arm. If the system detects the user is in a state of focus, it activates the arm. Using an onboard camera, the arm employs a visual tracking algorithm to detect and follow the face of any person who approaches, acting as a clear, non-verbal ‚ÄúDo Not Disturb‚Äù signal.</li>\n</ol>\n<h2 id=\"technical-details\">Technical Details</h2>\n<p>The project‚Äôs architecture integrates complex software and hardware components, primarily developed in <strong>Python</strong> for its rapid prototyping capabilities and SDK availability.</p>\n<ul>\n<li><strong>Computer Vision:</strong> The visual tracking system uses a two-stage approach for robust performance. It first uses <strong>Haar Cascades</strong> for general human face detection. Then, a deep learning model is used for more precise identification and tracking. To prevent erratic arm movements, a <strong>‚Äúlockdown period‚Äù</strong> was implemented; if the target face is lost, the system waits a few seconds and predicts the face‚Äôs next likely position before re-engaging, ensuring smoother motion.</li>\n<li><strong>Robotic Arm Control:</strong> The robotic arm has **2 degrees of freedom ** and is controlled using a <strong>PID (Proportional-Integral-Derivative)</strong> algorithm. This required the meticulous tuning of six distinct PID parameters (three for each axis) to accurately translate the 2D coordinates from the camera feed into physical movement. This process was a significant challenge, involving extensive debugging of signs and values to correct initial bugs that caused the arm to lock up or behave erratically.</li>\n<li><strong>System Integration and Challenges:</strong> The Raspberry Pi communicated with the robotic arm over a <strong>single-threaded serial port</strong>. This limitation made it difficult to run the PID control loop concurrently with other logic, such as returning the arm to a home position. The solution was to create a routine that could rapidly send velocity commands to override the PID data flow when necessary. Other challenges included creating an accurate mapping between the camera‚Äôs pixels and the arm‚Äôs real-world distance and dealing with hardware failures, such as two servo motors breaking from over-current before software constraints were added.</li>\n</ul>\n<h2 id=\"project-summary\">Project Summary</h2>\n<p>The ‚ÄúDo Not Disturb‚Äù project is a successful proof-of-concept that effectively integrates BCI, deep learning, and robotics to solve a nuanced real-world problem. The development process provided a significant learning experience in complex system integration, PID control algorithms, hardware interfacing with a Raspberry Pi, and robust debugging. The project culminated in winning first place in the ‚ÄúBrain Interaction‚Äù robotics category at a competition. The complete codebase, including testing scripts and error handling, is available on GitHub for further review.</p>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-to-this-project","text":"My feelings to this project"},{"depth":2,"slug":"introduction","text":"Introduction"},{"depth":2,"slug":"why-this-project-and-what-it-does","text":"Why This Project and What It Does"},{"depth":2,"slug":"technical-details","text":"Technical Details"},{"depth":2,"slug":"project-summary","text":"Project Summary"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Do Not Disturb Robotics Arm Project","description":"A brain-computer interface and computer vision powered robotic arm that intelligently prevents interruptions during deep focus.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/RoboticsArmControl.git","tags":["Robotics","BCI","Computer Vision","Raspberry Pi","PID Control","Python"]},"imagePaths":[]}},"collection":"projects","slug":"do-not-disturb/donotdistrub"},{"id":"flight-computer/Flight_Computer.md","data":{"title":"Rocket Flight Computer System","description":"A sophisticated flight computer for rockets featuring STM32MP1 processor, multi-level memory hierarchy, and comprehensive sensor integration.","pubDate":"2025-07-31T00:00:00.000Z","tags":["Aerospace","STM32","Flight Control","Embedded Systems","Sensors","Real-time Systems"],"githubLink":"https://github.com/Laurence-Wu/RocketFlightComputer.git"},"body":"## Technical Implementation: How It Works\r\n\r\nThe system is built around a powerful microprocessor and uses a sophisticated architecture for power, memory, and communication.\r\n\r\n- **Power System**:\r\n  - It starts with a **14.8V LiPo battery**.\r\n  - The Power Board uses two-phase buck converters to efficiently step this down to **7.4V and 5V**.\r\n  - The 5V rail powers the Flight Computer, which uses a **PMIC** (Power Management IC) to generate a stable **3.3V** for sensitive components like sensors and memory chips.\r\n- **Processing and Memory**:\r\n  - **Processor**: The brain of the flight computer is a powerful **STM32MP157 series** microprocessor.\r\n  - **Memory Hierarchy**: A multi-level memory system is used to balance speed and cost:\r\n    - **DDR RAM**: For fast, volatile memory access.\r\n    - **NOR Flash**: For fast booting of the system.\r\n    - **NAND Flash / SD Card**: For cheaper, high-capacity, non-volatile data storage.\r\n- **Sensors and Communication**:\r\n  - **Sensors**: The board integrates a **GPS**, **IMU** (Inertial Measurement Unit), and a **barometer** to track the rocket's position, orientation, and altitude.\r\n  - **Communication Protocols**: Different protocols are used for specific tasks to optimize performance:\r\n    - **I2C**: For communicating with the sensors.\r\n    - **SPI**: For high-speed communication with the NOR flash.\r\n    - **SDIO**: For communicating with the SD card.\r\n    - **CAN bus**: For robust communication between the Flight Computer and the Telemetry board.","filePath":"src/content/projects/flight-computer/Flight_Computer.md","digest":"86ad15833d89612f","rendered":{"html":"<h2 id=\"technical-implementation-how-it-works\">Technical Implementation: How It Works</h2>\n<p>The system is built around a powerful microprocessor and uses a sophisticated architecture for power, memory, and communication.</p>\n<ul>\n<li><strong>Power System</strong>:\n<ul>\n<li>It starts with a <strong>14.8V LiPo battery</strong>.</li>\n<li>The Power Board uses two-phase buck converters to efficiently step this down to <strong>7.4V and 5V</strong>.</li>\n<li>The 5V rail powers the Flight Computer, which uses a <strong>PMIC</strong> (Power Management IC) to generate a stable <strong>3.3V</strong> for sensitive components like sensors and memory chips.</li>\n</ul>\n</li>\n<li><strong>Processing and Memory</strong>:\n<ul>\n<li><strong>Processor</strong>: The brain of the flight computer is a powerful <strong>STM32MP157 series</strong> microprocessor.</li>\n<li><strong>Memory Hierarchy</strong>: A multi-level memory system is used to balance speed and cost:\n<ul>\n<li><strong>DDR RAM</strong>: For fast, volatile memory access.</li>\n<li><strong>NOR Flash</strong>: For fast booting of the system.</li>\n<li><strong>NAND Flash / SD Card</strong>: For cheaper, high-capacity, non-volatile data storage.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Sensors and Communication</strong>:\n<ul>\n<li><strong>Sensors</strong>: The board integrates a <strong>GPS</strong>, <strong>IMU</strong> (Inertial Measurement Unit), and a <strong>barometer</strong> to track the rocket‚Äôs position, orientation, and altitude.</li>\n<li><strong>Communication Protocols</strong>: Different protocols are used for specific tasks to optimize performance:\n<ul>\n<li><strong>I2C</strong>: For communicating with the sensors.</li>\n<li><strong>SPI</strong>: For high-speed communication with the NOR flash.</li>\n<li><strong>SDIO</strong>: For communicating with the SD card.</li>\n<li><strong>CAN bus</strong>: For robust communication between the Flight Computer and the Telemetry board.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","metadata":{"headings":[{"depth":2,"slug":"technical-implementation-how-it-works","text":"Technical Implementation: How It Works"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Rocket Flight Computer System","description":"A sophisticated flight computer for rockets featuring STM32MP1 processor, multi-level memory hierarchy, and comprehensive sensor integration.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/RocketFlightComputer.git","tags":["Aerospace","STM32","Flight Control","Embedded Systems","Sensors","Real-time Systems"]},"imagePaths":[]}},"collection":"projects","slug":"flight-computer/flight_computer"},{"id":"personal-website/personal_website.md","data":{"title":"Personal Website: Custom Astro Blog & Interactive Art Canvas","description":"A fully custom, open-source personal website and blog built with Astro, featuring an interactive Impressionist art canvas and a real-time Markdown-to-MDX engine.","pubDate":"2024-07-31T00:00:00.000Z","tags":["Astro","TypeScript","React","Canvas","Open Source","Frontend"],"githubLink":"https://github.com/Laurence-Wu/laurence-wu.github.io.git"},"body":"This is my custom-built personal website. I created it because I was really frustrated with the limitations of my old site, which used a Hexo theme. I wanted more creative freedom and an easier way to customize things, so I decided to build a new site from scratch using the Astro framework. This site is now my personal blog, but it's also a showcase for some of my technical and artistic ideas, especially the interactive homepage and a custom content-processing engine I built.\r\n\r\n#### Why I Built This and What It Does\r\n\r\nThe main reason I built this was that I just couldn't stand the tech on my old site anymore. It was built with a Hexo theme that used **Stylus**, a CSS preprocessor, and JS without a clean architecture. The whole structure was confusing, and the deployment process basically locked me in, killing any real freedom to change things.\r\n\r\nTo fix this, I built a new site that does two main things:\r\n\r\n1. **It's My Personal Blog:** I wanted a place to host articles where I could easily show complex technical info, like mathematical formulas using LATEX and diagrams using Mermaid.js, without any hassle.\r\n2. **It's an Interactive Art Canvas:** This is the part I'm really excited about. I'm a huge fan of **Impressionism and Neo-Impressionism** (like Van Gogh), so I made the homepage a huge canvas where you can draw. As you draw, your input is transformed in real-time into an artistic, Impressionist-style rendering. My goal was to build an algorithm that could let anyone create this style of art easily, while still running smoothly.\r\n\r\n#### The Technical Details\r\n\r\nLet's get into the technical side of things. I kept it simple by making it a frontend-only application hosted on **GitHub Pages**. This meant I didn't have to worry about a backend and could just focus on the user interface.\r\n\r\n- **Framework:** I built the site with **Astro**. I chose it because it's really popular for blogs and has great integrations for all the tools I wanted to use, like TypeScript, React, and those Mermaid graphs and math formulas.\r\n- **Interactive Canvas:** The drawing feature was a fun challenge. To get that smooth, fluid animation, I realized I had to focus on calculating particle **velocities** instead of just their positions. I used derivatives and randomly split angles to create the final artwork, and spent a good amount of time optimizing it for performance.\r\n- **My Custom Markdown to MDX Engine:** One of the biggest pieces of this project was a custom engine I built to automatically convert my standard Markdown (`.md`) files into MDX (`.mdx`). Honestly, manually adding all the JavaScript imports and tags just to show a graph or a formula in MDX is a huge pain. So, I built this engine to handle it for me. It uses **regular expressions** to parse the content‚Äîwhich took a lot of time and tuning to get right. It works in **real-time**, so whenever I save a file, it automatically regenerates the MDX, just like the hot-reloading you see in modern frameworks.\r\n- **Animations and Search:** You'll probably notice I added a lot of **CSS animations**‚ÄîI just really like how they make a site feel alive. I know it might be a bit heavy on older computers since all the rendering happens on your machine, not a server. For the search feature, I just used a pre-existing `npm` package that works really well.\r\n\r\n\r\n\r\n#### Project Summary\r\n\r\nSo, to sum it all up, this website is my personal platform, built from the ground up with Astro because my old site was just too limiting. The parts I'm most proud of are the Impressionist drawing canvas on the homepage and the custom Markdown-to-MDX engine that makes writing posts so much easier. The whole project is entirely frontend-based, heavily animated, and **open-source**. Feel free to check out the code‚Äîif you have ideas or want to contribute, I'd be more than happy to hear from you! It represents a huge personal effort to blend the artistic things I love with some really challenging technical work.","filePath":"src/content/projects/personal-website/personal_website.md","digest":"dd98e9529fec5190","rendered":{"html":"<p>This is my custom-built personal website. I created it because I was really frustrated with the limitations of my old site, which used a Hexo theme. I wanted more creative freedom and an easier way to customize things, so I decided to build a new site from scratch using the Astro framework. This site is now my personal blog, but it‚Äôs also a showcase for some of my technical and artistic ideas, especially the interactive homepage and a custom content-processing engine I built.</p>\n<h4 id=\"why-i-built-this-and-what-it-does\">Why I Built This and What It Does</h4>\n<p>The main reason I built this was that I just couldn‚Äôt stand the tech on my old site anymore. It was built with a Hexo theme that used <strong>Stylus</strong>, a CSS preprocessor, and JS without a clean architecture. The whole structure was confusing, and the deployment process basically locked me in, killing any real freedom to change things.</p>\n<p>To fix this, I built a new site that does two main things:</p>\n<ol>\n<li><strong>It‚Äôs My Personal Blog:</strong> I wanted a place to host articles where I could easily show complex technical info, like mathematical formulas using LATEX and diagrams using Mermaid.js, without any hassle.</li>\n<li><strong>It‚Äôs an Interactive Art Canvas:</strong> This is the part I‚Äôm really excited about. I‚Äôm a huge fan of <strong>Impressionism and Neo-Impressionism</strong> (like Van Gogh), so I made the homepage a huge canvas where you can draw. As you draw, your input is transformed in real-time into an artistic, Impressionist-style rendering. My goal was to build an algorithm that could let anyone create this style of art easily, while still running smoothly.</li>\n</ol>\n<h4 id=\"the-technical-details\">The Technical Details</h4>\n<p>Let‚Äôs get into the technical side of things. I kept it simple by making it a frontend-only application hosted on <strong>GitHub Pages</strong>. This meant I didn‚Äôt have to worry about a backend and could just focus on the user interface.</p>\n<ul>\n<li><strong>Framework:</strong> I built the site with <strong>Astro</strong>. I chose it because it‚Äôs really popular for blogs and has great integrations for all the tools I wanted to use, like TypeScript, React, and those Mermaid graphs and math formulas.</li>\n<li><strong>Interactive Canvas:</strong> The drawing feature was a fun challenge. To get that smooth, fluid animation, I realized I had to focus on calculating particle <strong>velocities</strong> instead of just their positions. I used derivatives and randomly split angles to create the final artwork, and spent a good amount of time optimizing it for performance.</li>\n<li><strong>My Custom Markdown to MDX Engine:</strong> One of the biggest pieces of this project was a custom engine I built to automatically convert my standard Markdown (<code>.md</code>) files into MDX (<code>.mdx</code>). Honestly, manually adding all the JavaScript imports and tags just to show a graph or a formula in MDX is a huge pain. So, I built this engine to handle it for me. It uses <strong>regular expressions</strong> to parse the content‚Äîwhich took a lot of time and tuning to get right. It works in <strong>real-time</strong>, so whenever I save a file, it automatically regenerates the MDX, just like the hot-reloading you see in modern frameworks.</li>\n<li><strong>Animations and Search:</strong> You‚Äôll probably notice I added a lot of <strong>CSS animations</strong>‚ÄîI just really like how they make a site feel alive. I know it might be a bit heavy on older computers since all the rendering happens on your machine, not a server. For the search feature, I just used a pre-existing <code>npm</code> package that works really well.</li>\n</ul>\n<h4 id=\"project-summary\">Project Summary</h4>\n<p>So, to sum it all up, this website is my personal platform, built from the ground up with Astro because my old site was just too limiting. The parts I‚Äôm most proud of are the Impressionist drawing canvas on the homepage and the custom Markdown-to-MDX engine that makes writing posts so much easier. The whole project is entirely frontend-based, heavily animated, and <strong>open-source</strong>. Feel free to check out the code‚Äîif you have ideas or want to contribute, I‚Äôd be more than happy to hear from you! It represents a huge personal effort to blend the artistic things I love with some really challenging technical work.</p>","metadata":{"headings":[{"depth":4,"slug":"why-i-built-this-and-what-it-does","text":"Why I Built This and What It Does"},{"depth":4,"slug":"the-technical-details","text":"The Technical Details"},{"depth":4,"slug":"project-summary","text":"Project Summary"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Personal Website: Custom Astro Blog & Interactive Art Canvas","description":"A fully custom, open-source personal website and blog built with Astro, featuring an interactive Impressionist art canvas and a real-time Markdown-to-MDX engine.","pubDate":"2024-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/laurence-wu.github.io.git","tags":["Astro","TypeScript","React","Canvas","Open Source","Frontend"]},"imagePaths":[]}},"collection":"projects","slug":"personal-website/personal_website"},{"id":"spoiler-alert/Spoiler_alert.md","data":{"title":"Spoiler Alert: Fridge Website to Alert People About Their Spoiled Food","description":"A comprehensive web application for smart fridge management with AI-powered recipe generation and food expiration tracking.","pubDate":"2024-01-15T00:00:00.000Z","tags":["React","Flask","MySQL","HTTPS","Web Development","Full Stack"],"githubLink":"https://github.com/Laurence-Wu/spoiler_alert.git"},"body":"## My feelings about this project\r\n\r\nAs a first-year student, I was excited to build my first professional website. This was at a time before AI was as powerful as it is today. I remember contemplating the entire backend architecture with Flask, weighing it against tools like SQLAlchemy, and handling all the manual work involved.\r\n\r\nThat experience makes me wonder: Is AI truly advancing the journey of a programmer? Will we still feel that same sense of, \"I built this from the ground up,\" after finishing a project? I don't have all the answers, but I feel lucky that I had the chance to experience building a website manually.\r\n\r\n## Features\r\n\r\n#### 1.1 Smart Fridge Management\r\n\r\n- **Visual Fridge Interface**: An Interactive fridge that opens/closes to show contents\r\n- **Food Item Tracking**: Add, view, and manage food items with quantities and expiration dates\r\n- **Expiration Alerts**: Visual warnings for items nearing expiration (within 3-7 days)\r\n- **Freezer Support**: Toggle items between fridge and freezer storage\r\n- **AI-Powered Suggestions**: Automatic category and expiration date suggestions using Google Gemini AI\r\n- **Search & Sort**: Find and organize food items by various attributes\r\n\r\n#### 1.2 Recipe Generation\r\n\r\n- **AI Recipe Generation**: Generate recipes based on available fridge ingredients using Google Gemini AI\r\n- **Multiple Recipe Options**: Generate up to 3 different recipe suggestions per request\r\n- **Detailed Recipe View**: View complete recipe instructions and ingredient lists\r\n\r\n#### 1.3 Shopping List & Wishlist\r\n\r\n- **Recipe-Based Shopping**: Create shopping lists from saved recipes\r\n- **Ingredient Comparison**: Compare recipe requirements with current fridge contents\r\n- **Smart Shopping Lists**: Automatically calculate missing ingredients\r\n\r\n#### 1.4 User Management\r\n\r\n- **User Authentication**: Secure login/signup with password hashing (bcrypt)\r\n- **Multi-User Support**: Each user can have their own fridges and recipes\r\n- **Fridge Sharing**: Support for multiple users accessing shared fridges\r\n\r\n### Technology Stack\r\n\r\n#### 2.1 Frontend\r\n\r\n- **React 18.3.1** - Modern React with hooks and functional components\r\n- **Axios 1.7.7** - HTTP client for API calls\r\n- **Google Generative AI 0.21.0** - Integration with Gemini AI for recipe generation\r\n- **CSS Modules** - Scoped styling\r\n\r\n#### 2.2 Backend\r\n\r\n- **Flask** - Python web framework\r\n- **PyMySQL** - MySQL database connector\r\n- **bcrypt** - Password hashing\r\n- **Flask-CORS** - Cross-origin resource sharing\r\n\r\n### Project Structure\r\n\r\n```\r\nspoiler_alert/\r\n‚îú‚îÄ‚îÄ frontend/                 # React frontend application\r\n‚îÇ   ‚îú‚îÄ‚îÄ src/\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Home/            # Landing page components\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Login/           # Authentication pages\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Signup/          \r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Fridge/          # Main fridge management interface\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Recipes/         # Recipe generation and display\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Wishlist/        # Shopping list management\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Navbar/          # Navigation component\r\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Util.js          # Utility functions and mock data\r\n‚îÇ   ‚îî‚îÄ‚îÄ public/              # Static assets\r\n‚îú‚îÄ‚îÄ backend/                 # Flask backend API\r\n‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Main Flask application\r\n‚îÇ   ‚îú‚îÄ‚îÄ get_data.py         # Database read operations\r\n‚îÇ   ‚îú‚îÄ‚îÄ add_to_table.py     # Database insert operations\r\n‚îÇ   ‚îú‚îÄ‚îÄ remove_from_table.py # Database delete operations\r\n‚îÇ   ‚îú‚îÄ‚îÄ update_table.py     # Database update operations\r\n‚îÇ   ‚îî‚îÄ‚îÄ templates/          # Flask templates (if needed)\r\n‚îî‚îÄ‚îÄ README.md\r\n```","filePath":"src/content/projects/spoiler-alert/Spoiler_alert.md","digest":"9d493c45231bf7d2","rendered":{"html":"<h2 id=\"my-feelings-about-this-project\">My feelings about this project</h2>\n<p>As a first-year student, I was excited to build my first professional website. This was at a time before AI was as powerful as it is today. I remember contemplating the entire backend architecture with Flask, weighing it against tools like SQLAlchemy, and handling all the manual work involved.</p>\n<p>That experience makes me wonder: Is AI truly advancing the journey of a programmer? Will we still feel that same sense of, ‚ÄúI built this from the ground up,‚Äù after finishing a project? I don‚Äôt have all the answers, but I feel lucky that I had the chance to experience building a website manually.</p>\n<h2 id=\"features\">Features</h2>\n<h4 id=\"11-smart-fridge-management\">1.1 Smart Fridge Management</h4>\n<ul>\n<li><strong>Visual Fridge Interface</strong>: An Interactive fridge that opens/closes to show contents</li>\n<li><strong>Food Item Tracking</strong>: Add, view, and manage food items with quantities and expiration dates</li>\n<li><strong>Expiration Alerts</strong>: Visual warnings for items nearing expiration (within 3-7 days)</li>\n<li><strong>Freezer Support</strong>: Toggle items between fridge and freezer storage</li>\n<li><strong>AI-Powered Suggestions</strong>: Automatic category and expiration date suggestions using Google Gemini AI</li>\n<li><strong>Search &#x26; Sort</strong>: Find and organize food items by various attributes</li>\n</ul>\n<h4 id=\"12-recipe-generation\">1.2 Recipe Generation</h4>\n<ul>\n<li><strong>AI Recipe Generation</strong>: Generate recipes based on available fridge ingredients using Google Gemini AI</li>\n<li><strong>Multiple Recipe Options</strong>: Generate up to 3 different recipe suggestions per request</li>\n<li><strong>Detailed Recipe View</strong>: View complete recipe instructions and ingredient lists</li>\n</ul>\n<h4 id=\"13-shopping-list--wishlist\">1.3 Shopping List &#x26; Wishlist</h4>\n<ul>\n<li><strong>Recipe-Based Shopping</strong>: Create shopping lists from saved recipes</li>\n<li><strong>Ingredient Comparison</strong>: Compare recipe requirements with current fridge contents</li>\n<li><strong>Smart Shopping Lists</strong>: Automatically calculate missing ingredients</li>\n</ul>\n<h4 id=\"14-user-management\">1.4 User Management</h4>\n<ul>\n<li><strong>User Authentication</strong>: Secure login/signup with password hashing (bcrypt)</li>\n<li><strong>Multi-User Support</strong>: Each user can have their own fridges and recipes</li>\n<li><strong>Fridge Sharing</strong>: Support for multiple users accessing shared fridges</li>\n</ul>\n<h3 id=\"technology-stack\">Technology Stack</h3>\n<h4 id=\"21-frontend\">2.1 Frontend</h4>\n<ul>\n<li><strong>React 18.3.1</strong> - Modern React with hooks and functional components</li>\n<li><strong>Axios 1.7.7</strong> - HTTP client for API calls</li>\n<li><strong>Google Generative AI 0.21.0</strong> - Integration with Gemini AI for recipe generation</li>\n<li><strong>CSS Modules</strong> - Scoped styling</li>\n</ul>\n<h4 id=\"22-backend\">2.2 Backend</h4>\n<ul>\n<li><strong>Flask</strong> - Python web framework</li>\n<li><strong>PyMySQL</strong> - MySQL database connector</li>\n<li><strong>bcrypt</strong> - Password hashing</li>\n<li><strong>Flask-CORS</strong> - Cross-origin resource sharing</li>\n</ul>\n<h3 id=\"project-structure\">Project Structure</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>spoiler_alert/</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ frontend/                 # React frontend application</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ src/</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Home/            # Landing page components</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Login/           # Authentication pages</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Signup/          </span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Fridge/          # Main fridge management interface</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Recipes/         # Recipe generation and display</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Wishlist/        # Shopping list management</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Navbar/          # Navigation component</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Util.js          # Utility functions and mock data</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îî‚îÄ‚îÄ public/              # Static assets</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ backend/                 # Flask backend API</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Main Flask application</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ get_data.py         # Database read operations</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ add_to_table.py     # Database insert operations</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ remove_from_table.py # Database delete operations</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ update_table.py     # Database update operations</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îî‚îÄ‚îÄ templates/          # Flask templates (if needed)</span></span>\n<span class=\"line\"><span>‚îî‚îÄ‚îÄ README.md</span></span></code></pre>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-about-this-project","text":"My feelings about this project"},{"depth":2,"slug":"features","text":"Features"},{"depth":4,"slug":"11-smart-fridge-management","text":"1.1 Smart Fridge Management"},{"depth":4,"slug":"12-recipe-generation","text":"1.2 Recipe Generation"},{"depth":4,"slug":"13-shopping-list--wishlist","text":"1.3 Shopping List & Wishlist"},{"depth":4,"slug":"14-user-management","text":"1.4 User Management"},{"depth":3,"slug":"technology-stack","text":"Technology Stack"},{"depth":4,"slug":"21-frontend","text":"2.1 Frontend"},{"depth":4,"slug":"22-backend","text":"2.2 Backend"},{"depth":3,"slug":"project-structure","text":"Project Structure"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Spoiler Alert: Fridge Website to Alert People About Their Spoiled Food","description":"A comprehensive web application for smart fridge management with AI-powered recipe generation and food expiration tracking.","pubDate":"2024-01-15T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/spoiler_alert.git","tags":["React","Flask","MySQL","HTTPS","Web Development","Full Stack"]},"imagePaths":[]}},"collection":"projects","slug":"spoiler-alert/spoiler_alert"},{"id":"story-forge/Story Forge An AI chatting game that allows the user to break the linear storyline.md","data":{"title":"Story Forge: AI-Powered Nonlinear Narrative Game","description":"A full-stack AI-driven text adventure game engine that lets players break the linear storyline and co-create unique stories.","pubDate":"2025-07-31T00:00:00.000Z","tags":["AI","Game Engine","React","TypeScript","WebSockets","Prompt Engineering","Full Stack"],"githubLink":"http://115.29.205.103/"},"body":"*(sorry this is not an open source project)*\r\n\r\n## My feelings to this project\r\n\r\nThis is a project that, when I first heard its idea,  a thought just hits me: Fuck everything and I'm gonna make this project happen. It felt like the project I was always meant to work on‚Äîone that connected a deep fascination with philosophy to the art of creating for the web. During that hackathon, our team built it within 3 days and got the first price in software development track.\r\n\r\n## Project Overview\r\n\r\n> A revolutionary text adventure game engine that breaks the boundaries between player agency and narrative coherence\r\n\r\nAt its heart, Story Forge was born from a simple but powerful idea: what if a story wasn't just something you followed, but something you helped create meaning for? We looked at traditional games and saw beautiful, handcrafted tales, but they were almost **always on rails**. You follow the path, maybe choose a branch here or there, but the destination and its meaning are already set for you. **We wanted to break free from that. We imagined a game where the player's journey could give a story its soul.**\r\n\r\n> Think of it like a single word. On its own, a word is just a word. But when you place it in a sentence, surrounded by a story you've built yourself, its meaning transforms. That‚Äôs what we want you to feel. By interacting with the world and its AI-driven characters, you build your own unique path, your own personal context. So when you finally reach that shared ending, it feels different for you than it would for anyone else, because the journey that brought you there was yours and yours alone. It‚Äôs an incredibly personal experience, and for now, we're excited to be sharing it with our players in Chinese.\r\n\r\nPlay the game : StoryForge http://115.29.205.103/\r\n\r\n## Technical Details\r\n\r\nThe project is a full-stack application with a sophisticated architecture designed to handle real-time, AI-driven narrative generation.\r\n\r\n- **Architecture & Tech Stack:**\r\n  - **Frontend:** Built with **React** using **TypeScript**, which was chosen for its strong typing and structural benefits. It includes a separate visual component for rendering the story's dependency tree.\r\n  - **Backend:** The core of the backend is a **dependency tree** that manages narrative logic and prerequisites for events. It uses **JSON** and **YML** files for configuration and data exchange.\r\n  - **Communication Protocol:** The project uses **WebSockets** for real-time, bidirectional communication between the client and server. This was chosen over a standard HTTP API because its streaming nature perfectly matches the dynamic, continuous flow of the storytelling engine.\r\n  - **AI Integration:** The system uses **OpenRouter** to access large language models like **Gemini**. A critical backend component is a custom **prompt engine** designed to force the AI to generate responses in a structured **JSON** format that the dependency tree can parse.\r\n  - **Deployment:** The application is containerized using **Docker** and deployed on **Chinese hosting services** to cater to the target market and avoid issues with the Great Firewall of China.\r\n- **Key Technical Challenges and Learnings:**\r\n  - **Session Management:** Managing **session IDs** in a WebSocket environment was a major challenge. It is crucial for separating the data of different users (and even different game instances from the same user) to prevent narrative overlap. Improper handling could lead to bugs and security vulnerabilities.\r\n  - **AI as a Bottleneck:** The speed of the third-party AI API was the primary performance bottleneck. This was mitigated by implementing **lazy loading** and **streaming** responses to the frontend.\r\n  - **Prompt Engineering:** The team learned that effective **prompt engineering** is a vital and often underestimated skill. Crafting precise prompts was necessary to ensure the AI returned reliable, correctly formatted JSON data for the dependency tree.\r\n  - **Development Strategy:** The team adopted a **decoupling** strategy, building and testing complex features (like the tree renderer) in an isolated environment before integrating them into the main project. They also learned the value of setting up a simple frontend-backend connection test before starting full development.\r\n  - **State and Code Management:** Proper frontend architecture was essential. This included carefully indexing components, separating CSS and TSX files, implementing robust **error handlers** for key functions, and managing distinct application states (e.g., `game_start`, `narrative_output`, `special_choice`).\r\n\r\n## Architecture\r\n\r\n### Backend Components\r\n```\r\nStoryForge/\r\n‚îú‚îÄ‚îÄ engine.py              # Core game engine\r\n‚îú‚îÄ‚îÄ socket_server.py       # WebSocket server\r\n‚îú‚îÄ‚îÄ agents/                # AI agent implementations\r\n‚îÇ   ‚îú‚îÄ‚îÄ arbiter.py         # Decision validation\r\n‚îÇ   ‚îú‚îÄ‚îÄ narrator.py        # Story generation\r\n‚îÇ   ‚îî‚îÄ‚îÄ reality_agent.py   # Reality reconstruction\r\n‚îú‚îÄ‚îÄ models/                # Data models\r\n‚îÇ   ‚îú‚îÄ‚îÄ story.py           # Story structures\r\n‚îÇ   ‚îú‚îÄ‚îÄ context.py         # Game context\r\n‚îÇ   ‚îî‚îÄ‚îÄ agents.py          # Agent interfaces\r\n‚îú‚îÄ‚îÄ utils/                 # Utilities\r\n‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # LLM integration\r\n‚îÇ   ‚îú‚îÄ‚îÄ save_manager.py    # Save system\r\n‚îÇ   ‚îî‚îÄ‚îÄ command_handler.py # Command processing\r\n‚îî‚îÄ‚îÄ data/story/            # Story definitions\r\n    ‚îî‚îÄ‚îÄ [story-name]/\r\n        ‚îú‚îÄ‚îÄ world_definition.yaml\r\n        ‚îú‚îÄ‚îÄ condition_tree.yaml\r\n        ‚îî‚îÄ‚îÄ images/\r\n```\r\n\r\n### Frontend Components\r\n```\r\nfrontend/\r\n‚îú‚îÄ‚îÄ src/\r\n‚îÇ   ‚îú‚îÄ‚îÄ components/        # React components\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Stage.tsx      # Main game display\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Console.tsx    # Command interface\r\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EventTree.tsx  # Decision history\r\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ UserActions.tsx # Action buttons\r\n‚îÇ   ‚îú‚îÄ‚îÄ hooks/             # Custom React hooks\r\n‚îÇ   ‚îú‚îÄ‚îÄ services/          # API services\r\n‚îÇ   ‚îî‚îÄ‚îÄ types/             # TypeScript definitions\r\n‚îú‚îÄ‚îÄ public/                # Static assets\r\n‚îî‚îÄ‚îÄ dist/                  # Built application\r\n```\r\n\r\n### Data Flow\r\n```\r\nPlayer Input ‚Üí Socket.IO ‚Üí Engine ‚Üí AI Agents ‚Üí Story Generation ‚Üí Frontend Update\r\n     ‚Üë                                    ‚Üì\r\nSave System ‚Üê Delta Storage ‚Üê Game State ‚Üê Reality Validation\r\n```\r\n\r\n## Configuration\r\n\r\n### Story Creation\r\nCreate new stories by adding directories to `data/story/`:\r\n\r\n**world_definition.yaml**\r\n```yaml\r\nworld_definition:\r\n  story_name: \"Your Story Title\"\r\n  core_concept: \"Central theme or question\"\r\n  ontological_layers:\r\n    surface_narrative:\r\n      official_story: \"What appears to happen\"\r\n      rules: [\"Surface world rules\"]\r\n    underlying_structure:\r\n      hidden_truth: \"Deeper reality\"\r\n      rules: [\"Hidden mechanics\"]\r\n  protagonist_intent: \"Player's initial goal\"\r\n  endings:\r\n    good_ending: \"Positive resolution\"\r\n    bad_ending: \"Negative outcome\"\r\n```\r\n\r\n**condition_tree.yaml**\r\n\r\n```yaml\r\ntree_id: \"your_story\"\r\nroot_conditions: [\"start\"]\r\nterminal_conditions: [\"ending\"]\r\nnodes:\r\n  - node_id: \"start\"\r\n    symbolic_goal: \"Begin the adventure\"\r\n    default_interpretation: \"You find yourself...\"\r\n    dependencies: []\r\n```","filePath":"src/content/projects/story-forge/Story Forge An AI chatting game that allows the user to break the linear storyline.md","digest":"29aab2acca0992cb","rendered":{"html":"<p><em>(sorry this is not an open source project)</em></p>\n<h2 id=\"my-feelings-to-this-project\">My feelings to this project</h2>\n<p>This is a project that, when I first heard its idea,  a thought just hits me: Fuck everything and I‚Äôm gonna make this project happen. It felt like the project I was always meant to work on‚Äîone that connected a deep fascination with philosophy to the art of creating for the web. During that hackathon, our team built it within 3 days and got the first price in software development track.</p>\n<h2 id=\"project-overview\">Project Overview</h2>\n<blockquote>\n<p>A revolutionary text adventure game engine that breaks the boundaries between player agency and narrative coherence</p>\n</blockquote>\n<p>At its heart, Story Forge was born from a simple but powerful idea: what if a story wasn‚Äôt just something you followed, but something you helped create meaning for? We looked at traditional games and saw beautiful, handcrafted tales, but they were almost <strong>always on rails</strong>. You follow the path, maybe choose a branch here or there, but the destination and its meaning are already set for you. <strong>We wanted to break free from that. We imagined a game where the player‚Äôs journey could give a story its soul.</strong></p>\n<blockquote>\n<p>Think of it like a single word. On its own, a word is just a word. But when you place it in a sentence, surrounded by a story you‚Äôve built yourself, its meaning transforms. That‚Äôs what we want you to feel. By interacting with the world and its AI-driven characters, you build your own unique path, your own personal context. So when you finally reach that shared ending, it feels different for you than it would for anyone else, because the journey that brought you there was yours and yours alone. It‚Äôs an incredibly personal experience, and for now, we‚Äôre excited to be sharing it with our players in Chinese.</p>\n</blockquote>\n<p>Play the game : StoryForge <a href=\"http://115.29.205.103/\">http://115.29.205.103/</a></p>\n<h2 id=\"technical-details\">Technical Details</h2>\n<p>The project is a full-stack application with a sophisticated architecture designed to handle real-time, AI-driven narrative generation.</p>\n<ul>\n<li><strong>Architecture &#x26; Tech Stack:</strong>\n<ul>\n<li><strong>Frontend:</strong> Built with <strong>React</strong> using <strong>TypeScript</strong>, which was chosen for its strong typing and structural benefits. It includes a separate visual component for rendering the story‚Äôs dependency tree.</li>\n<li><strong>Backend:</strong> The core of the backend is a <strong>dependency tree</strong> that manages narrative logic and prerequisites for events. It uses <strong>JSON</strong> and <strong>YML</strong> files for configuration and data exchange.</li>\n<li><strong>Communication Protocol:</strong> The project uses <strong>WebSockets</strong> for real-time, bidirectional communication between the client and server. This was chosen over a standard HTTP API because its streaming nature perfectly matches the dynamic, continuous flow of the storytelling engine.</li>\n<li><strong>AI Integration:</strong> The system uses <strong>OpenRouter</strong> to access large language models like <strong>Gemini</strong>. A critical backend component is a custom <strong>prompt engine</strong> designed to force the AI to generate responses in a structured <strong>JSON</strong> format that the dependency tree can parse.</li>\n<li><strong>Deployment:</strong> The application is containerized using <strong>Docker</strong> and deployed on <strong>Chinese hosting services</strong> to cater to the target market and avoid issues with the Great Firewall of China.</li>\n</ul>\n</li>\n<li><strong>Key Technical Challenges and Learnings:</strong>\n<ul>\n<li><strong>Session Management:</strong> Managing <strong>session IDs</strong> in a WebSocket environment was a major challenge. It is crucial for separating the data of different users (and even different game instances from the same user) to prevent narrative overlap. Improper handling could lead to bugs and security vulnerabilities.</li>\n<li><strong>AI as a Bottleneck:</strong> The speed of the third-party AI API was the primary performance bottleneck. This was mitigated by implementing <strong>lazy loading</strong> and <strong>streaming</strong> responses to the frontend.</li>\n<li><strong>Prompt Engineering:</strong> The team learned that effective <strong>prompt engineering</strong> is a vital and often underestimated skill. Crafting precise prompts was necessary to ensure the AI returned reliable, correctly formatted JSON data for the dependency tree.</li>\n<li><strong>Development Strategy:</strong> The team adopted a <strong>decoupling</strong> strategy, building and testing complex features (like the tree renderer) in an isolated environment before integrating them into the main project. They also learned the value of setting up a simple frontend-backend connection test before starting full development.</li>\n<li><strong>State and Code Management:</strong> Proper frontend architecture was essential. This included carefully indexing components, separating CSS and TSX files, implementing robust <strong>error handlers</strong> for key functions, and managing distinct application states (e.g., <code>game_start</code>, <code>narrative_output</code>, <code>special_choice</code>).</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"architecture\">Architecture</h2>\n<h3 id=\"backend-components\">Backend Components</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>StoryForge/</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ engine.py              # Core game engine</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ socket_server.py       # WebSocket server</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ agents/                # AI agent implementations</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ arbiter.py         # Decision validation</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ narrator.py        # Story generation</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îî‚îÄ‚îÄ reality_agent.py   # Reality reconstruction</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ models/                # Data models</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ story.py           # Story structures</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ context.py         # Game context</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îî‚îÄ‚îÄ agents.py          # Agent interfaces</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ utils/                 # Utilities</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # LLM integration</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ save_manager.py    # Save system</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îî‚îÄ‚îÄ command_handler.py # Command processing</span></span>\n<span class=\"line\"><span>‚îî‚îÄ‚îÄ data/story/            # Story definitions</span></span>\n<span class=\"line\"><span>    ‚îî‚îÄ‚îÄ [story-name]/</span></span>\n<span class=\"line\"><span>        ‚îú‚îÄ‚îÄ world_definition.yaml</span></span>\n<span class=\"line\"><span>        ‚îú‚îÄ‚îÄ condition_tree.yaml</span></span>\n<span class=\"line\"><span>        ‚îî‚îÄ‚îÄ images/</span></span></code></pre>\n<h3 id=\"frontend-components\">Frontend Components</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>frontend/</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ src/</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ components/        # React components</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Stage.tsx      # Main game display</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Console.tsx    # Command interface</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EventTree.tsx  # Decision history</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ UserActions.tsx # Action buttons</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ hooks/             # Custom React hooks</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îú‚îÄ‚îÄ services/          # API services</span></span>\n<span class=\"line\"><span>‚îÇ   ‚îî‚îÄ‚îÄ types/             # TypeScript definitions</span></span>\n<span class=\"line\"><span>‚îú‚îÄ‚îÄ public/                # Static assets</span></span>\n<span class=\"line\"><span>‚îî‚îÄ‚îÄ dist/                  # Built application</span></span></code></pre>\n<h3 id=\"data-flow\">Data Flow</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>Player Input ‚Üí Socket.IO ‚Üí Engine ‚Üí AI Agents ‚Üí Story Generation ‚Üí Frontend Update</span></span>\n<span class=\"line\"><span>     ‚Üë                                    ‚Üì</span></span>\n<span class=\"line\"><span>Save System ‚Üê Delta Storage ‚Üê Game State ‚Üê Reality Validation</span></span></code></pre>\n<h2 id=\"configuration\">Configuration</h2>\n<h3 id=\"story-creation\">Story Creation</h3>\n<p>Create new stories by adding directories to <code>data/story/</code>:</p>\n<p><strong>world_definition.yaml</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"yaml\"><code><span class=\"line\"><span style=\"color:#85E89D\">world_definition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  story_name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Your Story Title\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  core_concept</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Central theme or question\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  ontological_layers</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    surface_narrative</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      official_story</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"What appears to happen\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      rules</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"Surface world rules\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    underlying_structure</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      hidden_truth</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Deeper reality\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      rules</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"Hidden mechanics\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  protagonist_intent</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Player's initial goal\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  endings</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    good_ending</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Positive resolution\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    bad_ending</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Negative outcome\"</span></span></code></pre>\n<p><strong>condition_tree.yaml</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"yaml\"><code><span class=\"line\"><span style=\"color:#85E89D\">tree_id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"your_story\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">root_conditions</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"start\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">terminal_conditions</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"ending\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">nodes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">node_id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"start\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    symbolic_goal</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Begin the adventure\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    default_interpretation</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"You find yourself...\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    dependencies</span><span style=\"color:#E1E4E8\">: []</span></span></code></pre>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-to-this-project","text":"My feelings to this project"},{"depth":2,"slug":"project-overview","text":"Project Overview"},{"depth":2,"slug":"technical-details","text":"Technical Details"},{"depth":2,"slug":"architecture","text":"Architecture"},{"depth":3,"slug":"backend-components","text":"Backend Components"},{"depth":3,"slug":"frontend-components","text":"Frontend Components"},{"depth":3,"slug":"data-flow","text":"Data Flow"},{"depth":2,"slug":"configuration","text":"Configuration"},{"depth":3,"slug":"story-creation","text":"Story Creation"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Story Forge: AI-Powered Nonlinear Narrative Game","description":"A full-stack AI-driven text adventure game engine that lets players break the linear storyline and co-create unique stories.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"http://115.29.205.103/","tags":["AI","Game Engine","React","TypeScript","WebSockets","Prompt Engineering","Full Stack"]},"imagePaths":[]}},"collection":"projects","slug":"story-forge/story-forge-an-ai-chatting-game-that-allows-the-user-to-break-the-linear-storyline"}];
const getUrl = undefined;

  // Dynamic import for MiniSearch to handle browser compatibility
  async function loadMiniSearch() {
    try {
      const { default: MiniSearch } = await import('minisearch');
      return MiniSearch;
    } catch (error) {
      return null;
    }
  }
  
  class SearchManager {
    constructor() {
      this.miniSearch = null;
      this.searchInput = document.getElementById('search-input');
      this.searchIcon = document.getElementById('search-icon');
      this.searchResults = document.getElementById('search-results');
      this.resultsList = document.getElementById('results-list');
      this.resultsCount = document.getElementById('results-count');
      this.clearBtn = document.getElementById('clear-search');
      
      this.init();
    }
    
    async init() {
      const MiniSearch = await loadMiniSearch();
      if (!MiniSearch) {
        this.setupFallbackSearch();
        return;
      }
      
      this.miniSearch = new MiniSearch({
        fields: ['title', 'description', 'tags', 'content'],
        storeFields: ['title', 'description', 'pubDate', 'type', 'slug', 'tags'],
        searchOptions: {
          boost: { title: 2, description: 1.5 },
          fuzzy: 0.2,
          prefix: true
        }
      });
      
      this.initializeData();
      this.bindEvents();
    }
    
    setupFallbackSearch() {
      // Fallback search without MiniSearch
      this.documents = this.prepareDocuments();
      this.bindEvents();
    }
    
    prepareDocuments() {
      const documents = [];
      
      // Add blog posts
      allBlogPosts.forEach(post => {
        documents.push({
          id: `blog-${post.slug}`,
          title: post.data.title,
          description: post.data.description,
          pubDate: post.data.pubDate,
          type: 'blog',
          slug: post.slug,
          tags: (post.data.tags || []).join(' '),
          content: `${post.data.title} ${post.data.description} ${(post.data.tags || []).join(' ')}`
        });
      });
      
      // Add projects
      allProjects.forEach(project => {
        documents.push({
          id: `project-${project.slug}`,
          title: project.data.title,
          description: project.data.description,
          pubDate: project.data.pubDate,
          type: 'project',
          slug: project.slug,
          tags: (project.data.tags || []).join(' '),
          content: `${project.data.title} ${project.data.description} ${(project.data.tags || []).join(' ')}`
        });
      });
      
      return documents;
    }
    
    initializeData() {
      const documents = this.prepareDocuments();
      this.miniSearch.addAll(documents);
    }
    
    bindEvents() {
      // Real-time search on input
      this.searchInput.addEventListener('input', this.handleRealtimeSearch.bind(this));
      
      // Search on Enter key press
      this.searchInput.addEventListener('keydown', (e) => {
        if (e.key === 'Enter') {
          e.preventDefault();
          this.performSearch();
        } else if (e.key === 'Escape') {
          this.clearSearch();
        }
      });
      
      // Search on icon button click
      this.searchIcon.addEventListener('click', this.performSearch.bind(this));
      
      // Clear search
      this.clearBtn.addEventListener('click', this.clearSearch.bind(this));
      
      // Close results when clicking outside
      document.addEventListener('click', (e) => {
        if (!e.target.closest('.search-container')) {
          this.hideResults();
        }
      });
      
      // Reposition results on window resize
      window.addEventListener('resize', () => {
        if (!this.searchResults.classList.contains('hidden')) {
          this.positionResults();
        }
      });
      
      // Reposition results on scroll
      window.addEventListener('scroll', () => {
        if (!this.searchResults.classList.contains('hidden')) {
          this.positionResults();
        }
      });
    }
    
    handleRealtimeSearch(e) {
      const query = e.target.value.trim();
      
      // Only show real-time results for queries of 3+ characters
      if (query.length >= 3) {
        const results = this.search(query, 8);
        this.displayResults(results, query);
      } else {
        this.hideResults();
      }
    }
    
    performSearch() {
      const query = this.searchInput.value.trim();
      
      if (query.length < 2) {
        // Show message for short queries
        this.displayNoResults('Please enter at least 2 characters to search');
        return;
      }
      
      const results = this.search(query, 8);
      this.displayResults(results, query);
    }
    
    search(query, limit = 8) {
      if (this.miniSearch) {
        return this.miniSearch.search(query, { limit });
      } else {
        // Fallback search
        return this.fallbackSearch(query, limit);
      }
    }
    
    fallbackSearch(query, limit) {
      const queryLower = query.toLowerCase();
      const results = [];
      
      for (const doc of this.documents) {
        let score = 0;
        const titleMatch = doc.title.toLowerCase().includes(queryLower);
        const descMatch = doc.description.toLowerCase().includes(queryLower);
        const tagsMatch = doc.tags.toLowerCase().includes(queryLower);
        
        if (titleMatch) score += 2;
        if (descMatch) score += 1.5;
        if (tagsMatch) score += 1;
        
        if (score > 0) {
          results.push({ ...doc, score });
        }
      }
      
      return results
        .sort((a, b) => b.score - a.score)
        .slice(0, limit);
    }
    
    displayResults(results, query) {
      this.resultsCount.textContent = `${results.length} result${results.length !== 1 ? 's' : ''}`;
      
      if (results.length === 0) {
        this.displayNoResults(`No results found for "${query}"`);
      } else {
        this.resultsList.innerHTML = results.map(result => {
          const date = new Date(result.pubDate).toLocaleDateString('en-US', {
            year: 'numeric',
            month: 'short'
          });
          
          const url = result.type === 'blog' 
            ? `/blog/${result.slug}/`
            : `/projects/${result.slug}/`;
          
          return `
            <a href="${url}" class="result-item">
              <div class="result-title">${this.highlightText(result.title, query)}</div>
              <div class="result-description">${this.highlightText(result.description, query)}</div>
              <div class="result-meta">
                <span class="result-type">${result.type}</span>
                <span class="result-date">üìÖ ${date}</span>
              </div>
            </a>
          `;
        }).join('');
      }
      
      this.showResults();
    }
    
    displayNoResults(message) {
      this.resultsCount.textContent = '0 results';
      this.resultsList.innerHTML = `
        <div class="no-results">
          <div class="no-results-icon">üîç</div>
          <div class="no-results-message">${message}</div>
          <div class="no-results-hint">Try using different keywords or check your spelling</div>
        </div>
      `;
      this.showResults();
    }
    
    highlightText(text, query) {
      if (!query || !text) return text;
      
      const regex = new RegExp(`(${query.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')})`, 'gi');
      return text.replace(regex, '<mark>$1</mark>');
    }
    
    showResults() {
      this.searchResults.classList.remove('hidden');
      this.positionResults();
    }
    
    positionResults() {
      const searchContainer = this.searchInput.closest('.search-container');
      const rect = searchContainer.getBoundingClientRect();
      const viewport = {
        height: window.innerHeight,
        width: window.innerWidth
      };
      
      // Calculate available space below and above
      const spaceBelow = viewport.height - rect.bottom;
      const spaceAbove = rect.top;
      
      // Position the results dropdown
      if (spaceBelow >= 300 || spaceBelow >= spaceAbove) {
        // Show below if there's enough space or more space below than above
        this.searchResults.style.top = `${rect.bottom + 4}px`;
        this.searchResults.style.maxHeight = `${Math.min(300, spaceBelow - 20)}px`;
      } else {
        // Show above if more space above
        this.searchResults.style.top = `${rect.top - Math.min(300, spaceAbove - 20) - 4}px`;
        this.searchResults.style.maxHeight = `${Math.min(300, spaceAbove - 20)}px`;
      }
      
      this.searchResults.style.left = `${rect.left}px`;
      this.searchResults.style.width = `${rect.width}px`;
    }
    
    hideResults() {
      this.searchResults.classList.add('hidden');
    }
    
    clearSearch() {
      this.searchInput.value = '';
      this.hideResults();
      this.searchInput.focus();
    }
  }
  
  // Initialize search when DOM is loaded
  document.addEventListener('DOMContentLoaded', () => {
    new SearchManager();
  });
</script>  </div> <div class="sidebar-section" data-astro-cid-5tznm7mj> <h3 data-astro-cid-5tznm7mj>Categories</h3> <div class="tag-cloud" data-astro-cid-5tznm7mj> <a href="/blog/tag/triton/" class="tag-filter" data-astro-cid-5tznm7mj>triton</a><a href="/blog/tag/GPU/" class="tag-filter" data-astro-cid-5tznm7mj>GPU</a><a href="/blog/tag/CUDA/" class="tag-filter" data-astro-cid-5tznm7mj>CUDA</a><a href="/blog/tag/compiler/" class="tag-filter" data-astro-cid-5tznm7mj>compiler</a><a href="/blog/tag/code/" class="tag-filter" data-astro-cid-5tznm7mj>code</a><a href="/blog/tag/optimization/" class="tag-filter" data-astro-cid-5tznm7mj>optimization</a><a href="/blog/tag/machine-learning/" class="tag-filter" data-astro-cid-5tznm7mj>machine-learning</a><a href="/blog/tag/quantization/" class="tag-filter" data-astro-cid-5tznm7mj>quantization</a><a href="/blog/tag/QAT/" class="tag-filter" data-astro-cid-5tznm7mj>QAT</a><a href="/blog/tag/LoRA/" class="tag-filter" data-astro-cid-5tznm7mj>LoRA</a><a href="/blog/tag/pytorch/" class="tag-filter" data-astro-cid-5tznm7mj>pytorch</a><a href="/blog/tag/precision-training/" class="tag-filter" data-astro-cid-5tznm7mj>precision-training</a><a href="/blog/tag/wikitext/" class="tag-filter" data-astro-cid-5tznm7mj>wikitext</a><a href="/blog/tag/model-compression/" class="tag-filter" data-astro-cid-5tznm7mj>model-compression</a><a href="/blog/tag/SystemVerilog/" class="tag-filter" data-astro-cid-5tznm7mj>SystemVerilog</a><a href="/blog/tag/hardware/" class="tag-filter" data-astro-cid-5tznm7mj>hardware</a><a href="/blog/tag/FPGA/" class="tag-filter" data-astro-cid-5tznm7mj>FPGA</a><a href="/blog/tag/digital-design/" class="tag-filter" data-astro-cid-5tznm7mj>digital-design</a><a href="/blog/tag/learning-journal/" class="tag-filter" data-astro-cid-5tznm7mj>learning-journal</a> </div> </div> </div> <div class="blog-posts" data-astro-cid-5tznm7mj> <article class="blog-post-card" data-astro-cid-5tznm7mj> <header data-astro-cid-5tznm7mj> <h2 data-astro-cid-5tznm7mj> <a href="/blog/matrix-multiplication/" data-astro-cid-5tznm7mj>Intro to Triton with Matrix Multiplication</a> </h2> <div class="post-meta" data-astro-cid-5tznm7mj> <time datetime="2025-03-18T00:00:00.000Z" data-astro-cid-5tznm7mj> March 17, 2025 </time> <span class="author" data-astro-cid-5tznm7mj>by Xiaoyou Wu</span> </div> </header> <p class="post-description" data-astro-cid-5tznm7mj>Introduction to GPU programming with Triton and build the matrix multiplication along the way</p> <div class="post-tags" data-astro-cid-5tznm7mj> <a href="/blog/tag/triton/" class="tag" data-astro-cid-5tznm7mj>triton</a><a href="/blog/tag/GPU/" class="tag" data-astro-cid-5tznm7mj>GPU</a><a href="/blog/tag/CUDA/" class="tag" data-astro-cid-5tznm7mj>CUDA</a><a href="/blog/tag/compiler/" class="tag" data-astro-cid-5tznm7mj>compiler</a><a href="/blog/tag/code/" class="tag" data-astro-cid-5tznm7mj>code</a><a href="/blog/tag/optimization/" class="tag" data-astro-cid-5tznm7mj>optimization</a> </div> <a href="/blog/matrix-multiplication/" class="read-more" data-astro-cid-5tznm7mj>
Read more ‚Üí
</a> </article><article class="blog-post-card" data-astro-cid-5tznm7mj> <header data-astro-cid-5tznm7mj> <h2 data-astro-cid-5tznm7mj> <a href="/blog/qat-implementation/" data-astro-cid-5tznm7mj>Quantization Aware Training Implementation Guide</a> </h2> <div class="post-meta" data-astro-cid-5tznm7mj> <time datetime="2025-01-15T00:00:00.000Z" data-astro-cid-5tznm7mj> January 14, 2025 </time> <span class="author" data-astro-cid-5tznm7mj>by Xiaoyou Wu</span> </div> </header> <p class="post-description" data-astro-cid-5tznm7mj>Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies</p> <div class="post-tags" data-astro-cid-5tznm7mj> <a href="/blog/tag/machine-learning/" class="tag" data-astro-cid-5tznm7mj>machine-learning</a><a href="/blog/tag/quantization/" class="tag" data-astro-cid-5tznm7mj>quantization</a><a href="/blog/tag/QAT/" class="tag" data-astro-cid-5tznm7mj>QAT</a><a href="/blog/tag/LoRA/" class="tag" data-astro-cid-5tznm7mj>LoRA</a><a href="/blog/tag/GPU/" class="tag" data-astro-cid-5tznm7mj>GPU</a><a href="/blog/tag/optimization/" class="tag" data-astro-cid-5tznm7mj>optimization</a><a href="/blog/tag/pytorch/" class="tag" data-astro-cid-5tznm7mj>pytorch</a> </div> <a href="/blog/qat-implementation/" class="read-more" data-astro-cid-5tznm7mj>
Read more ‚Üí
</a> </article><article class="blog-post-card" data-astro-cid-5tznm7mj> <header data-astro-cid-5tznm7mj> <h2 data-astro-cid-5tznm7mj> <a href="/blog/eic_interview_report/" data-astro-cid-5tznm7mj>Quantization Strategy Report: Switchable and Cyclic Precision Training</a> </h2> <div class="post-meta" data-astro-cid-5tznm7mj> <time datetime="2025-01-10T00:00:00.000Z" data-astro-cid-5tznm7mj> January 9, 2025 </time> <span class="author" data-astro-cid-5tznm7mj>by Xiaoyou Wu</span> </div> </header> <p class="post-description" data-astro-cid-5tznm7mj>Comprehensive report on quantization strategies including switchable precision and cyclic precision training applied to WikiText-103 dataset</p> <div class="post-tags" data-astro-cid-5tznm7mj> <a href="/blog/tag/quantization/" class="tag" data-astro-cid-5tznm7mj>quantization</a><a href="/blog/tag/machine-learning/" class="tag" data-astro-cid-5tznm7mj>machine-learning</a><a href="/blog/tag/precision-training/" class="tag" data-astro-cid-5tznm7mj>precision-training</a><a href="/blog/tag/wikitext/" class="tag" data-astro-cid-5tznm7mj>wikitext</a><a href="/blog/tag/model-compression/" class="tag" data-astro-cid-5tznm7mj>model-compression</a> </div> <a href="/blog/eic_interview_report/" class="read-more" data-astro-cid-5tznm7mj>
Read more ‚Üí
</a> </article><article class="blog-post-card" data-astro-cid-5tznm7mj> <header data-astro-cid-5tznm7mj> <h2 data-astro-cid-5tznm7mj> <a href="/blog/vip_progress/" data-astro-cid-5tznm7mj>VIP Progress Journal - SystemVerilog Learning</a> </h2> <div class="post-meta" data-astro-cid-5tznm7mj> <time datetime="2024-08-25T00:00:00.000Z" data-astro-cid-5tznm7mj> August 24, 2024 </time> <span class="author" data-astro-cid-5tznm7mj>by Xiaoyou Wu</span> </div> </header> <p class="post-description" data-astro-cid-5tznm7mj>Weekly progress journal documenting my journey learning SystemVerilog, including module structures, always blocks, and FSM implementations</p> <div class="post-tags" data-astro-cid-5tznm7mj> <a href="/blog/tag/SystemVerilog/" class="tag" data-astro-cid-5tznm7mj>SystemVerilog</a><a href="/blog/tag/hardware/" class="tag" data-astro-cid-5tznm7mj>hardware</a><a href="/blog/tag/FPGA/" class="tag" data-astro-cid-5tznm7mj>FPGA</a><a href="/blog/tag/digital-design/" class="tag" data-astro-cid-5tznm7mj>digital-design</a><a href="/blog/tag/learning-journal/" class="tag" data-astro-cid-5tznm7mj>learning-journal</a> </div> <a href="/blog/vip_progress/" class="read-more" data-astro-cid-5tznm7mj>
Read more ‚Üí
</a> </article> </div> </div> </section>   </main>  <!-- KaTeX JavaScript for Math Rendering --> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInDocument()"></script> <script>
    function renderMathInDocument() {
      if (typeof window.renderMathInElement !== 'undefined') {
        window.renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError: false
        });
      }
    }
    
    document.addEventListener("DOMContentLoaded", renderMathInDocument);
  </script> <!-- Simple Mermaid Test Script --> <script type="module">
    console.log('[Simple Mermaid Test] Starting...');

    // Simple function to load and initialize Mermaid
    async function loadAndTestMermaid() {
      try {
        console.log('[Simple Mermaid Test] Loading Mermaid from CDN...');
        
        // Load Mermaid if not already loaded
        if (!window.mermaid) {
          const script = document.createElement('script');
          script.src = 'https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js';
          
          await new Promise((resolve, reject) => {
            script.onload = resolve;
            script.onerror = reject;
            document.head.appendChild(script);
          });
          
          console.log('[Simple Mermaid Test] Mermaid loaded from CDN');
        } else {
          console.log('[Simple Mermaid Test] Mermaid already available');
        }

        if (!window.mermaid) {
          throw new Error('Mermaid not available after loading');
        }

        console.log('[Simple Mermaid Test] Initializing Mermaid...');
        
        // Initialize Mermaid with simple config
        window.mermaid.initialize({
          startOnLoad: false,
          theme: 'neutral'
        });

        console.log('[Simple Mermaid Test] Finding diagram elements...');
        
        // Find all mermaid diagrams
        const diagrams = document.querySelectorAll('.mermaid-diagram');
        console.log(`[Simple Mermaid Test] Found ${diagrams.length} diagrams`);

        // Process each diagram
        for (let i = 0; i < diagrams.length; i++) {
          const diagram = diagrams[i];
          const id = diagram.id || `test-diagram-${i}`;
          const codeElement = diagram.querySelector('.mermaid-code code');
          
          console.log(`[Simple Mermaid Test] Processing diagram ${id}`);
          
          if (codeElement) {
            const code = codeElement.textContent.trim();
            console.log(`[Simple Mermaid Test] Code for ${id}:`, code.substring(0, 100) + '...');
            
            try {
              // Render the diagram
              const { svg } = await window.mermaid.render(`${id}-render`, code);
              console.log(`[Simple Mermaid Test] SVG generated for ${id}`);
              
              // Replace the loading text with the SVG
              const container = diagram.querySelector('.mermaid-container');
              if (container) {
                container.innerHTML = svg;
                console.log(`[Simple Mermaid Test] SVG inserted for ${id}`);
              } else {
                console.error(`[Simple Mermaid Test] No container found for ${id}`);
              }
              
            } catch (renderError) {
              console.error(`[Simple Mermaid Test] Render error for ${id}:`, renderError);
              
              const container = diagram.querySelector('.mermaid-container');
              if (container) {
                container.innerHTML = `<div style="color: red; padding: 1rem;">Error: ${renderError.message}</div>`;
              }
            }
          } else {
            console.error(`[Simple Mermaid Test] No code element found for ${id}`);
          }
        }

        console.log('[Simple Mermaid Test] Completed processing all diagrams');
        
      } catch (error) {
        console.error('[Simple Mermaid Test] Fatal error:', error);
      }
    }

    // Run the test when DOM is ready
    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', loadAndTestMermaid);
    } else {
      loadAndTestMermaid();
    }

    // Export for manual testing
    window.testMermaid = loadAndTestMermaid;
  </script> </body> </html>  