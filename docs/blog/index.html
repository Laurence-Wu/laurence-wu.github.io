<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Blog - Laurence</title><meta name="description" content="Read my latest thoughts on technology, engineering, and various interests."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://laurence-wu.github.io/blog/"><meta property="og:title" content="Blog - Laurence"><meta property="og:description" content="Read my latest thoughts on technology, engineering, and various interests."><meta property="og:image" content="/assets/profile-image.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://laurence-wu.github.io/blog/"><meta property="twitter:title" content="Blog - Laurence"><meta property="twitter:description" content="Read my latest thoughts on technology, engineering, and various interests."><meta property="twitter:image" content="/assets/profile-image.jpg"><!-- KaTeX CSS for Math Rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous"><!-- Google Fonts - Preserved Typography System --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;500;600;700&family=Montserrat:wght@300;400;500;600;700&family=Cormorant+Garamond:wght@300;400;500;600;700&display=swap" rel="stylesheet"><!-- Fira Code for code blocks --><link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap" rel="stylesheet"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Global CSS Import --><style>@keyframes floatingWave{0%,to{transform:translate(0) translateY(0) scale(1);background-position:0% 0%,100% 100%,50% 50%}25%{transform:translate(10px) translateY(-15px) scale(1.02);background-position:10% 10%,90% 90%,60% 40%}50%{transform:translate(-5px) translateY(10px) scale(.98);background-position:20% 30%,80% 70%,30% 70%}75%{transform:translate(-15px) translateY(-5px) scale(1.01);background-position:30% 20%,70% 80%,40% 60%}}@keyframes gridWave{0%,to{transform:translate(0) translateY(0) rotate(0);opacity:.6}33%{transform:translate(20px) translateY(-10px) rotate(.5deg);opacity:.4}66%{transform:translate(-10px) translateY(15px) rotate(-.3deg);opacity:.8}}.main-content[data-astro-cid-dmqsi53g]{margin-top:0;min-height:calc(100vh - 80px);position:relative;z-index:1}.main-content[data-astro-cid-dmqsi53g]>[data-astro-cid-dmqsi53g]{position:relative;z-index:2}.main-content[data-astro-cid-dmqsi53g]:before{content:"";position:fixed;inset:80px 0 0;background:radial-gradient(circle at 20% 20%,rgba(138,154,126,.15) 0%,transparent 50%),radial-gradient(circle at 80% 80%,rgba(212,175,55,.1) 0%,transparent 50%),radial-gradient(circle at 40% 60%,rgba(138,154,126,.08) 0%,transparent 50%);background-color:#f4e8d1;background-size:100px 100px,150px 150px,200px 200px;animation:floatingWave 20s ease-in-out infinite;z-index:-1;pointer-events:none}.main-content[data-astro-cid-dmqsi53g]:after{content:"";position:fixed;inset:80px 0 0;background-image:linear-gradient(rgba(138,154,126,.05) 1px,transparent 1px),linear-gradient(90deg,rgba(138,154,126,.05) 1px,transparent 1px),linear-gradient(rgba(212,175,55,.03) 1px,transparent 1px),linear-gradient(90deg,rgba(212,175,55,.03) 1px,transparent 1px);background-size:50px 50px,50px 50px,80px 80px,80px 80px;animation:gridWave 20s ease-in-out infinite;z-index:-1;pointer-events:none}
</style>
<link rel="stylesheet" href="/_astro/index.DHDHwiTa.css">
<link rel="stylesheet" href="/_astro/index.BCrk01Mj.css"></head> <body data-astro-cid-37fxchfa> <nav class="navbar" data-astro-cid-pux6a34n> <div class="navbar-brand" data-astro-cid-pux6a34n>Xiaoyou Wu</div> <a href="/" class="navbar-link " data-astro-cid-pux6a34n>
Home
</a> <a href="/blog" class="navbar-link active" data-astro-cid-pux6a34n>
Blog
</a> <a href="/projects" class="navbar-link " data-astro-cid-pux6a34n>
Projects
</a> <a href="/about" class="navbar-link " data-astro-cid-pux6a34n>
About
</a> </nav>  <script type="module">function r(){const e=document.querySelector(".navbar");window.scrollY>50?e?.classList.add("scrolled"):e?.classList.remove("scrolled")}window.addEventListener("scroll",r);document.querySelector(".navbar")?.addEventListener("contextmenu",e=>{e.preventDefault()});document.querySelector(".navbar")?.addEventListener("dragstart",e=>{e.preventDefault()});</script> <main class="main-content" data-astro-cid-37fxchfa>  <main class="main-content" data-astro-cid-dmqsi53g>  <div class="blog-header" data-astro-cid-5tznm7mj> <div class="container" data-astro-cid-5tznm7mj> <h1 data-astro-cid-5tznm7mj>Blog</h1> <p data-astro-cid-5tznm7mj>My thoughts on technology, engineering, and life</p> </div> </div> <section class="blog-content" data-astro-cid-5tznm7mj> <div class="container" data-astro-cid-5tznm7mj> <div class="blog-sidebar" data-astro-cid-5tznm7mj> <div class="sidebar-section" data-astro-cid-5tznm7mj> <h3 data-astro-cid-5tznm7mj>Search</h3> <div class="search-container" data-astro-cid-mjrxwznw> <div class="search-input-wrapper" data-astro-cid-mjrxwznw> <input type="text" id="search-input" placeholder="Search blogs & projects..." autocomplete="off" data-astro-cid-mjrxwznw> <button id="search-icon" class="search-icon" type="button" aria-label="Search" data-astro-cid-mjrxwznw> <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" data-astro-cid-mjrxwznw> <circle cx="11" cy="11" r="8" data-astro-cid-mjrxwznw></circle> <path d="m21 21-4.35-4.35" data-astro-cid-mjrxwznw></path> </svg> </button> </div> <div id="search-results" class="search-results hidden" data-astro-cid-mjrxwznw> <div class="results-header" data-astro-cid-mjrxwznw> <span id="results-count" data-astro-cid-mjrxwznw>0 results</span> <button id="clear-search" class="clear-btn" data-astro-cid-mjrxwznw>×</button> </div> <div id="results-list" class="results-list" data-astro-cid-mjrxwznw></div> </div> </div>  <!-- GLOBAL STYLES TO OVERRIDE ASTRO SCOPING -->  <script type="module">const allBlogPosts = [{"id":"matrix-multiplication/Matrix_multiplication.md","data":{"title":"Intro to Triton with Matrix Multiplication","description":"Introduction to GPU programming with Triton and build the matrix multiplication along the way","pubDate":"2025-03-18T00:00:00.000Z","author":"Xiaoyou Wu","tags":["triton","GPU","CUDA","compiler","code","optimization"],"image":"./images/grouped_vs_row_major_ordering.png"},"body":"As the most important operation in GPU computation, matrix multiplication optimization is a must learn!\r\n\r\n**Bruh, let's dive right in ~**\r\n\r\n## From the architecture to the idea\r\n\r\n### Memory Architecture of GPU\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph \"CPU / Host\"\r\n        A[Host Code] --> B{Kernel Launch}\r\n    end\r\n\r\n    subgraph \"GPU / Device\"\r\n        C[Command Queue]\r\n        D(Thread Blocks)\r\n        E[Streaming Multiprocessors]\r\n        F[L1 Cache / Shared Memory]\r\n        G[CUDA Cores]\r\n        H[L2 Cache]\r\n        I[High-Bandwidth Memory]\r\n    end\r\n\r\n    %% --- Define Connections ---\r\n    B -- Command --> C\r\n    C --> D\r\n    D --> E\r\n    E -- Contains --> F\r\n    E -- Contains --> G\r\n\r\n    %% Memory Flow\r\n    G <--> F\r\n    F <--> H\r\n    H <--> I\r\n```\r\n\r\n1. Command from CPU: CPU will run a pointer to the compiled GPU function. _But how is it compiled, and what needs to be included? This will be discussed in the compilation part._\r\n2. Then, the command will undergo an asynchronous operation using a physical memory buffer FIFO. Compile instructions will be loaded.\r\n3. Here, the grid of blocks is simply a soft level abstraction. The information about the grid blocks will be compiled for each thread block, which is in the SMs (Streaming Multiprocessor). **_<u>This is incredible, frontend and backend separation techniques, I guess it helps the hardware security.</u>_**\r\n4. **<u>VRAM</u>** is basically a more general external memory, and **<u>HBM</u>** is just a high-performance type.\r\n5. Then, for the SM, it will manage warp, which manages 32 threads for executing and transporting information to other parts:\r\n\r\n```mermaid\r\ngraph TD\r\n    subgraph GPU Device\r\n        subgraph Streaming Multiprocessor 1\r\n            subgraph CUDA Core 1A\r\n                R1[Registers]\r\n            end\r\n            subgraph CUDA Core 1B\r\n                R2[Registers]\r\n            end\r\n            L1_1[L1 Cache / Shared Memory]\r\n        end\r\n\r\n        subgraph Streaming Multiprocessor 2\r\n            subgraph CUDA Core 2A\r\n                R3[Registers]\r\n            end\r\n            subgraph CUDA Core 2B\r\n                R4[Registers]\r\n            end\r\n            L1_2[L1 Cache / Shared Memory]\r\n        end\r\n\r\n        L2[L2 Cache]\r\n        HBM[High-Bandwidth Memory]\r\n    end\r\n\r\n    %% --- Data Access Paths ---\r\n    R1 <--> L1_1\r\n    R2 <--> L1_1\r\n    R3 <--> L1_2\r\n    R4 <--> L1_2\r\n    L1_1 <--> L2\r\n    L1_2 <--> L2\r\n    L2 <--> HBM\r\n\r\n    style R1 fill:#f9f,stroke:#333,stroke-width:2px\r\n    style R2 fill:#f9f,stroke:#333,stroke-width:2px\r\n    style R3 fill:#f9f,stroke:#333,stroke-width:2px\r\n    style R4 fill:#f9f,stroke:#333,stroke-width:2px\r\n    style L1_1 fill:#bbf,stroke:#333,stroke-width:2px\r\n    style L1_2 fill:#bbf,stroke:#333,stroke-width:2px\r\n    style L2 fill:#bdf,stroke:#333,stroke-width:2px\r\n    style HBM fill:#fb9,stroke:#333,stroke-width:2px\r\n```\r\n\r\nAND there are their properties:\r\n\r\n| Memory Hardware                        | Key Parameters                                                                                          | Primary Job                                                                                                        |\r\n| -------------------------------------- | ------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |\r\n| **Registers**                          | **Size:** Tiniest (bytes per thread)<br />**Speed:** Instantaneous **Scope:** Private to **one thread** | Holds the immediate working data for a single thread's current instruction. The absolute fastest memory available. |\r\n| **L1 Cache / Shared Memory**           | **Size:** Small (~100 KB per SM) <br />**Speed:** Fastest <br />**Scope:** Per SM                       | A high-speed scratchpad for threads within a **block** to share data and cooperate.                                |\r\n| **L2 Cache**                           | **Size:** Medium (a few MB) <br />**Speed:** Fast<br />**Scope:** Shared by **all SMs**                 | A unified cache for the entire GPU, catching data requests that miss L1 to avoid slow HBM access.                  |\r\n| **High-Bandwidth Memory (HBM) / VRAM** | **Size:** Large (many GB) **Speed:** Slowest **Scope:** Global (entire GPU)                             | The main data storage for the GPU, holding all large datasets, textures, and models.                               |\r\n\r\n### Compilation architecture\r\n\r\n```mermaid\r\ngraph TD\r\n    %% --- Node Definitions ---\r\n\r\n    subgraph \"Stage 1: Compilation\"\r\n        A[\"📄 Source File .cu\\nContains both Host CPU and Device GPU code\"]\r\n        B[\"1. NVCC Compiler Driver\"]\r\n    end\r\n\r\n    subgraph \"Host CPU Path\"\r\n        C[\"Host Code C++\"]\r\n        D[\"Host Compiler\\ne.g., GCC, Clang, MSVC\"]\r\n        E[\"Host Object File .o\"]\r\n    end\r\n\r\n    subgraph \"Device GPU Path\"\r\n        F[\"Device Code CUDA C++\"]\r\n        G[\"CUDA Frontend Compiler\"]\r\n        H[\"PTX\\nParallel Thread Execution\\n<I>Intermediate Assembly</I>\"]\r\n        I[\"PTX Assembler\"]\r\n        J[\"SASS\\nStreaming Assembler\\n<B>GPU Machine Code</B>\"]\r\n        K[\"Device Object File .o\"]\r\n    end\r\n\r\n    subgraph \"Stage 2: Linking\"\r\n        L[\"Linker\"]\r\n        M[\"✅ Final Executable\\nContains Host code + embedded GPU code\"]\r\n    end\r\n\r\n    subgraph \"Stage 3: Execution\"\r\n        N[\"Program runs on Host CPU\"]\r\n        O[\"CUDA Driver\"]\r\n        P{\"<B>JIT Compilation</B>\\nJust-In-Time\"}\r\n        Q[\"🚀 GPU Executes Code\"]\r\n    end\r\n\r\n    %% --- Connection Definitions ---\r\n    A --> B\r\n    B -- Separates Code --> C & F\r\n    C --> D --> E\r\n    F --> G --> H --> I --> J --> K\r\n    E --> L\r\n    K --> L\r\n    L --> M\r\n    M --> N --> O --> P\r\n    P -- \"If embedded SASS doesn't match current GPU\" --> I\r\n    P -- \"If SASS is compatible\" --> Q\r\n\r\n    %% --- Styling ---\r\n    style B fill:#f9f,stroke:#333,stroke-width:2px\r\n    style L fill:#f9f,stroke:#333,stroke-width:2px\r\n    style N fill:#f9f,stroke:#333,stroke-width:2px\r\n    style O fill:#f9f,stroke:#333,stroke-width:2px\r\n```\r\n\r\n1. Command from CPU: The command will be compiled in the CPU. The kernel function should include the behavior of the Grid of Blocks and the number of threads per block.\r\n2. **<u>JIT (just in time)</u>** means it compiles the program, not before, but during the GPU's execution.\r\n3. **<u>Bank conflict:</u>** since multiple threads can potentially want to access the same shared memory bank (a column) simultaneously, we would have to serialize them / or arrange the data properly\r\n4. **<u>PTX</u>** provides forward compatibility, and **<u>SASS</u>** provides the maximum performance.\r\n5. **<u>Memory coalescing</u>**: When a warp accesses the GPU's main VRAM, the hardware checks the addresses they are requesting. If these addresses are close together and fall within a single, aligned memory segment, the GPU \"coalesces\" them. Instead of performing 32 small, separate memory fetches, it performs one single, large fetch that grabs all the requested data at once. Shared Memory/registers are fast enough, and L1/L2 also have this feature called locality.\r\n\r\n## Torch Implementation of Matrix Multiplication\r\n\r\n```python\r\ndef matrix_multilication(x,y):\r\n  # consider matrix x and y with dimention M,N,K\r\n  M,N = x.shape\r\n  N,K = y.shape\r\n  accumulator = torch.zeros(M,K)\r\n  for row_x in range(M):\r\n    for col_y in range(K):\r\n      dot_product = torch.dot(x[row_X,:],y[:,col_y])\r\n      accumulator[row_x][col_y] = dot_product\r\n\treturn accumulator\r\n# Basically just use\r\nresult = torch.matmul(x,y)\r\n```\r\n\r\n## High-Level Implementation of Matrix Multiplication\r\n\r\n```python\r\n#Implement the tiling in the Triton\r\ndef matrix_multiplication(x,y,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K):\r\n  M,N = x.shape\r\n  N,K = y.shape\r\n  z = torch.zeros(M,K)\r\n  for m in range(0,M,BLOCK_SIZE_M):\r\n  \tfor k in range(0,K,BLOCK_SIZE_K):\r\n      accumulator = zeros((BLOCK_SIZE_M,BLOCK_SIZE_K),dtype=float32)\r\n      #in the SM So we have to export the ACCU to the L2 first\r\n      for i in range(0,n,BLOCK_SIZE_N):\r\n        a = x[m:m+BLOCK_SIZE_M,n:n+BLOCK_SIZE_N]\r\n        b = y[n:n+BLOCK_SIZE_N,k:k+BLOCK_SIZE_K]\r\n        accumulator += dot(a,b)\r\n      z[m:m+BLOCK_SIZE,k:k+BLOCK_SIZE_K] = accumulator\r\n  return z\r\n```\r\n\r\nHere are some useful insights:\r\n\r\n- Why do we use the accumulator? Why can't we just add to the output matrix? A: Because of the coalescing the access to the information is faster\r\n- Why do we do a small matrix multiplication instead of a faster dot product? A: They are essentially the same because they are just 1D arrays in the memory space, and the difference between these two algorithms won't make much of a difference.\r\n\r\n## Triton Kernel for Matrix Multiplication\r\n\r\n```python\r\n@triton.jit\r\ndef kernel_maxmul(\r\n        # Pointers to matrices\r\n        a_ptr, b_ptr, c_ptr,\r\n        # Matrix dimensions\r\n        M, N, K,\r\n        # The stride variables represent how much to increase the ptr by when moving by 1\r\n        stride_am, stride_ak,\r\n        stride_bk, stride_bn,\r\n        stride_cm, stride_cn,\r\n        # Meta-parameters\r\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  # for tiling\r\n        GROUP_SIZE_M: tl.constexpr,  # for super band\r\n        ACTIVATION: tl.constexpr  )\r\n\r\n    # This is done in a grouped ordering to promote L2 data reuse.(exactly why we use the GROUP_SIZE_M)\r\n    pid = tl.program_id(axis=0)\r\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\r\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\r\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\r\n    group_id = pid // num_pid_in_group\r\n    first_pid_m = group_id * GROUP_SIZE_M\r\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\r\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\r\n    pid_n = (pid % num_pid_in_group) // group_size_m\r\n\r\n    # ------------------------------------------------------\r\n    # This helps to guide integer analysis in the backend to optimize load/store offset address calculation\r\n    tl.assume(pid_m >= 0)\r\n    tl.assume(pid_n >= 0)\r\n    tl.assume(stride_am > 0)\r\n    tl.assume(stride_ak > 0)\r\n    tl.assume(stride_bn > 0)\r\n    tl.assume(stride_bk > 0)\r\n    tl.assume(stride_cm > 0)\r\n    tl.assume(stride_cn > 0)\r\n\r\n    # ----------------------------------------------------------\r\n    # Create pointers for the first blocks of A and B.\r\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\r\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\r\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\r\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\r\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\r\n\r\n    # -----------------------------------------------------------\r\n    # Iterate to compute a block of the C matrix.\r\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\r\n    # of fp32 values for higher accuracy.\r\n    # `accumulator` will be converted back to fp16 after the loop.\r\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\r\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\r\n        # Load the next block of A and B, generate a mask by checking the K dimension.\r\n        # If it is out of bounds, set it to 0.\r\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\r\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\r\n        # We accumulate along the K dimension.\r\n        accumulator = tl.dot(a, b, accumulator)\r\n        # Advance the ptrs to the next K block.\r\n        a_ptrs += BLOCK_SIZE_K * stride_ak\r\n        b_ptrs += BLOCK_SIZE_K * stride_bk\r\n    # You can fuse arbitrary activation functions here\r\n    # while the accumulator is still in FP32!\r\n    if ACTIVATION == \"leaky_relu\":\r\n        accumulator = leaky_relu(accumulator)\r\n    c = accumulator.to(tl.float16)\r\n\r\n    # -----------------------------------------------------------\r\n    # Write back the block of the output matrix C with masks.\r\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\r\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\r\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\r\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\r\n    tl.store(c_ptrs, c, mask=c_mask)\r\n\r\n```\r\n\r\n- We used a novel parameter: GROUP_SIZE_M, which was meant to be the height of a super-block with several BLOCK_SIZE_M tall. The following picture shows this idea better. Let me use the quote from the original tutorial <u>_\"In the following matmul where each matrix is 9 blocks by 9 blocks, we can see that if we compute the output in row-major ordering, we need to load 90 blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped ordering, we only need to load 54 blocks.\"_</u>\r\n\r\n## ![grouped_vs_row_major_ordering](./images/grouped_vs_row_major_ordering.png)\r\n\r\n## Conclusion\r\n\r\nGPU performance is dictated by its memory hierarchy: extremely fast but small on-chip memory (Registers, L1 Cache/Shared Memory) and large but slow off-chip memory (VRAM/HBM). The primary optimization goal is to minimize traffic to slow VRAM by maximizing data reuse in the fast caches.\r\n\r\nThe key software strategy is **tiling** (or blocking), where large matrices are broken into smaller blocks that fit into fast on-chip memory. Computations are performed on these blocks using a local **accumulator** to sum results, ensuring only one final, slow write to VRAM per block. This algorithm must also account for hardware features, ensuring memory access is **coalesced** to maximize VRAM bandwidth and that it avoids **bank conflicts** in shared memory.\r\n\r\nBlock Tiling also works ! And it fosters the calculation speed.","filePath":"src/content/blogs/matrix-multiplication/Matrix_multiplication.md","assetImports":["./images/grouped_vs_row_major_ordering.png"],"digest":"dbc98fc1884d0340","rendered":{"html":"<p>As the most important operation in GPU computation, matrix multiplication optimization is a must learn!</p>\n<p><strong>Bruh, let’s dive right in ~</strong></p>\n<h2 id=\"from-the-architecture-to-the-idea\">From the architecture to the idea</h2>\n<h3 id=\"memory-architecture-of-gpu\">Memory Architecture of GPU</h3>\n<figure class=\"mermaid-diagram\" id=\"mermaid-7g5tu3wwp\">\n  <div class=\"mermaid-container\">\n    Loading diagram...\n  </div>\n  <details class=\"mermaid-source\">\n    <summary>Source</summary>\n    <pre class=\"mermaid-code\"><code>graph TD\n    subgraph \"CPU / Host\"\n        A[Host Code] --> B{Kernel Launch}\n    end\n\n    subgraph \"GPU / Device\"\n        C[Command Queue]\n        D(Thread Blocks)\n        E[Streaming Multiprocessors]\n        F[L1 Cache / Shared Memory]\n        G[CUDA Cores]\n        H[L2 Cache]\n        I[High-Bandwidth Memory]\n    end\n\n    %% --- Define Connections ---\n    B -- Command --> C\n    C --> D\n    D --> E\n    E -- Contains --> F\n    E -- Contains --> G\n\n    %% Memory Flow\n    G &#x3C;--> F\n    F &#x3C;--> H\n    H &#x3C;--> I</code></pre>\n  </details>\n</figure>\n<ol>\n<li>Command from CPU: CPU will run a pointer to the compiled GPU function. <em>But how is it compiled, and what needs to be included? This will be discussed in the compilation part.</em></li>\n<li>Then, the command will undergo an asynchronous operation using a physical memory buffer FIFO. Compile instructions will be loaded.</li>\n<li>Here, the grid of blocks is simply a soft level abstraction. The information about the grid blocks will be compiled for each thread block, which is in the SMs (Streaming Multiprocessor). <strong><em><u>This is incredible, frontend and backend separation techniques, I guess it helps the hardware security.</u></em></strong></li>\n<li><strong><u>VRAM</u></strong> is basically a more general external memory, and <strong><u>HBM</u></strong> is just a high-performance type.</li>\n<li>Then, for the SM, it will manage warp, which manages 32 threads for executing and transporting information to other parts:</li>\n</ol>\n<figure class=\"mermaid-diagram\" id=\"mermaid-3cwme6cbl\">\n  <div class=\"mermaid-container\">\n    Loading diagram...\n  </div>\n  <details class=\"mermaid-source\">\n    <summary>Source</summary>\n    <pre class=\"mermaid-code\"><code>graph TD\n    subgraph GPU Device\n        subgraph Streaming Multiprocessor 1\n            subgraph CUDA Core 1A\n                R1[Registers]\n            end\n            subgraph CUDA Core 1B\n                R2[Registers]\n            end\n            L1_1[L1 Cache / Shared Memory]\n        end\n\n        subgraph Streaming Multiprocessor 2\n            subgraph CUDA Core 2A\n                R3[Registers]\n            end\n            subgraph CUDA Core 2B\n                R4[Registers]\n            end\n            L1_2[L1 Cache / Shared Memory]\n        end\n\n        L2[L2 Cache]\n        HBM[High-Bandwidth Memory]\n    end\n\n    %% --- Data Access Paths ---\n    R1 &#x3C;--> L1_1\n    R2 &#x3C;--> L1_1\n    R3 &#x3C;--> L1_2\n    R4 &#x3C;--> L1_2\n    L1_1 &#x3C;--> L2\n    L1_2 &#x3C;--> L2\n    L2 &#x3C;--> HBM\n\n    style R1 fill:#f9f,stroke:#333,stroke-width:2px\n    style R2 fill:#f9f,stroke:#333,stroke-width:2px\n    style R3 fill:#f9f,stroke:#333,stroke-width:2px\n    style R4 fill:#f9f,stroke:#333,stroke-width:2px\n    style L1_1 fill:#bbf,stroke:#333,stroke-width:2px\n    style L1_2 fill:#bbf,stroke:#333,stroke-width:2px\n    style L2 fill:#bdf,stroke:#333,stroke-width:2px\n    style HBM fill:#fb9,stroke:#333,stroke-width:2px</code></pre>\n  </details>\n</figure>\n<p>AND there are their properties:</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Memory Hardware</th><th>Key Parameters</th><th>Primary Job</th></tr></thead><tbody><tr><td><strong>Registers</strong></td><td><strong>Size:</strong> Tiniest (bytes per thread)<br><strong>Speed:</strong> Instantaneous <strong>Scope:</strong> Private to <strong>one thread</strong></td><td>Holds the immediate working data for a single thread’s current instruction. The absolute fastest memory available.</td></tr><tr><td><strong>L1 Cache / Shared Memory</strong></td><td><strong>Size:</strong> Small (~100 KB per SM) <br><strong>Speed:</strong> Fastest <br><strong>Scope:</strong> Per SM</td><td>A high-speed scratchpad for threads within a <strong>block</strong> to share data and cooperate.</td></tr><tr><td><strong>L2 Cache</strong></td><td><strong>Size:</strong> Medium (a few MB) <br><strong>Speed:</strong> Fast<br><strong>Scope:</strong> Shared by <strong>all SMs</strong></td><td>A unified cache for the entire GPU, catching data requests that miss L1 to avoid slow HBM access.</td></tr><tr><td><strong>High-Bandwidth Memory (HBM) / VRAM</strong></td><td><strong>Size:</strong> Large (many GB) <strong>Speed:</strong> Slowest <strong>Scope:</strong> Global (entire GPU)</td><td>The main data storage for the GPU, holding all large datasets, textures, and models.</td></tr></tbody></table>\n<h3 id=\"compilation-architecture\">Compilation architecture</h3>\n<figure class=\"mermaid-diagram\" id=\"mermaid-8bl34qyc9\">\n  <div class=\"mermaid-container\">\n    Loading diagram...\n  </div>\n  <details class=\"mermaid-source\">\n    <summary>Source</summary>\n    <pre class=\"mermaid-code\"><code>graph TD\n    %% --- Node Definitions ---\n\n    subgraph \"Stage 1: Compilation\"\n        A[\"📄 Source File .cu\\nContains both Host CPU and Device GPU code\"]\n        B[\"1. NVCC Compiler Driver\"]\n    end\n\n    subgraph \"Host CPU Path\"\n        C[\"Host Code C++\"]\n        D[\"Host Compiler\\ne.g., GCC, Clang, MSVC\"]\n        E[\"Host Object File .o\"]\n    end\n\n    subgraph \"Device GPU Path\"\n        F[\"Device Code CUDA C++\"]\n        G[\"CUDA Frontend Compiler\"]\n        H[\"PTX\\nParallel Thread Execution\\n&#x3C;I>Intermediate Assembly&#x3C;/I>\"]\n        I[\"PTX Assembler\"]\n        J[\"SASS\\nStreaming Assembler\\n&#x3C;B>GPU Machine Code&#x3C;/B>\"]\n        K[\"Device Object File .o\"]\n    end\n\n    subgraph \"Stage 2: Linking\"\n        L[\"Linker\"]\n        M[\"✅ Final Executable\\nContains Host code + embedded GPU code\"]\n    end\n\n    subgraph \"Stage 3: Execution\"\n        N[\"Program runs on Host CPU\"]\n        O[\"CUDA Driver\"]\n        P{\"&#x3C;B>JIT Compilation&#x3C;/B>\\nJust-In-Time\"}\n        Q[\"🚀 GPU Executes Code\"]\n    end\n\n    %% --- Connection Definitions ---\n    A --> B\n    B -- Separates Code --> C &#x26; F\n    C --> D --> E\n    F --> G --> H --> I --> J --> K\n    E --> L\n    K --> L\n    L --> M\n    M --> N --> O --> P\n    P -- \"If embedded SASS doesn't match current GPU\" --> I\n    P -- \"If SASS is compatible\" --> Q\n\n    %% --- Styling ---\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style L fill:#f9f,stroke:#333,stroke-width:2px\n    style N fill:#f9f,stroke:#333,stroke-width:2px\n    style O fill:#f9f,stroke:#333,stroke-width:2px</code></pre>\n  </details>\n</figure>\n<ol>\n<li>Command from CPU: The command will be compiled in the CPU. The kernel function should include the behavior of the Grid of Blocks and the number of threads per block.</li>\n<li><strong><u>JIT (just in time)</u></strong> means it compiles the program, not before, but during the GPU’s execution.</li>\n<li><strong><u>Bank conflict:</u></strong> since multiple threads can potentially want to access the same shared memory bank (a column) simultaneously, we would have to serialize them / or arrange the data properly</li>\n<li><strong><u>PTX</u></strong> provides forward compatibility, and <strong><u>SASS</u></strong> provides the maximum performance.</li>\n<li><strong><u>Memory coalescing</u></strong>: When a warp accesses the GPU’s main VRAM, the hardware checks the addresses they are requesting. If these addresses are close together and fall within a single, aligned memory segment, the GPU “coalesces” them. Instead of performing 32 small, separate memory fetches, it performs one single, large fetch that grabs all the requested data at once. Shared Memory/registers are fast enough, and L1/L2 also have this feature called locality.</li>\n</ol>\n<h2 id=\"torch-implementation-of-matrix-multiplication\">Torch Implementation of Matrix Multiplication</h2>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> matrix_multilication</span><span style=\"color:#E1E4E8\">(x,y):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">  # consider matrix x and y with dimention M,N,K</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  M,N </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  N,K </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.zeros(M,K)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  for</span><span style=\"color:#E1E4E8\"> row_x </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(M):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> col_y </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(K):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      dot_product </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.dot(x[row_X,:],y[:,col_y])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      accumulator[row_x][col_y] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dot_product</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> accumulator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Basically just use</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.matmul(x,y)</span></span></code></pre>\n<h2 id=\"high-level-implementation-of-matrix-multiplication\">High-Level Implementation of Matrix Multiplication</h2>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#6A737D\">#Implement the tiling in the Triton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> matrix_multiplication</span><span style=\"color:#E1E4E8\">(x,y,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  M,N </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  N,K </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  z </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.zeros(M,K)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,M,</span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  \tfor</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,K,</span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> zeros((</span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">),</span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">float32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">      #in the SM So we have to export the ACCU to the L2 first</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,n,</span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x[m:m</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">,n:n</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> y[n:n</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">,k:k</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        accumulator </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> dot(a,b)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      z[m:m</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE</span><span style=\"color:#E1E4E8\">,k:k</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> accumulator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  return</span><span style=\"color:#E1E4E8\"> z</span></span></code></pre>\n<p>Here are some useful insights:</p>\n<ul>\n<li>Why do we use the accumulator? Why can’t we just add to the output matrix? A: Because of the coalescing the access to the information is faster</li>\n<li>Why do we do a small matrix multiplication instead of a faster dot product? A: They are essentially the same because they are just 1D arrays in the memory space, and the difference between these two algorithms won’t make much of a difference.</li>\n</ul>\n<h2 id=\"triton-kernel-for-matrix-multiplication\">Triton Kernel for Matrix Multiplication</h2>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#B392F0\">@triton.jit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> kernel_maxmul</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Pointers to matrices</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a_ptr, b_ptr, c_ptr,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Matrix dimensions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        M, N, K,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # The stride variables represent how much to increase the ptr by when moving by 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stride_am, stride_ak,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stride_bk, stride_bn,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stride_cm, stride_cn,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Meta-parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  </span><span style=\"color:#6A737D\"># for tiling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        GROUP_SIZE_M: tl.constexpr,  </span><span style=\"color:#6A737D\"># for super band</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ACTIVATION: tl.constexpr  )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This is done in a grouped ordering to promote L2 data reuse.(exactly why we use the GROUP_SIZE_M)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pid </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.program_id(</span><span style=\"color:#FFAB70\">axis</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_pid_m </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.cdiv(M, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_pid_n </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.cdiv(N, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_pid_in_group </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> GROUP_SIZE_M</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> num_pid_n</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    group_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pid </span><span style=\"color:#F97583\">//</span><span style=\"color:#E1E4E8\"> num_pid_in_group</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    first_pid_m </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> group_id </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> GROUP_SIZE_M</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    group_size_m </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(num_pid_m </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> first_pid_m, </span><span style=\"color:#79B8FF\">GROUP_SIZE_M</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pid_m </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> first_pid_m </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> ((pid </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> num_pid_in_group) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> group_size_m)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pid_n </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (pid </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> num_pid_in_group) </span><span style=\"color:#F97583\">//</span><span style=\"color:#E1E4E8\"> group_size_m</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ------------------------------------------------------</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This helps to guide integer analysis in the backend to optimize load/store offset address calculation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(pid_m </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(pid_n </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_am </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_ak </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_bn </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_bk </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_cm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.assume(stride_cn </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ----------------------------------------------------------</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create pointers for the first blocks of A and B.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_am </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (pid_m </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_M</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> M</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_bn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (pid_n </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_N</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> N</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a_ptrs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> a_ptr </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (offs_am[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stride_am </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> offs_k[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stride_ak)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b_ptrs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> b_ptr </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (offs_k[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stride_bk </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> offs_bn[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> stride_bn)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # -----------------------------------------------------------</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Iterate to compute a block of the C matrix.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # of fp32 values for higher accuracy.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # `accumulator` will be converted back to fp16 after the loop.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.zeros((</span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tl.float32)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, tl.cdiv(K, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">)):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Load the next block of A and B, generate a mask by checking the K dimension.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If it is out of bounds, set it to 0.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.load(a_ptrs, </span><span style=\"color:#FFAB70\">mask</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">offs_k[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> K </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">other</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.load(b_ptrs, </span><span style=\"color:#FFAB70\">mask</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">offs_k[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> K </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_K</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">other</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # We accumulate along the K dimension.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tl.dot(a, b, accumulator)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Advance the ptrs to the next K block.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        a_ptrs </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_K</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> stride_ak</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        b_ptrs </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_K</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> stride_bk</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # You can fuse arbitrary activation functions here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # while the accumulator is still in FP32!</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> ACTIVATION</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"leaky_relu\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        accumulator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> leaky_relu(accumulator)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> accumulator.to(tl.float16)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # -----------------------------------------------------------</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Write back the block of the output matrix C with masks.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_cm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pid_m </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_M</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_M</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offs_cn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pid_n </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> BLOCK_SIZE_N</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tl.arange(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">BLOCK_SIZE_N</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c_ptrs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> c_ptr </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> stride_cm </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> offs_cm[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> stride_cn </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> offs_cn[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (offs_cm[:, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> M) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> (offs_cn[</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, :] </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> N)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tl.store(c_ptrs, c, </span><span style=\"color:#FFAB70\">mask</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">c_mask)</span></span>\n<span class=\"line\"></span></code></pre>\n<ul>\n<li>We used a novel parameter: GROUP_SIZE_M, which was meant to be the height of a super-block with several BLOCK_SIZE_M tall. The following picture shows this idea better. Let me use the quote from the original tutorial <u><em>“In the following matmul where each matrix is 9 blocks by 9 blocks, we can see that if we compute the output in row-major ordering, we need to load 90 blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped ordering, we only need to load 54 blocks.”</em></u></li>\n</ul>\n<h2 id=\"\"><img __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;./images/grouped_vs_row_major_ordering.png&#x22;,&#x22;alt&#x22;:&#x22;grouped_vs_row_major_ordering&#x22;,&#x22;index&#x22;:0}\"></h2>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>GPU performance is dictated by its memory hierarchy: extremely fast but small on-chip memory (Registers, L1 Cache/Shared Memory) and large but slow off-chip memory (VRAM/HBM). The primary optimization goal is to minimize traffic to slow VRAM by maximizing data reuse in the fast caches.</p>\n<p>The key software strategy is <strong>tiling</strong> (or blocking), where large matrices are broken into smaller blocks that fit into fast on-chip memory. Computations are performed on these blocks using a local <strong>accumulator</strong> to sum results, ensuring only one final, slow write to VRAM per block. This algorithm must also account for hardware features, ensuring memory access is <strong>coalesced</strong> to maximize VRAM bandwidth and that it avoids <strong>bank conflicts</strong> in shared memory.</p>\n<p>Block Tiling also works ! And it fosters the calculation speed.</p>","metadata":{"headings":[{"depth":2,"slug":"from-the-architecture-to-the-idea","text":"From the architecture to the idea"},{"depth":3,"slug":"memory-architecture-of-gpu","text":"Memory Architecture of GPU"},{"depth":3,"slug":"compilation-architecture","text":"Compilation architecture"},{"depth":2,"slug":"torch-implementation-of-matrix-multiplication","text":"Torch Implementation of Matrix Multiplication"},{"depth":2,"slug":"high-level-implementation-of-matrix-multiplication","text":"High-Level Implementation of Matrix Multiplication"},{"depth":2,"slug":"triton-kernel-for-matrix-multiplication","text":"Triton Kernel for Matrix Multiplication"},{"depth":2,"slug":"","text":""},{"depth":2,"slug":"conclusion","text":"Conclusion"}],"localImagePaths":["./images/grouped_vs_row_major_ordering.png"],"remoteImagePaths":[],"frontmatter":{"title":"Intro to Triton with Matrix Multiplication","description":"Introduction to GPU programming with Triton and build the matrix multiplication along the way","pubDate":"2025-03-18T00:00:00.000Z","author":"Xiaoyou Wu","tags":["triton","GPU","CUDA","compiler","code","optimization"],"image":"./images/grouped_vs_row_major_ordering.png"},"imagePaths":["./images/grouped_vs_row_major_ordering.png"]}},"collection":"blogs","slug":"matrix-multiplication/matrix_multiplication"}];
const allProjects = [{"id":"ai-translation/AI_translation.md","data":{"title":"AI Translation Startup (Seek Hub)","description":"A smart, AI-powered platform for perfect PDF-to-Word and multi-format document translation, preserving layout and formatting.","pubDate":"2025-07-31T00:00:00.000Z","tags":["AI","Translation","PDF","DOCX","Next.js","Python","Startup"],"githubLink":"https://laurence-wu.github.io/projects/"},"body":"## My feelings to this project\r\n\r\nThis is like buiding a child of my own. Every feature properly tested to integrated, and all those annoying configuration on the google cloud. This is my first serious startup project.\r\n\r\n## Introduction\r\n\r\nWe all know how frustrating translating documents can be, especially complex PDFs. You lose your formatting, the context gets mixed up, and you spend hours fixing everything manually. Seek Hub is our solution: a smart, AI-powered platform designed to make the entire process effortless and deliver a perfect translation every time.\r\n\r\n## Why We're Building This and What It Does\r\n\r\nWe started this project because we saw a clear need for a simpler, more reliable translation tool. In a world of powerful AI agents, the process of translating a document should be easy. Our goal is to handle all the complex and tedious work for you, so you can get a high-quality translation without the headache.\r\n\r\nHere’s how simple we’ve made it:\r\n\r\n1. You start by uploading your PDF document.\r\n2. Our AI agent immediately gets to work, automatically translating the entire file. The most important part is that it **perfectly preserves the original layout and formatting**—no more broken tables or misplaced images.\r\n3. In just a few moments, you’re presented with a ready-to-use, fully translated document.\r\n\r\nThe AI is smart enough to understand the document's context, but if you want to make a quick change, you can easily review the translation and accept smart suggestions for alternative wording. The goal is that you receive a document that is 99% of the way there, or completely finished, without you having to do any of the heavy lifting.\r\n\r\n## The Technical Details\r\n\r\nThe convenience you experience on the frontend is made possible by some serious engineering on the backend.\r\n\r\n- **Architecture:** We use a modern stack with a **Next.js** frontend and a **Python** backend. Our database is a NoSQL solution, currently **Firebase**, for smooth and reliable file uploads.\r\n- **The Core Engine:** Our platform's real power comes from our custom-built backend pipeline.\r\n  1. **Format Preservation Engine:** When you upload a PDF, our proprietary **PDF-to-Docs engine** is the first thing it touches. Its single most important job is to map out your document's structure. This allows us to reassemble the translated document perfectly, so you don't waste any time rebuilding tables, realigning images, or fixing layouts.\r\n  2. **High-Speed Translation:** To deliver your translation quickly, our backend uses a sophisticated **asynchronous pipeline**. This system breaks down large documents and processes them in parallel using a **connection pool** and a **thread pool**. All this complexity is handled completely behind the scenes. For you, it just means you get your translated file back incredibly fast, even if it's a very large document.\r\n  3. **Automatic Reassembly:** Once the translation is complete, our system automatically rebuilds the PDF, delivering a final product that looks just like your original.\r\n\r\n## Project Summary\r\n\r\nTo put it simply, **Seek Hub is designed to make high-quality document translation effortless.** It removes the biggest headaches from the process: losing your formatting and doing tedious manual work. Our platform's core promise is to deliver a perfectly formatted, accurately translated PDF back to you in a fraction of the time it would normally take. By leveraging a powerful AI agent and a smart backend pipeline, we've created a tool that just works, letting you focus on your content, not the complex process of translation.","filePath":"src/content/projects/ai-translation/AI_translation.md","digest":"41022e05c2de1e79","rendered":{"html":"<h2 id=\"my-feelings-to-this-project\">My feelings to this project</h2>\n<p>This is like buiding a child of my own. Every feature properly tested to integrated, and all those annoying configuration on the google cloud. This is my first serious startup project.</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>We all know how frustrating translating documents can be, especially complex PDFs. You lose your formatting, the context gets mixed up, and you spend hours fixing everything manually. Seek Hub is our solution: a smart, AI-powered platform designed to make the entire process effortless and deliver a perfect translation every time.</p>\n<h2 id=\"why-were-building-this-and-what-it-does\">Why We’re Building This and What It Does</h2>\n<p>We started this project because we saw a clear need for a simpler, more reliable translation tool. In a world of powerful AI agents, the process of translating a document should be easy. Our goal is to handle all the complex and tedious work for you, so you can get a high-quality translation without the headache.</p>\n<p>Here’s how simple we’ve made it:</p>\n<ol>\n<li>You start by uploading your PDF document.</li>\n<li>Our AI agent immediately gets to work, automatically translating the entire file. The most important part is that it <strong>perfectly preserves the original layout and formatting</strong>—no more broken tables or misplaced images.</li>\n<li>In just a few moments, you’re presented with a ready-to-use, fully translated document.</li>\n</ol>\n<p>The AI is smart enough to understand the document’s context, but if you want to make a quick change, you can easily review the translation and accept smart suggestions for alternative wording. The goal is that you receive a document that is 99% of the way there, or completely finished, without you having to do any of the heavy lifting.</p>\n<h2 id=\"the-technical-details\">The Technical Details</h2>\n<p>The convenience you experience on the frontend is made possible by some serious engineering on the backend.</p>\n<ul>\n<li><strong>Architecture:</strong> We use a modern stack with a <strong>Next.js</strong> frontend and a <strong>Python</strong> backend. Our database is a NoSQL solution, currently <strong>Firebase</strong>, for smooth and reliable file uploads.</li>\n<li><strong>The Core Engine:</strong> Our platform’s real power comes from our custom-built backend pipeline.\n<ol>\n<li><strong>Format Preservation Engine:</strong> When you upload a PDF, our proprietary <strong>PDF-to-Docs engine</strong> is the first thing it touches. Its single most important job is to map out your document’s structure. This allows us to reassemble the translated document perfectly, so you don’t waste any time rebuilding tables, realigning images, or fixing layouts.</li>\n<li><strong>High-Speed Translation:</strong> To deliver your translation quickly, our backend uses a sophisticated <strong>asynchronous pipeline</strong>. This system breaks down large documents and processes them in parallel using a <strong>connection pool</strong> and a <strong>thread pool</strong>. All this complexity is handled completely behind the scenes. For you, it just means you get your translated file back incredibly fast, even if it’s a very large document.</li>\n<li><strong>Automatic Reassembly:</strong> Once the translation is complete, our system automatically rebuilds the PDF, delivering a final product that looks just like your original.</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"project-summary\">Project Summary</h2>\n<p>To put it simply, <strong>Seek Hub is designed to make high-quality document translation effortless.</strong> It removes the biggest headaches from the process: losing your formatting and doing tedious manual work. Our platform’s core promise is to deliver a perfectly formatted, accurately translated PDF back to you in a fraction of the time it would normally take. By leveraging a powerful AI agent and a smart backend pipeline, we’ve created a tool that just works, letting you focus on your content, not the complex process of translation.</p>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-to-this-project","text":"My feelings to this project"},{"depth":2,"slug":"introduction","text":"Introduction"},{"depth":2,"slug":"why-were-building-this-and-what-it-does","text":"Why We’re Building This and What It Does"},{"depth":2,"slug":"the-technical-details","text":"The Technical Details"},{"depth":2,"slug":"project-summary","text":"Project Summary"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"AI Translation Startup (Seek Hub)","description":"A smart, AI-powered platform for perfect PDF-to-Word and multi-format document translation, preserving layout and formatting.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://laurence-wu.github.io/projects/","tags":["AI","Translation","PDF","DOCX","Next.js","Python","Startup"]},"imagePaths":[]}},"collection":"projects","slug":"ai-translation/ai_translation"},{"id":"buzzcar/BuzzCar.md","data":{"title":"BuzzRacer V2 Autonomous Car Project","description":"A custom-designed autonomous vehicle with advanced power management, LiDAR sensing, and compact PCB integration for algorithm development.","pubDate":"2025-07-31T00:00:00.000Z","tags":["Autonomous Vehicles","Arduino","LiDAR","PCB Design","Power Management","Embedded Systems"],"githubLink":"https://github.com/Laurence-Wu/BuzzRacer-V2.git"},"body":"## My feelings about this project\r\n\r\nThis is a project that I have always wanted to do, which is to self-design a small car. Thanks to Nick and the lab he's affiliated with, I got the chance to really design such a thing.\r\n\r\n## Technical Implementation: How It Works\r\n\r\n- **Central Controller**: An **Arduino Nano 33 IoT** serves as the brain of the vehicle, processing sensor data and controlling the actuators.\r\n- **Power Management**: This is a critical feature of the design.\r\n  - The system is powered by a **2s LiPo battery** (6.5V - 8.4V).\r\n  - It includes a crucial **hot-plug management** feature, allowing a wired DC power supply to be connected or disconnected for debugging without interrupting power to the Arduino.\r\n  - Onboard converters step the battery voltage down to a stable 5V to power the various components.\r\n  - The board also integrates **voltage sensing** to monitor battery levels for the control algorithm.\r\n- **Sensors and Actuators**:\r\n  - **Sensing**: Four **Benewake TF-Mini S Lidar** modules are used for environment detection, all communicating over a shared I2C bus.\r\n  - **Driving**: A brushed DC motor is controlled by a **TB67H451FNG motor driver IC**, with traces designed to handle up to 3A of current.\r\n  - **Steering**: A standard hobby servo handles steering, controlled directly by the Arduino.\r\n- **PCB Design**:\r\n  - The board is a **two-layer PCB** with dimensions under 100x100mm to fit the car chassis and manage manufacturing costs.\r\n  - It's designed using primarily **0805 SMT components**, which are small enough for a compact design but large enough to be hand-soldered with a heat gun.\r\n\r\n------\r\n\r\n## Conclusion\r\n\r\nIn summary, the BuzzRacer V2 project is a comprehensive engineering task to create a custom, all-in-one solution for a small-scale autonomous car. It combines mechanical design, by conforming to a specific chassis, with complex electronics design. The key technical challenges are the sophisticated **power management system** with hot-plug capability and the efficient **spatial integration** of all necessary components onto a compact, dual-layer PCB. The final result will be a durable and streamlined platform, ideal for developing and testing autonomous driving algorithms.","filePath":"src/content/projects/buzzcar/BuzzCar.md","digest":"fe2998d0b9c1ad23","rendered":{"html":"<h2 id=\"my-feelings-about-this-project\">My feelings about this project</h2>\n<p>This is a project that I have always wanted to do, which is to self-design a small car. Thanks to Nick and the lab he’s affiliated with, I got the chance to really design such a thing.</p>\n<h2 id=\"technical-implementation-how-it-works\">Technical Implementation: How It Works</h2>\n<ul>\n<li><strong>Central Controller</strong>: An <strong>Arduino Nano 33 IoT</strong> serves as the brain of the vehicle, processing sensor data and controlling the actuators.</li>\n<li><strong>Power Management</strong>: This is a critical feature of the design.\n<ul>\n<li>The system is powered by a <strong>2s LiPo battery</strong> (6.5V - 8.4V).</li>\n<li>It includes a crucial <strong>hot-plug management</strong> feature, allowing a wired DC power supply to be connected or disconnected for debugging without interrupting power to the Arduino.</li>\n<li>Onboard converters step the battery voltage down to a stable 5V to power the various components.</li>\n<li>The board also integrates <strong>voltage sensing</strong> to monitor battery levels for the control algorithm.</li>\n</ul>\n</li>\n<li><strong>Sensors and Actuators</strong>:\n<ul>\n<li><strong>Sensing</strong>: Four <strong>Benewake TF-Mini S Lidar</strong> modules are used for environment detection, all communicating over a shared I2C bus.</li>\n<li><strong>Driving</strong>: A brushed DC motor is controlled by a <strong>TB67H451FNG motor driver IC</strong>, with traces designed to handle up to 3A of current.</li>\n<li><strong>Steering</strong>: A standard hobby servo handles steering, controlled directly by the Arduino.</li>\n</ul>\n</li>\n<li><strong>PCB Design</strong>:\n<ul>\n<li>The board is a <strong>two-layer PCB</strong> with dimensions under 100x100mm to fit the car chassis and manage manufacturing costs.</li>\n<li>It’s designed using primarily <strong>0805 SMT components</strong>, which are small enough for a compact design but large enough to be hand-soldered with a heat gun.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In summary, the BuzzRacer V2 project is a comprehensive engineering task to create a custom, all-in-one solution for a small-scale autonomous car. It combines mechanical design, by conforming to a specific chassis, with complex electronics design. The key technical challenges are the sophisticated <strong>power management system</strong> with hot-plug capability and the efficient <strong>spatial integration</strong> of all necessary components onto a compact, dual-layer PCB. The final result will be a durable and streamlined platform, ideal for developing and testing autonomous driving algorithms.</p>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-about-this-project","text":"My feelings about this project"},{"depth":2,"slug":"technical-implementation-how-it-works","text":"Technical Implementation: How It Works"},{"depth":2,"slug":"conclusion","text":"Conclusion"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"BuzzRacer V2 Autonomous Car Project","description":"A custom-designed autonomous vehicle with advanced power management, LiDAR sensing, and compact PCB integration for algorithm development.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/BuzzRacer-V2.git","tags":["Autonomous Vehicles","Arduino","LiDAR","PCB Design","Power Management","Embedded Systems"]},"imagePaths":[]}},"collection":"projects","slug":"buzzcar/buzzcar"},{"id":"lips-to-speech/Lips_to_speech.md","data":{"title":"Lip-to-Speech Communication Device","description":"An assistive technology device that converts lip movements to speech using computer vision, AI models, and vibration feedback.","pubDate":"2025-07-31T00:00:00.000Z","tags":["Assistive Technology","Computer Vision","AI/ML","ESP32","Speech Synthesis","Accessibility"],"githubLink":"https://github.com/Laurence-Wu/LipToSpeech.git"},"body":"## Technical Implementation: \r\n\r\nThe project uses a distributed system where the PCB handles data capture and a laptop performs the heavy computational tasks.\r\n\r\n- **Hardware and Power Design**:\r\n  - **Power Source**: The device is powered by two LiPo batteries in series, providing **7.4V**.\r\n  - **Voltage Regulation**: A clever two-stage power system is used. First, an efficient **buck converter** steps the voltage down to 5V. Then, a **Low-Dropout (LDO) regulator** provides a very stable 3.3V output to power the sensitive main processor (ESP32-S3) and the camera. This design combines the high efficiency of a buck converter with the clean, stable power of an LDO, which is critical for the processor's performance.\r\n  - **Core Components**: The PCB is built around an **ESP32-S3 microcontroller** and an **OV5640 camera module**.\r\n- **Data Processing and Workflow**:\r\n  1. The **OV5640 camera** captures images of the user's lips.\r\n  2. The **ESP32-S3** processor takes these images and uses its built-in Wi-Fi to stream the data to a connected laptop.\r\n  3. All the intensive AI processing happens on the **laptop**:\r\n     - An open-source **lip-to-text model** analyzes the video stream and generates text.\r\n     - An open-source **text-to-speech model** converts that text into an audio signal.\r\n  4. The final audio signal is sent back to the ESP32-S3.\r\n  5. The ESP32-S3 outputs this as a **vibration signal** through a connected sound module.","filePath":"src/content/projects/lips-to-speech/Lips_to_speech.md","digest":"aeaf39cd9f57c147","rendered":{"html":"<h2 id=\"technical-implementation\">Technical Implementation:</h2>\n<p>The project uses a distributed system where the PCB handles data capture and a laptop performs the heavy computational tasks.</p>\n<ul>\n<li><strong>Hardware and Power Design</strong>:\n<ul>\n<li><strong>Power Source</strong>: The device is powered by two LiPo batteries in series, providing <strong>7.4V</strong>.</li>\n<li><strong>Voltage Regulation</strong>: A clever two-stage power system is used. First, an efficient <strong>buck converter</strong> steps the voltage down to 5V. Then, a <strong>Low-Dropout (LDO) regulator</strong> provides a very stable 3.3V output to power the sensitive main processor (ESP32-S3) and the camera. This design combines the high efficiency of a buck converter with the clean, stable power of an LDO, which is critical for the processor’s performance.</li>\n<li><strong>Core Components</strong>: The PCB is built around an <strong>ESP32-S3 microcontroller</strong> and an <strong>OV5640 camera module</strong>.</li>\n</ul>\n</li>\n<li><strong>Data Processing and Workflow</strong>:\n<ol>\n<li>The <strong>OV5640 camera</strong> captures images of the user’s lips.</li>\n<li>The <strong>ESP32-S3</strong> processor takes these images and uses its built-in Wi-Fi to stream the data to a connected laptop.</li>\n<li>All the intensive AI processing happens on the <strong>laptop</strong>:\n<ul>\n<li>An open-source <strong>lip-to-text model</strong> analyzes the video stream and generates text.</li>\n<li>An open-source <strong>text-to-speech model</strong> converts that text into an audio signal.</li>\n</ul>\n</li>\n<li>The final audio signal is sent back to the ESP32-S3.</li>\n<li>The ESP32-S3 outputs this as a <strong>vibration signal</strong> through a connected sound module.</li>\n</ol>\n</li>\n</ul>","metadata":{"headings":[{"depth":2,"slug":"technical-implementation","text":"Technical Implementation:"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Lip-to-Speech Communication Device","description":"An assistive technology device that converts lip movements to speech using computer vision, AI models, and vibration feedback.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/LipToSpeech.git","tags":["Assistive Technology","Computer Vision","AI/ML","ESP32","Speech Synthesis","Accessibility"]},"imagePaths":[]}},"collection":"projects","slug":"lips-to-speech/lips_to_speech"},{"id":"do-not-disturb/DoNotDistrub.md","data":{"title":"Do Not Disturb Robotics Arm Project","description":"A brain-computer interface and computer vision powered robotic arm that intelligently prevents interruptions during deep focus.","pubDate":"2025-07-31T00:00:00.000Z","tags":["Robotics","BCI","Computer Vision","Raspberry Pi","PID Control","Python"],"githubLink":"https://github.com/Laurence-Wu/RoboticsArmControl.git"},"body":"## My feelings to this project\r\n\r\nThis project means everything to me. I built it solo during a four-day hackathon, fueled by passion and just six hours of sleep. I was so exhausted afterward that I bombed my finals in statistics and signal processing. You could say I traded my GPA for this project, but it was worth it.\r\n\r\nThat was the moment I knew I had to be in robotics—it just clicked. It was the perfect fusion of computer vision, embedded systems, control theory, and PCB design, all working together to bring a complex robotic arm to life. I still vividly remember the late nights: writing error handling / testing scripts, recording and adding safety angular constraints,troubleshooting communication failures on the shared serial port, and fixing the burned out motors from overcurrent.\r\n\r\nThe more I thought about it, the more I understood. This world may not be a world of science; it might be a complex world that are engineered with 1000 more preprocessing steps.\r\n\r\n## Introduction\r\n\r\nThe \"Do Not Disturb\" project is an integrated robotics system designed to intelligently prevent interruptions based on a user's mental state. The project was conceived from the personal experience of being disrupted while in a state of deep focus, or \"flow.\" By combining Brain-Computer Interface (BCI) technology with computer vision and a robotic arm, the system serves as an automated guardian that only activates when the user is genuinely concentrating, solving the nuanced problem of distinguishing between welcome and unwelcome interruptions.\r\n\r\n## Why This Project and What It Does\r\n\r\nThe primary motivation behind this project was to create a system that could understand the user's level of focus and act accordingly. The creator identified that not all interruptions are undesirable; sometimes, a distraction is welcome when one is not deeply engaged in a task. The challenge was to build a system that could differentiate between a focused state and an unfocused one.\r\n\r\nThe system operates through the collaboration of two main components:\r\n\r\n1. **Brainwave Analysis System:** A user wears a BCI headset that captures EEG (electroencephalogram) data. This raw data is first processed by the manufacturer's API and then fed into a custom deep learning network. This network, trained on public data, performs a two-category classification to determine if the user is currently \"focusing\" or \"not focusing.\"\r\n2. **Robotic Guardian System:** The classification result is sent over the cloud to a **Raspberry Pi** that controls a robotic arm. If the system detects the user is in a state of focus, it activates the arm. Using an onboard camera, the arm employs a visual tracking algorithm to detect and follow the face of any person who approaches, acting as a clear, non-verbal \"Do Not Disturb\" signal.\r\n\r\n## Technical Details\r\n\r\nThe project's architecture integrates complex software and hardware components, primarily developed in **Python** for its rapid prototyping capabilities and SDK availability.\r\n\r\n- **Computer Vision:** The visual tracking system uses a two-stage approach for robust performance. It first uses **Haar Cascades** for general human face detection. Then, a deep learning model is used for more precise identification and tracking. To prevent erratic arm movements, a **\"lockdown period\"** was implemented; if the target face is lost, the system waits a few seconds and predicts the face's next likely position before re-engaging, ensuring smoother motion.\r\n- **Robotic Arm Control:** The robotic arm has **2 degrees of freedom ** and is controlled using a **PID (Proportional-Integral-Derivative)** algorithm. This required the meticulous tuning of six distinct PID parameters (three for each axis) to accurately translate the 2D coordinates from the camera feed into physical movement. This process was a significant challenge, involving extensive debugging of signs and values to correct initial bugs that caused the arm to lock up or behave erratically.\r\n- **System Integration and Challenges:** The Raspberry Pi communicated with the robotic arm over a **single-threaded serial port**. This limitation made it difficult to run the PID control loop concurrently with other logic, such as returning the arm to a home position. The solution was to create a routine that could rapidly send velocity commands to override the PID data flow when necessary. Other challenges included creating an accurate mapping between the camera's pixels and the arm's real-world distance and dealing with hardware failures, such as two servo motors breaking from over-current before software constraints were added.\r\n\r\n## Project Summary\r\n\r\nThe \"Do Not Disturb\" project is a successful proof-of-concept that effectively integrates BCI, deep learning, and robotics to solve a nuanced real-world problem. The development process provided a significant learning experience in complex system integration, PID control algorithms, hardware interfacing with a Raspberry Pi, and robust debugging. The project culminated in winning first place in the \"Brain Interaction\" robotics category at a competition. The complete codebase, including testing scripts and error handling, is available on GitHub for further review.","filePath":"src/content/projects/do-not-disturb/DoNotDistrub.md","digest":"4ea3874fb2188451","rendered":{"html":"<h2 id=\"my-feelings-to-this-project\">My feelings to this project</h2>\n<p>This project means everything to me. I built it solo during a four-day hackathon, fueled by passion and just six hours of sleep. I was so exhausted afterward that I bombed my finals in statistics and signal processing. You could say I traded my GPA for this project, but it was worth it.</p>\n<p>That was the moment I knew I had to be in robotics—it just clicked. It was the perfect fusion of computer vision, embedded systems, control theory, and PCB design, all working together to bring a complex robotic arm to life. I still vividly remember the late nights: writing error handling / testing scripts, recording and adding safety angular constraints,troubleshooting communication failures on the shared serial port, and fixing the burned out motors from overcurrent.</p>\n<p>The more I thought about it, the more I understood. This world may not be a world of science; it might be a complex world that are engineered with 1000 more preprocessing steps.</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>The “Do Not Disturb” project is an integrated robotics system designed to intelligently prevent interruptions based on a user’s mental state. The project was conceived from the personal experience of being disrupted while in a state of deep focus, or “flow.” By combining Brain-Computer Interface (BCI) technology with computer vision and a robotic arm, the system serves as an automated guardian that only activates when the user is genuinely concentrating, solving the nuanced problem of distinguishing between welcome and unwelcome interruptions.</p>\n<h2 id=\"why-this-project-and-what-it-does\">Why This Project and What It Does</h2>\n<p>The primary motivation behind this project was to create a system that could understand the user’s level of focus and act accordingly. The creator identified that not all interruptions are undesirable; sometimes, a distraction is welcome when one is not deeply engaged in a task. The challenge was to build a system that could differentiate between a focused state and an unfocused one.</p>\n<p>The system operates through the collaboration of two main components:</p>\n<ol>\n<li><strong>Brainwave Analysis System:</strong> A user wears a BCI headset that captures EEG (electroencephalogram) data. This raw data is first processed by the manufacturer’s API and then fed into a custom deep learning network. This network, trained on public data, performs a two-category classification to determine if the user is currently “focusing” or “not focusing.”</li>\n<li><strong>Robotic Guardian System:</strong> The classification result is sent over the cloud to a <strong>Raspberry Pi</strong> that controls a robotic arm. If the system detects the user is in a state of focus, it activates the arm. Using an onboard camera, the arm employs a visual tracking algorithm to detect and follow the face of any person who approaches, acting as a clear, non-verbal “Do Not Disturb” signal.</li>\n</ol>\n<h2 id=\"technical-details\">Technical Details</h2>\n<p>The project’s architecture integrates complex software and hardware components, primarily developed in <strong>Python</strong> for its rapid prototyping capabilities and SDK availability.</p>\n<ul>\n<li><strong>Computer Vision:</strong> The visual tracking system uses a two-stage approach for robust performance. It first uses <strong>Haar Cascades</strong> for general human face detection. Then, a deep learning model is used for more precise identification and tracking. To prevent erratic arm movements, a <strong>“lockdown period”</strong> was implemented; if the target face is lost, the system waits a few seconds and predicts the face’s next likely position before re-engaging, ensuring smoother motion.</li>\n<li><strong>Robotic Arm Control:</strong> The robotic arm has **2 degrees of freedom ** and is controlled using a <strong>PID (Proportional-Integral-Derivative)</strong> algorithm. This required the meticulous tuning of six distinct PID parameters (three for each axis) to accurately translate the 2D coordinates from the camera feed into physical movement. This process was a significant challenge, involving extensive debugging of signs and values to correct initial bugs that caused the arm to lock up or behave erratically.</li>\n<li><strong>System Integration and Challenges:</strong> The Raspberry Pi communicated with the robotic arm over a <strong>single-threaded serial port</strong>. This limitation made it difficult to run the PID control loop concurrently with other logic, such as returning the arm to a home position. The solution was to create a routine that could rapidly send velocity commands to override the PID data flow when necessary. Other challenges included creating an accurate mapping between the camera’s pixels and the arm’s real-world distance and dealing with hardware failures, such as two servo motors breaking from over-current before software constraints were added.</li>\n</ul>\n<h2 id=\"project-summary\">Project Summary</h2>\n<p>The “Do Not Disturb” project is a successful proof-of-concept that effectively integrates BCI, deep learning, and robotics to solve a nuanced real-world problem. The development process provided a significant learning experience in complex system integration, PID control algorithms, hardware interfacing with a Raspberry Pi, and robust debugging. The project culminated in winning first place in the “Brain Interaction” robotics category at a competition. The complete codebase, including testing scripts and error handling, is available on GitHub for further review.</p>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-to-this-project","text":"My feelings to this project"},{"depth":2,"slug":"introduction","text":"Introduction"},{"depth":2,"slug":"why-this-project-and-what-it-does","text":"Why This Project and What It Does"},{"depth":2,"slug":"technical-details","text":"Technical Details"},{"depth":2,"slug":"project-summary","text":"Project Summary"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Do Not Disturb Robotics Arm Project","description":"A brain-computer interface and computer vision powered robotic arm that intelligently prevents interruptions during deep focus.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/RoboticsArmControl.git","tags":["Robotics","BCI","Computer Vision","Raspberry Pi","PID Control","Python"]},"imagePaths":[]}},"collection":"projects","slug":"do-not-disturb/donotdistrub"},{"id":"flight-computer/Flight_Computer.md","data":{"title":"Rocket Flight Computer System","description":"A sophisticated flight computer for rockets featuring STM32MP1 processor, multi-level memory hierarchy, and comprehensive sensor integration.","pubDate":"2025-07-31T00:00:00.000Z","tags":["Aerospace","STM32","Flight Control","Embedded Systems","Sensors","Real-time Systems"],"githubLink":"https://github.com/Laurence-Wu/RocketFlightComputer.git"},"body":"## Technical Implementation: How It Works\r\n\r\nThe system is built around a powerful microprocessor and uses a sophisticated architecture for power, memory, and communication.\r\n\r\n- **Power System**:\r\n  - It starts with a **14.8V LiPo battery**.\r\n  - The Power Board uses two-phase buck converters to efficiently step this down to **7.4V and 5V**.\r\n  - The 5V rail powers the Flight Computer, which uses a **PMIC** (Power Management IC) to generate a stable **3.3V** for sensitive components like sensors and memory chips.\r\n- **Processing and Memory**:\r\n  - **Processor**: The brain of the flight computer is a powerful **STM32MP157 series** microprocessor.\r\n  - **Memory Hierarchy**: A multi-level memory system is used to balance speed and cost:\r\n    - **DDR RAM**: For fast, volatile memory access.\r\n    - **NOR Flash**: For fast booting of the system.\r\n    - **NAND Flash / SD Card**: For cheaper, high-capacity, non-volatile data storage.\r\n- **Sensors and Communication**:\r\n  - **Sensors**: The board integrates a **GPS**, **IMU** (Inertial Measurement Unit), and a **barometer** to track the rocket's position, orientation, and altitude.\r\n  - **Communication Protocols**: Different protocols are used for specific tasks to optimize performance:\r\n    - **I2C**: For communicating with the sensors.\r\n    - **SPI**: For high-speed communication with the NOR flash.\r\n    - **SDIO**: For communicating with the SD card.\r\n    - **CAN bus**: For robust communication between the Flight Computer and the Telemetry board.","filePath":"src/content/projects/flight-computer/Flight_Computer.md","digest":"86ad15833d89612f","rendered":{"html":"<h2 id=\"technical-implementation-how-it-works\">Technical Implementation: How It Works</h2>\n<p>The system is built around a powerful microprocessor and uses a sophisticated architecture for power, memory, and communication.</p>\n<ul>\n<li><strong>Power System</strong>:\n<ul>\n<li>It starts with a <strong>14.8V LiPo battery</strong>.</li>\n<li>The Power Board uses two-phase buck converters to efficiently step this down to <strong>7.4V and 5V</strong>.</li>\n<li>The 5V rail powers the Flight Computer, which uses a <strong>PMIC</strong> (Power Management IC) to generate a stable <strong>3.3V</strong> for sensitive components like sensors and memory chips.</li>\n</ul>\n</li>\n<li><strong>Processing and Memory</strong>:\n<ul>\n<li><strong>Processor</strong>: The brain of the flight computer is a powerful <strong>STM32MP157 series</strong> microprocessor.</li>\n<li><strong>Memory Hierarchy</strong>: A multi-level memory system is used to balance speed and cost:\n<ul>\n<li><strong>DDR RAM</strong>: For fast, volatile memory access.</li>\n<li><strong>NOR Flash</strong>: For fast booting of the system.</li>\n<li><strong>NAND Flash / SD Card</strong>: For cheaper, high-capacity, non-volatile data storage.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Sensors and Communication</strong>:\n<ul>\n<li><strong>Sensors</strong>: The board integrates a <strong>GPS</strong>, <strong>IMU</strong> (Inertial Measurement Unit), and a <strong>barometer</strong> to track the rocket’s position, orientation, and altitude.</li>\n<li><strong>Communication Protocols</strong>: Different protocols are used for specific tasks to optimize performance:\n<ul>\n<li><strong>I2C</strong>: For communicating with the sensors.</li>\n<li><strong>SPI</strong>: For high-speed communication with the NOR flash.</li>\n<li><strong>SDIO</strong>: For communicating with the SD card.</li>\n<li><strong>CAN bus</strong>: For robust communication between the Flight Computer and the Telemetry board.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","metadata":{"headings":[{"depth":2,"slug":"technical-implementation-how-it-works","text":"Technical Implementation: How It Works"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Rocket Flight Computer System","description":"A sophisticated flight computer for rockets featuring STM32MP1 processor, multi-level memory hierarchy, and comprehensive sensor integration.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/RocketFlightComputer.git","tags":["Aerospace","STM32","Flight Control","Embedded Systems","Sensors","Real-time Systems"]},"imagePaths":[]}},"collection":"projects","slug":"flight-computer/flight_computer"},{"id":"personal-website/personal_website.md","data":{"title":"Personal Website: Custom Astro Blog & Interactive Art Canvas","description":"A fully custom, open-source personal website and blog built with Astro, featuring an interactive Impressionist art canvas and a real-time Markdown-to-MDX engine.","pubDate":"2024-07-31T00:00:00.000Z","tags":["Astro","TypeScript","React","Canvas","Open Source","Frontend"],"githubLink":"https://github.com/Laurence-Wu/laurence-wu.github.io.git"},"body":"This is my custom-built personal website. I created it because I was really frustrated with the limitations of my old site, which used a Hexo theme. I wanted more creative freedom and an easier way to customize things, so I decided to build a new site from scratch using the Astro framework. This site is now my personal blog, but it's also a showcase for some of my technical and artistic ideas, especially the interactive homepage and a custom content-processing engine I built.\r\n\r\n#### Why I Built This and What It Does\r\n\r\nThe main reason I built this was that I just couldn't stand the tech on my old site anymore. It was built with a Hexo theme that used **Stylus**, a CSS preprocessor, and JS without a clean architecture. The whole structure was confusing, and the deployment process basically locked me in, killing any real freedom to change things.\r\n\r\nTo fix this, I built a new site that does two main things:\r\n\r\n1. **It's My Personal Blog:** I wanted a place to host articles where I could easily show complex technical info, like mathematical formulas using LATEX and diagrams using Mermaid.js, without any hassle.\r\n2. **It's an Interactive Art Canvas:** This is the part I'm really excited about. I'm a huge fan of **Impressionism and Neo-Impressionism** (like Van Gogh), so I made the homepage a huge canvas where you can draw. As you draw, your input is transformed in real-time into an artistic, Impressionist-style rendering. My goal was to build an algorithm that could let anyone create this style of art easily, while still running smoothly.\r\n\r\n#### The Technical Details\r\n\r\nLet's get into the technical side of things. I kept it simple by making it a frontend-only application hosted on **GitHub Pages**. This meant I didn't have to worry about a backend and could just focus on the user interface.\r\n\r\n- **Framework:** I built the site with **Astro**. I chose it because it's really popular for blogs and has great integrations for all the tools I wanted to use, like TypeScript, React, and those Mermaid graphs and math formulas.\r\n- **Interactive Canvas:** The drawing feature was a fun challenge. To get that smooth, fluid animation, I realized I had to focus on calculating particle **velocities** instead of just their positions. I used derivatives and randomly split angles to create the final artwork, and spent a good amount of time optimizing it for performance.\r\n- **My Custom Markdown to MDX Engine:** One of the biggest pieces of this project was a custom engine I built to automatically convert my standard Markdown (`.md`) files into MDX (`.mdx`). Honestly, manually adding all the JavaScript imports and tags just to show a graph or a formula in MDX is a huge pain. So, I built this engine to handle it for me. It uses **regular expressions** to parse the content—which took a lot of time and tuning to get right. It works in **real-time**, so whenever I save a file, it automatically regenerates the MDX, just like the hot-reloading you see in modern frameworks.\r\n- **Animations and Search:** You'll probably notice I added a lot of **CSS animations**—I just really like how they make a site feel alive. I know it might be a bit heavy on older computers since all the rendering happens on your machine, not a server. For the search feature, I just used a pre-existing `npm` package that works really well.\r\n\r\n\r\n\r\n#### Project Summary\r\n\r\nSo, to sum it all up, this website is my personal platform, built from the ground up with Astro because my old site was just too limiting. The parts I'm most proud of are the Impressionist drawing canvas on the homepage and the custom Markdown-to-MDX engine that makes writing posts so much easier. The whole project is entirely frontend-based, heavily animated, and **open-source**. Feel free to check out the code—if you have ideas or want to contribute, I'd be more than happy to hear from you! It represents a huge personal effort to blend the artistic things I love with some really challenging technical work.","filePath":"src/content/projects/personal-website/personal_website.md","digest":"dd98e9529fec5190","rendered":{"html":"<p>This is my custom-built personal website. I created it because I was really frustrated with the limitations of my old site, which used a Hexo theme. I wanted more creative freedom and an easier way to customize things, so I decided to build a new site from scratch using the Astro framework. This site is now my personal blog, but it’s also a showcase for some of my technical and artistic ideas, especially the interactive homepage and a custom content-processing engine I built.</p>\n<h4 id=\"why-i-built-this-and-what-it-does\">Why I Built This and What It Does</h4>\n<p>The main reason I built this was that I just couldn’t stand the tech on my old site anymore. It was built with a Hexo theme that used <strong>Stylus</strong>, a CSS preprocessor, and JS without a clean architecture. The whole structure was confusing, and the deployment process basically locked me in, killing any real freedom to change things.</p>\n<p>To fix this, I built a new site that does two main things:</p>\n<ol>\n<li><strong>It’s My Personal Blog:</strong> I wanted a place to host articles where I could easily show complex technical info, like mathematical formulas using LATEX and diagrams using Mermaid.js, without any hassle.</li>\n<li><strong>It’s an Interactive Art Canvas:</strong> This is the part I’m really excited about. I’m a huge fan of <strong>Impressionism and Neo-Impressionism</strong> (like Van Gogh), so I made the homepage a huge canvas where you can draw. As you draw, your input is transformed in real-time into an artistic, Impressionist-style rendering. My goal was to build an algorithm that could let anyone create this style of art easily, while still running smoothly.</li>\n</ol>\n<h4 id=\"the-technical-details\">The Technical Details</h4>\n<p>Let’s get into the technical side of things. I kept it simple by making it a frontend-only application hosted on <strong>GitHub Pages</strong>. This meant I didn’t have to worry about a backend and could just focus on the user interface.</p>\n<ul>\n<li><strong>Framework:</strong> I built the site with <strong>Astro</strong>. I chose it because it’s really popular for blogs and has great integrations for all the tools I wanted to use, like TypeScript, React, and those Mermaid graphs and math formulas.</li>\n<li><strong>Interactive Canvas:</strong> The drawing feature was a fun challenge. To get that smooth, fluid animation, I realized I had to focus on calculating particle <strong>velocities</strong> instead of just their positions. I used derivatives and randomly split angles to create the final artwork, and spent a good amount of time optimizing it for performance.</li>\n<li><strong>My Custom Markdown to MDX Engine:</strong> One of the biggest pieces of this project was a custom engine I built to automatically convert my standard Markdown (<code>.md</code>) files into MDX (<code>.mdx</code>). Honestly, manually adding all the JavaScript imports and tags just to show a graph or a formula in MDX is a huge pain. So, I built this engine to handle it for me. It uses <strong>regular expressions</strong> to parse the content—which took a lot of time and tuning to get right. It works in <strong>real-time</strong>, so whenever I save a file, it automatically regenerates the MDX, just like the hot-reloading you see in modern frameworks.</li>\n<li><strong>Animations and Search:</strong> You’ll probably notice I added a lot of <strong>CSS animations</strong>—I just really like how they make a site feel alive. I know it might be a bit heavy on older computers since all the rendering happens on your machine, not a server. For the search feature, I just used a pre-existing <code>npm</code> package that works really well.</li>\n</ul>\n<h4 id=\"project-summary\">Project Summary</h4>\n<p>So, to sum it all up, this website is my personal platform, built from the ground up with Astro because my old site was just too limiting. The parts I’m most proud of are the Impressionist drawing canvas on the homepage and the custom Markdown-to-MDX engine that makes writing posts so much easier. The whole project is entirely frontend-based, heavily animated, and <strong>open-source</strong>. Feel free to check out the code—if you have ideas or want to contribute, I’d be more than happy to hear from you! It represents a huge personal effort to blend the artistic things I love with some really challenging technical work.</p>","metadata":{"headings":[{"depth":4,"slug":"why-i-built-this-and-what-it-does","text":"Why I Built This and What It Does"},{"depth":4,"slug":"the-technical-details","text":"The Technical Details"},{"depth":4,"slug":"project-summary","text":"Project Summary"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Personal Website: Custom Astro Blog & Interactive Art Canvas","description":"A fully custom, open-source personal website and blog built with Astro, featuring an interactive Impressionist art canvas and a real-time Markdown-to-MDX engine.","pubDate":"2024-07-31T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/laurence-wu.github.io.git","tags":["Astro","TypeScript","React","Canvas","Open Source","Frontend"]},"imagePaths":[]}},"collection":"projects","slug":"personal-website/personal_website"},{"id":"spoiler-alert/Spoiler_alert.md","data":{"title":"Spoiler Alert: Fridge Website to Alert People About Their Spoiled Food","description":"A comprehensive web application for smart fridge management with AI-powered recipe generation and food expiration tracking.","pubDate":"2024-01-15T00:00:00.000Z","tags":["React","Flask","MySQL","HTTPS","Web Development","Full Stack"],"githubLink":"https://github.com/Laurence-Wu/spoiler_alert.git"},"body":"## My feelings about this project\r\n\r\nAs a first-year student, I was excited to build my first professional website. This was at a time before AI was as powerful as it is today. I remember contemplating the entire backend architecture with Flask, weighing it against tools like SQLAlchemy, and handling all the manual work involved.\r\n\r\nThat experience makes me wonder: Is AI truly advancing the journey of a programmer? Will we still feel that same sense of, \"I built this from the ground up,\" after finishing a project? I don't have all the answers, but I feel lucky that I had the chance to experience building a website manually.\r\n\r\n## Features\r\n\r\n#### 1.1 Smart Fridge Management\r\n\r\n- **Visual Fridge Interface**: An Interactive fridge that opens/closes to show contents\r\n- **Food Item Tracking**: Add, view, and manage food items with quantities and expiration dates\r\n- **Expiration Alerts**: Visual warnings for items nearing expiration (within 3-7 days)\r\n- **Freezer Support**: Toggle items between fridge and freezer storage\r\n- **AI-Powered Suggestions**: Automatic category and expiration date suggestions using Google Gemini AI\r\n- **Search & Sort**: Find and organize food items by various attributes\r\n\r\n#### 1.2 Recipe Generation\r\n\r\n- **AI Recipe Generation**: Generate recipes based on available fridge ingredients using Google Gemini AI\r\n- **Multiple Recipe Options**: Generate up to 3 different recipe suggestions per request\r\n- **Detailed Recipe View**: View complete recipe instructions and ingredient lists\r\n\r\n#### 1.3 Shopping List & Wishlist\r\n\r\n- **Recipe-Based Shopping**: Create shopping lists from saved recipes\r\n- **Ingredient Comparison**: Compare recipe requirements with current fridge contents\r\n- **Smart Shopping Lists**: Automatically calculate missing ingredients\r\n\r\n#### 1.4 User Management\r\n\r\n- **User Authentication**: Secure login/signup with password hashing (bcrypt)\r\n- **Multi-User Support**: Each user can have their own fridges and recipes\r\n- **Fridge Sharing**: Support for multiple users accessing shared fridges\r\n\r\n### Technology Stack\r\n\r\n#### 2.1 Frontend\r\n\r\n- **React 18.3.1** - Modern React with hooks and functional components\r\n- **Axios 1.7.7** - HTTP client for API calls\r\n- **Google Generative AI 0.21.0** - Integration with Gemini AI for recipe generation\r\n- **CSS Modules** - Scoped styling\r\n\r\n#### 2.2 Backend\r\n\r\n- **Flask** - Python web framework\r\n- **PyMySQL** - MySQL database connector\r\n- **bcrypt** - Password hashing\r\n- **Flask-CORS** - Cross-origin resource sharing\r\n\r\n### Project Structure\r\n\r\n```\r\nspoiler_alert/\r\n├── frontend/                 # React frontend application\r\n│   ├── src/\r\n│   │   ├── Home/            # Landing page components\r\n│   │   ├── Login/           # Authentication pages\r\n│   │   ├── Signup/          \r\n│   │   ├── Fridge/          # Main fridge management interface\r\n│   │   ├── Recipes/         # Recipe generation and display\r\n│   │   ├── Wishlist/        # Shopping list management\r\n│   │   ├── Navbar/          # Navigation component\r\n│   │   └── Util.js          # Utility functions and mock data\r\n│   └── public/              # Static assets\r\n├── backend/                 # Flask backend API\r\n│   ├── app.py              # Main Flask application\r\n│   ├── get_data.py         # Database read operations\r\n│   ├── add_to_table.py     # Database insert operations\r\n│   ├── remove_from_table.py # Database delete operations\r\n│   ├── update_table.py     # Database update operations\r\n│   └── templates/          # Flask templates (if needed)\r\n└── README.md\r\n```","filePath":"src/content/projects/spoiler-alert/Spoiler_alert.md","digest":"9d493c45231bf7d2","rendered":{"html":"<h2 id=\"my-feelings-about-this-project\">My feelings about this project</h2>\n<p>As a first-year student, I was excited to build my first professional website. This was at a time before AI was as powerful as it is today. I remember contemplating the entire backend architecture with Flask, weighing it against tools like SQLAlchemy, and handling all the manual work involved.</p>\n<p>That experience makes me wonder: Is AI truly advancing the journey of a programmer? Will we still feel that same sense of, “I built this from the ground up,” after finishing a project? I don’t have all the answers, but I feel lucky that I had the chance to experience building a website manually.</p>\n<h2 id=\"features\">Features</h2>\n<h4 id=\"11-smart-fridge-management\">1.1 Smart Fridge Management</h4>\n<ul>\n<li><strong>Visual Fridge Interface</strong>: An Interactive fridge that opens/closes to show contents</li>\n<li><strong>Food Item Tracking</strong>: Add, view, and manage food items with quantities and expiration dates</li>\n<li><strong>Expiration Alerts</strong>: Visual warnings for items nearing expiration (within 3-7 days)</li>\n<li><strong>Freezer Support</strong>: Toggle items between fridge and freezer storage</li>\n<li><strong>AI-Powered Suggestions</strong>: Automatic category and expiration date suggestions using Google Gemini AI</li>\n<li><strong>Search &#x26; Sort</strong>: Find and organize food items by various attributes</li>\n</ul>\n<h4 id=\"12-recipe-generation\">1.2 Recipe Generation</h4>\n<ul>\n<li><strong>AI Recipe Generation</strong>: Generate recipes based on available fridge ingredients using Google Gemini AI</li>\n<li><strong>Multiple Recipe Options</strong>: Generate up to 3 different recipe suggestions per request</li>\n<li><strong>Detailed Recipe View</strong>: View complete recipe instructions and ingredient lists</li>\n</ul>\n<h4 id=\"13-shopping-list--wishlist\">1.3 Shopping List &#x26; Wishlist</h4>\n<ul>\n<li><strong>Recipe-Based Shopping</strong>: Create shopping lists from saved recipes</li>\n<li><strong>Ingredient Comparison</strong>: Compare recipe requirements with current fridge contents</li>\n<li><strong>Smart Shopping Lists</strong>: Automatically calculate missing ingredients</li>\n</ul>\n<h4 id=\"14-user-management\">1.4 User Management</h4>\n<ul>\n<li><strong>User Authentication</strong>: Secure login/signup with password hashing (bcrypt)</li>\n<li><strong>Multi-User Support</strong>: Each user can have their own fridges and recipes</li>\n<li><strong>Fridge Sharing</strong>: Support for multiple users accessing shared fridges</li>\n</ul>\n<h3 id=\"technology-stack\">Technology Stack</h3>\n<h4 id=\"21-frontend\">2.1 Frontend</h4>\n<ul>\n<li><strong>React 18.3.1</strong> - Modern React with hooks and functional components</li>\n<li><strong>Axios 1.7.7</strong> - HTTP client for API calls</li>\n<li><strong>Google Generative AI 0.21.0</strong> - Integration with Gemini AI for recipe generation</li>\n<li><strong>CSS Modules</strong> - Scoped styling</li>\n</ul>\n<h4 id=\"22-backend\">2.2 Backend</h4>\n<ul>\n<li><strong>Flask</strong> - Python web framework</li>\n<li><strong>PyMySQL</strong> - MySQL database connector</li>\n<li><strong>bcrypt</strong> - Password hashing</li>\n<li><strong>Flask-CORS</strong> - Cross-origin resource sharing</li>\n</ul>\n<h3 id=\"project-structure\">Project Structure</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>spoiler_alert/</span></span>\n<span class=\"line\"><span>├── frontend/                 # React frontend application</span></span>\n<span class=\"line\"><span>│   ├── src/</span></span>\n<span class=\"line\"><span>│   │   ├── Home/            # Landing page components</span></span>\n<span class=\"line\"><span>│   │   ├── Login/           # Authentication pages</span></span>\n<span class=\"line\"><span>│   │   ├── Signup/          </span></span>\n<span class=\"line\"><span>│   │   ├── Fridge/          # Main fridge management interface</span></span>\n<span class=\"line\"><span>│   │   ├── Recipes/         # Recipe generation and display</span></span>\n<span class=\"line\"><span>│   │   ├── Wishlist/        # Shopping list management</span></span>\n<span class=\"line\"><span>│   │   ├── Navbar/          # Navigation component</span></span>\n<span class=\"line\"><span>│   │   └── Util.js          # Utility functions and mock data</span></span>\n<span class=\"line\"><span>│   └── public/              # Static assets</span></span>\n<span class=\"line\"><span>├── backend/                 # Flask backend API</span></span>\n<span class=\"line\"><span>│   ├── app.py              # Main Flask application</span></span>\n<span class=\"line\"><span>│   ├── get_data.py         # Database read operations</span></span>\n<span class=\"line\"><span>│   ├── add_to_table.py     # Database insert operations</span></span>\n<span class=\"line\"><span>│   ├── remove_from_table.py # Database delete operations</span></span>\n<span class=\"line\"><span>│   ├── update_table.py     # Database update operations</span></span>\n<span class=\"line\"><span>│   └── templates/          # Flask templates (if needed)</span></span>\n<span class=\"line\"><span>└── README.md</span></span></code></pre>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-about-this-project","text":"My feelings about this project"},{"depth":2,"slug":"features","text":"Features"},{"depth":4,"slug":"11-smart-fridge-management","text":"1.1 Smart Fridge Management"},{"depth":4,"slug":"12-recipe-generation","text":"1.2 Recipe Generation"},{"depth":4,"slug":"13-shopping-list--wishlist","text":"1.3 Shopping List & Wishlist"},{"depth":4,"slug":"14-user-management","text":"1.4 User Management"},{"depth":3,"slug":"technology-stack","text":"Technology Stack"},{"depth":4,"slug":"21-frontend","text":"2.1 Frontend"},{"depth":4,"slug":"22-backend","text":"2.2 Backend"},{"depth":3,"slug":"project-structure","text":"Project Structure"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Spoiler Alert: Fridge Website to Alert People About Their Spoiled Food","description":"A comprehensive web application for smart fridge management with AI-powered recipe generation and food expiration tracking.","pubDate":"2024-01-15T00:00:00.000Z","githubLink":"https://github.com/Laurence-Wu/spoiler_alert.git","tags":["React","Flask","MySQL","HTTPS","Web Development","Full Stack"]},"imagePaths":[]}},"collection":"projects","slug":"spoiler-alert/spoiler_alert"},{"id":"story-forge/Story Forge An AI chatting game that allows the user to break the linear storyline.md","data":{"title":"Story Forge: AI-Powered Nonlinear Narrative Game","description":"A full-stack AI-driven text adventure game engine that lets players break the linear storyline and co-create unique stories.","pubDate":"2025-07-31T00:00:00.000Z","tags":["AI","Game Engine","React","TypeScript","WebSockets","Prompt Engineering","Full Stack"],"githubLink":"http://115.29.205.103/"},"body":"*(sorry this is not an open source project)*\r\n\r\n## My feelings to this project\r\n\r\nThis is a project that, when I first heard its idea,  a thought just hits me: Fuck everything and I'm gonna make this project happen. It felt like the project I was always meant to work on—one that connected a deep fascination with philosophy to the art of creating for the web. During that hackathon, our team built it within 3 days and got the first price in software development track.\r\n\r\n## Project Overview\r\n\r\n> A revolutionary text adventure game engine that breaks the boundaries between player agency and narrative coherence\r\n\r\nAt its heart, Story Forge was born from a simple but powerful idea: what if a story wasn't just something you followed, but something you helped create meaning for? We looked at traditional games and saw beautiful, handcrafted tales, but they were almost **always on rails**. You follow the path, maybe choose a branch here or there, but the destination and its meaning are already set for you. **We wanted to break free from that. We imagined a game where the player's journey could give a story its soul.**\r\n\r\n> Think of it like a single word. On its own, a word is just a word. But when you place it in a sentence, surrounded by a story you've built yourself, its meaning transforms. That’s what we want you to feel. By interacting with the world and its AI-driven characters, you build your own unique path, your own personal context. So when you finally reach that shared ending, it feels different for you than it would for anyone else, because the journey that brought you there was yours and yours alone. It’s an incredibly personal experience, and for now, we're excited to be sharing it with our players in Chinese.\r\n\r\nPlay the game : StoryForge http://115.29.205.103/\r\n\r\n## Technical Details\r\n\r\nThe project is a full-stack application with a sophisticated architecture designed to handle real-time, AI-driven narrative generation.\r\n\r\n- **Architecture & Tech Stack:**\r\n  - **Frontend:** Built with **React** using **TypeScript**, which was chosen for its strong typing and structural benefits. It includes a separate visual component for rendering the story's dependency tree.\r\n  - **Backend:** The core of the backend is a **dependency tree** that manages narrative logic and prerequisites for events. It uses **JSON** and **YML** files for configuration and data exchange.\r\n  - **Communication Protocol:** The project uses **WebSockets** for real-time, bidirectional communication between the client and server. This was chosen over a standard HTTP API because its streaming nature perfectly matches the dynamic, continuous flow of the storytelling engine.\r\n  - **AI Integration:** The system uses **OpenRouter** to access large language models like **Gemini**. A critical backend component is a custom **prompt engine** designed to force the AI to generate responses in a structured **JSON** format that the dependency tree can parse.\r\n  - **Deployment:** The application is containerized using **Docker** and deployed on **Chinese hosting services** to cater to the target market and avoid issues with the Great Firewall of China.\r\n- **Key Technical Challenges and Learnings:**\r\n  - **Session Management:** Managing **session IDs** in a WebSocket environment was a major challenge. It is crucial for separating the data of different users (and even different game instances from the same user) to prevent narrative overlap. Improper handling could lead to bugs and security vulnerabilities.\r\n  - **AI as a Bottleneck:** The speed of the third-party AI API was the primary performance bottleneck. This was mitigated by implementing **lazy loading** and **streaming** responses to the frontend.\r\n  - **Prompt Engineering:** The team learned that effective **prompt engineering** is a vital and often underestimated skill. Crafting precise prompts was necessary to ensure the AI returned reliable, correctly formatted JSON data for the dependency tree.\r\n  - **Development Strategy:** The team adopted a **decoupling** strategy, building and testing complex features (like the tree renderer) in an isolated environment before integrating them into the main project. They also learned the value of setting up a simple frontend-backend connection test before starting full development.\r\n  - **State and Code Management:** Proper frontend architecture was essential. This included carefully indexing components, separating CSS and TSX files, implementing robust **error handlers** for key functions, and managing distinct application states (e.g., `game_start`, `narrative_output`, `special_choice`).\r\n\r\n## Architecture\r\n\r\n### Backend Components\r\n```\r\nStoryForge/\r\n├── engine.py              # Core game engine\r\n├── socket_server.py       # WebSocket server\r\n├── agents/                # AI agent implementations\r\n│   ├── arbiter.py         # Decision validation\r\n│   ├── narrator.py        # Story generation\r\n│   └── reality_agent.py   # Reality reconstruction\r\n├── models/                # Data models\r\n│   ├── story.py           # Story structures\r\n│   ├── context.py         # Game context\r\n│   └── agents.py          # Agent interfaces\r\n├── utils/                 # Utilities\r\n│   ├── llm_service.py     # LLM integration\r\n│   ├── save_manager.py    # Save system\r\n│   └── command_handler.py # Command processing\r\n└── data/story/            # Story definitions\r\n    └── [story-name]/\r\n        ├── world_definition.yaml\r\n        ├── condition_tree.yaml\r\n        └── images/\r\n```\r\n\r\n### Frontend Components\r\n```\r\nfrontend/\r\n├── src/\r\n│   ├── components/        # React components\r\n│   │   ├── Stage.tsx      # Main game display\r\n│   │   ├── Console.tsx    # Command interface\r\n│   │   ├── EventTree.tsx  # Decision history\r\n│   │   └── UserActions.tsx # Action buttons\r\n│   ├── hooks/             # Custom React hooks\r\n│   ├── services/          # API services\r\n│   └── types/             # TypeScript definitions\r\n├── public/                # Static assets\r\n└── dist/                  # Built application\r\n```\r\n\r\n### Data Flow\r\n```\r\nPlayer Input → Socket.IO → Engine → AI Agents → Story Generation → Frontend Update\r\n     ↑                                    ↓\r\nSave System ← Delta Storage ← Game State ← Reality Validation\r\n```\r\n\r\n## Configuration\r\n\r\n### Story Creation\r\nCreate new stories by adding directories to `data/story/`:\r\n\r\n**world_definition.yaml**\r\n```yaml\r\nworld_definition:\r\n  story_name: \"Your Story Title\"\r\n  core_concept: \"Central theme or question\"\r\n  ontological_layers:\r\n    surface_narrative:\r\n      official_story: \"What appears to happen\"\r\n      rules: [\"Surface world rules\"]\r\n    underlying_structure:\r\n      hidden_truth: \"Deeper reality\"\r\n      rules: [\"Hidden mechanics\"]\r\n  protagonist_intent: \"Player's initial goal\"\r\n  endings:\r\n    good_ending: \"Positive resolution\"\r\n    bad_ending: \"Negative outcome\"\r\n```\r\n\r\n**condition_tree.yaml**\r\n\r\n```yaml\r\ntree_id: \"your_story\"\r\nroot_conditions: [\"start\"]\r\nterminal_conditions: [\"ending\"]\r\nnodes:\r\n  - node_id: \"start\"\r\n    symbolic_goal: \"Begin the adventure\"\r\n    default_interpretation: \"You find yourself...\"\r\n    dependencies: []\r\n```","filePath":"src/content/projects/story-forge/Story Forge An AI chatting game that allows the user to break the linear storyline.md","digest":"29aab2acca0992cb","rendered":{"html":"<p><em>(sorry this is not an open source project)</em></p>\n<h2 id=\"my-feelings-to-this-project\">My feelings to this project</h2>\n<p>This is a project that, when I first heard its idea,  a thought just hits me: Fuck everything and I’m gonna make this project happen. It felt like the project I was always meant to work on—one that connected a deep fascination with philosophy to the art of creating for the web. During that hackathon, our team built it within 3 days and got the first price in software development track.</p>\n<h2 id=\"project-overview\">Project Overview</h2>\n<blockquote>\n<p>A revolutionary text adventure game engine that breaks the boundaries between player agency and narrative coherence</p>\n</blockquote>\n<p>At its heart, Story Forge was born from a simple but powerful idea: what if a story wasn’t just something you followed, but something you helped create meaning for? We looked at traditional games and saw beautiful, handcrafted tales, but they were almost <strong>always on rails</strong>. You follow the path, maybe choose a branch here or there, but the destination and its meaning are already set for you. <strong>We wanted to break free from that. We imagined a game where the player’s journey could give a story its soul.</strong></p>\n<blockquote>\n<p>Think of it like a single word. On its own, a word is just a word. But when you place it in a sentence, surrounded by a story you’ve built yourself, its meaning transforms. That’s what we want you to feel. By interacting with the world and its AI-driven characters, you build your own unique path, your own personal context. So when you finally reach that shared ending, it feels different for you than it would for anyone else, because the journey that brought you there was yours and yours alone. It’s an incredibly personal experience, and for now, we’re excited to be sharing it with our players in Chinese.</p>\n</blockquote>\n<p>Play the game : StoryForge <a href=\"http://115.29.205.103/\">http://115.29.205.103/</a></p>\n<h2 id=\"technical-details\">Technical Details</h2>\n<p>The project is a full-stack application with a sophisticated architecture designed to handle real-time, AI-driven narrative generation.</p>\n<ul>\n<li><strong>Architecture &#x26; Tech Stack:</strong>\n<ul>\n<li><strong>Frontend:</strong> Built with <strong>React</strong> using <strong>TypeScript</strong>, which was chosen for its strong typing and structural benefits. It includes a separate visual component for rendering the story’s dependency tree.</li>\n<li><strong>Backend:</strong> The core of the backend is a <strong>dependency tree</strong> that manages narrative logic and prerequisites for events. It uses <strong>JSON</strong> and <strong>YML</strong> files for configuration and data exchange.</li>\n<li><strong>Communication Protocol:</strong> The project uses <strong>WebSockets</strong> for real-time, bidirectional communication between the client and server. This was chosen over a standard HTTP API because its streaming nature perfectly matches the dynamic, continuous flow of the storytelling engine.</li>\n<li><strong>AI Integration:</strong> The system uses <strong>OpenRouter</strong> to access large language models like <strong>Gemini</strong>. A critical backend component is a custom <strong>prompt engine</strong> designed to force the AI to generate responses in a structured <strong>JSON</strong> format that the dependency tree can parse.</li>\n<li><strong>Deployment:</strong> The application is containerized using <strong>Docker</strong> and deployed on <strong>Chinese hosting services</strong> to cater to the target market and avoid issues with the Great Firewall of China.</li>\n</ul>\n</li>\n<li><strong>Key Technical Challenges and Learnings:</strong>\n<ul>\n<li><strong>Session Management:</strong> Managing <strong>session IDs</strong> in a WebSocket environment was a major challenge. It is crucial for separating the data of different users (and even different game instances from the same user) to prevent narrative overlap. Improper handling could lead to bugs and security vulnerabilities.</li>\n<li><strong>AI as a Bottleneck:</strong> The speed of the third-party AI API was the primary performance bottleneck. This was mitigated by implementing <strong>lazy loading</strong> and <strong>streaming</strong> responses to the frontend.</li>\n<li><strong>Prompt Engineering:</strong> The team learned that effective <strong>prompt engineering</strong> is a vital and often underestimated skill. Crafting precise prompts was necessary to ensure the AI returned reliable, correctly formatted JSON data for the dependency tree.</li>\n<li><strong>Development Strategy:</strong> The team adopted a <strong>decoupling</strong> strategy, building and testing complex features (like the tree renderer) in an isolated environment before integrating them into the main project. They also learned the value of setting up a simple frontend-backend connection test before starting full development.</li>\n<li><strong>State and Code Management:</strong> Proper frontend architecture was essential. This included carefully indexing components, separating CSS and TSX files, implementing robust <strong>error handlers</strong> for key functions, and managing distinct application states (e.g., <code>game_start</code>, <code>narrative_output</code>, <code>special_choice</code>).</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"architecture\">Architecture</h2>\n<h3 id=\"backend-components\">Backend Components</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>StoryForge/</span></span>\n<span class=\"line\"><span>├── engine.py              # Core game engine</span></span>\n<span class=\"line\"><span>├── socket_server.py       # WebSocket server</span></span>\n<span class=\"line\"><span>├── agents/                # AI agent implementations</span></span>\n<span class=\"line\"><span>│   ├── arbiter.py         # Decision validation</span></span>\n<span class=\"line\"><span>│   ├── narrator.py        # Story generation</span></span>\n<span class=\"line\"><span>│   └── reality_agent.py   # Reality reconstruction</span></span>\n<span class=\"line\"><span>├── models/                # Data models</span></span>\n<span class=\"line\"><span>│   ├── story.py           # Story structures</span></span>\n<span class=\"line\"><span>│   ├── context.py         # Game context</span></span>\n<span class=\"line\"><span>│   └── agents.py          # Agent interfaces</span></span>\n<span class=\"line\"><span>├── utils/                 # Utilities</span></span>\n<span class=\"line\"><span>│   ├── llm_service.py     # LLM integration</span></span>\n<span class=\"line\"><span>│   ├── save_manager.py    # Save system</span></span>\n<span class=\"line\"><span>│   └── command_handler.py # Command processing</span></span>\n<span class=\"line\"><span>└── data/story/            # Story definitions</span></span>\n<span class=\"line\"><span>    └── [story-name]/</span></span>\n<span class=\"line\"><span>        ├── world_definition.yaml</span></span>\n<span class=\"line\"><span>        ├── condition_tree.yaml</span></span>\n<span class=\"line\"><span>        └── images/</span></span></code></pre>\n<h3 id=\"frontend-components\">Frontend Components</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>frontend/</span></span>\n<span class=\"line\"><span>├── src/</span></span>\n<span class=\"line\"><span>│   ├── components/        # React components</span></span>\n<span class=\"line\"><span>│   │   ├── Stage.tsx      # Main game display</span></span>\n<span class=\"line\"><span>│   │   ├── Console.tsx    # Command interface</span></span>\n<span class=\"line\"><span>│   │   ├── EventTree.tsx  # Decision history</span></span>\n<span class=\"line\"><span>│   │   └── UserActions.tsx # Action buttons</span></span>\n<span class=\"line\"><span>│   ├── hooks/             # Custom React hooks</span></span>\n<span class=\"line\"><span>│   ├── services/          # API services</span></span>\n<span class=\"line\"><span>│   └── types/             # TypeScript definitions</span></span>\n<span class=\"line\"><span>├── public/                # Static assets</span></span>\n<span class=\"line\"><span>└── dist/                  # Built application</span></span></code></pre>\n<h3 id=\"data-flow\">Data Flow</h3>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>Player Input → Socket.IO → Engine → AI Agents → Story Generation → Frontend Update</span></span>\n<span class=\"line\"><span>     ↑                                    ↓</span></span>\n<span class=\"line\"><span>Save System ← Delta Storage ← Game State ← Reality Validation</span></span></code></pre>\n<h2 id=\"configuration\">Configuration</h2>\n<h3 id=\"story-creation\">Story Creation</h3>\n<p>Create new stories by adding directories to <code>data/story/</code>:</p>\n<p><strong>world_definition.yaml</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"yaml\"><code><span class=\"line\"><span style=\"color:#85E89D\">world_definition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  story_name</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Your Story Title\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  core_concept</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Central theme or question\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  ontological_layers</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    surface_narrative</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      official_story</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"What appears to happen\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      rules</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"Surface world rules\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    underlying_structure</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      hidden_truth</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Deeper reality\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      rules</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"Hidden mechanics\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  protagonist_intent</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Player's initial goal\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  endings</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    good_ending</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Positive resolution\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    bad_ending</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Negative outcome\"</span></span></code></pre>\n<p><strong>condition_tree.yaml</strong></p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"yaml\"><code><span class=\"line\"><span style=\"color:#85E89D\">tree_id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"your_story\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">root_conditions</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"start\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">terminal_conditions</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"ending\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">nodes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">node_id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"start\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    symbolic_goal</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Begin the adventure\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    default_interpretation</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"You find yourself...\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    dependencies</span><span style=\"color:#E1E4E8\">: []</span></span></code></pre>","metadata":{"headings":[{"depth":2,"slug":"my-feelings-to-this-project","text":"My feelings to this project"},{"depth":2,"slug":"project-overview","text":"Project Overview"},{"depth":2,"slug":"technical-details","text":"Technical Details"},{"depth":2,"slug":"architecture","text":"Architecture"},{"depth":3,"slug":"backend-components","text":"Backend Components"},{"depth":3,"slug":"frontend-components","text":"Frontend Components"},{"depth":3,"slug":"data-flow","text":"Data Flow"},{"depth":2,"slug":"configuration","text":"Configuration"},{"depth":3,"slug":"story-creation","text":"Story Creation"}],"localImagePaths":[],"remoteImagePaths":[],"frontmatter":{"title":"Story Forge: AI-Powered Nonlinear Narrative Game","description":"A full-stack AI-driven text adventure game engine that lets players break the linear storyline and co-create unique stories.","pubDate":"2025-07-31T00:00:00.000Z","githubLink":"http://115.29.205.103/","tags":["AI","Game Engine","React","TypeScript","WebSockets","Prompt Engineering","Full Stack"]},"imagePaths":[]}},"collection":"projects","slug":"story-forge/story-forge-an-ai-chatting-game-that-allows-the-user-to-break-the-linear-storyline"}];
const getUrl = undefined;

  // Dynamic import for MiniSearch to handle browser compatibility
  async function loadMiniSearch() {
    try {
      const { default: MiniSearch } = await import('minisearch');
      return MiniSearch;
    } catch (error) {
      return null;
    }
  }
  
  class SearchManager {
    constructor() {
      this.miniSearch = null;
      this.searchInput = document.getElementById('search-input');
      this.searchIcon = document.getElementById('search-icon');
      this.searchResults = document.getElementById('search-results');
      this.resultsList = document.getElementById('results-list');
      this.resultsCount = document.getElementById('results-count');
      this.clearBtn = document.getElementById('clear-search');
      
      this.init();
    }
    
    async init() {
      const MiniSearch = await loadMiniSearch();
      if (!MiniSearch) {
        this.setupFallbackSearch();
        return;
      }
      
      this.miniSearch = new MiniSearch({
        fields: ['title', 'description', 'tags', 'content'],
        storeFields: ['title', 'description', 'pubDate', 'type', 'slug', 'tags'],
        searchOptions: {
          boost: { title: 2, description: 1.5 },
          fuzzy: 0.2,
          prefix: true
        }
      });
      
      this.initializeData();
      this.bindEvents();
    }
    
    setupFallbackSearch() {
      // Fallback search without MiniSearch
      this.documents = this.prepareDocuments();
      this.bindEvents();
    }
    
    prepareDocuments() {
      const documents = [];
      
      // Add blog posts
      allBlogPosts.forEach(post => {
        documents.push({
          id: `blog-${post.slug}`,
          title: post.data.title,
          description: post.data.description,
          pubDate: post.data.pubDate,
          type: 'blog',
          slug: post.slug,
          tags: (post.data.tags || []).join(' '),
          content: `${post.data.title} ${post.data.description} ${(post.data.tags || []).join(' ')}`
        });
      });
      
      // Add projects
      allProjects.forEach(project => {
        documents.push({
          id: `project-${project.slug}`,
          title: project.data.title,
          description: project.data.description,
          pubDate: project.data.pubDate,
          type: 'project',
          slug: project.slug,
          tags: (project.data.tags || []).join(' '),
          content: `${project.data.title} ${project.data.description} ${(project.data.tags || []).join(' ')}`
        });
      });
      
      return documents;
    }
    
    initializeData() {
      const documents = this.prepareDocuments();
      this.miniSearch.addAll(documents);
    }
    
    bindEvents() {
      // Real-time search on input
      this.searchInput.addEventListener('input', this.handleRealtimeSearch.bind(this));
      
      // Search on Enter key press
      this.searchInput.addEventListener('keydown', (e) => {
        if (e.key === 'Enter') {
          e.preventDefault();
          this.performSearch();
        } else if (e.key === 'Escape') {
          this.clearSearch();
        }
      });
      
      // Search on icon button click
      this.searchIcon.addEventListener('click', this.performSearch.bind(this));
      
      // Clear search
      this.clearBtn.addEventListener('click', this.clearSearch.bind(this));
      
      // Close results when clicking outside
      document.addEventListener('click', (e) => {
        if (!e.target.closest('.search-container')) {
          this.hideResults();
        }
      });
      
      // Reposition results on window resize
      window.addEventListener('resize', () => {
        if (!this.searchResults.classList.contains('hidden')) {
          this.positionResults();
        }
      });
      
      // Reposition results on scroll
      window.addEventListener('scroll', () => {
        if (!this.searchResults.classList.contains('hidden')) {
          this.positionResults();
        }
      });
    }
    
    handleRealtimeSearch(e) {
      const query = e.target.value.trim();
      
      // Only show real-time results for queries of 3+ characters
      if (query.length >= 3) {
        const results = this.search(query, 8);
        this.displayResults(results, query);
      } else {
        this.hideResults();
      }
    }
    
    performSearch() {
      const query = this.searchInput.value.trim();
      
      if (query.length < 2) {
        // Show message for short queries
        this.displayNoResults('Please enter at least 2 characters to search');
        return;
      }
      
      const results = this.search(query, 8);
      this.displayResults(results, query);
    }
    
    search(query, limit = 8) {
      if (this.miniSearch) {
        return this.miniSearch.search(query, { limit });
      } else {
        // Fallback search
        return this.fallbackSearch(query, limit);
      }
    }
    
    fallbackSearch(query, limit) {
      const queryLower = query.toLowerCase();
      const results = [];
      
      for (const doc of this.documents) {
        let score = 0;
        const titleMatch = doc.title.toLowerCase().includes(queryLower);
        const descMatch = doc.description.toLowerCase().includes(queryLower);
        const tagsMatch = doc.tags.toLowerCase().includes(queryLower);
        
        if (titleMatch) score += 2;
        if (descMatch) score += 1.5;
        if (tagsMatch) score += 1;
        
        if (score > 0) {
          results.push({ ...doc, score });
        }
      }
      
      return results
        .sort((a, b) => b.score - a.score)
        .slice(0, limit);
    }
    
    displayResults(results, query) {
      this.resultsCount.textContent = `${results.length} result${results.length !== 1 ? 's' : ''}`;
      
      if (results.length === 0) {
        this.displayNoResults(`No results found for "${query}"`);
      } else {
        this.resultsList.innerHTML = results.map(result => {
          const date = new Date(result.pubDate).toLocaleDateString('en-US', {
            year: 'numeric',
            month: 'short'
          });
          
          const url = result.type === 'blog' 
            ? `/blog/${result.slug}/`
            : `/projects/${result.slug}/`;
          
          return `
            <a href="${url}" class="result-item">
              <div class="result-title">${this.highlightText(result.title, query)}</div>
              <div class="result-description">${this.highlightText(result.description, query)}</div>
              <div class="result-meta">
                <span class="result-type">${result.type}</span>
                <span class="result-date">📅 ${date}</span>
              </div>
            </a>
          `;
        }).join('');
      }
      
      this.showResults();
    }
    
    displayNoResults(message) {
      this.resultsCount.textContent = '0 results';
      this.resultsList.innerHTML = `
        <div class="no-results">
          <div class="no-results-icon">🔍</div>
          <div class="no-results-message">${message}</div>
          <div class="no-results-hint">Try using different keywords or check your spelling</div>
        </div>
      `;
      this.showResults();
    }
    
    highlightText(text, query) {
      if (!query || !text) return text;
      
      const regex = new RegExp(`(${query.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')})`, 'gi');
      return text.replace(regex, '<mark>$1</mark>');
    }
    
    showResults() {
      this.searchResults.classList.remove('hidden');
      this.positionResults();
    }
    
    positionResults() {
      const searchContainer = this.searchInput.closest('.search-container');
      const rect = searchContainer.getBoundingClientRect();
      const viewport = {
        height: window.innerHeight,
        width: window.innerWidth
      };
      
      // Calculate available space below and above
      const spaceBelow = viewport.height - rect.bottom;
      const spaceAbove = rect.top;
      
      // Position the results dropdown
      if (spaceBelow >= 300 || spaceBelow >= spaceAbove) {
        // Show below if there's enough space or more space below than above
        this.searchResults.style.top = `${rect.bottom + 4}px`;
        this.searchResults.style.maxHeight = `${Math.min(300, spaceBelow - 20)}px`;
      } else {
        // Show above if more space above
        this.searchResults.style.top = `${rect.top - Math.min(300, spaceAbove - 20) - 4}px`;
        this.searchResults.style.maxHeight = `${Math.min(300, spaceAbove - 20)}px`;
      }
      
      this.searchResults.style.left = `${rect.left}px`;
      this.searchResults.style.width = `${rect.width}px`;
    }
    
    hideResults() {
      this.searchResults.classList.add('hidden');
    }
    
    clearSearch() {
      this.searchInput.value = '';
      this.hideResults();
      this.searchInput.focus();
    }
  }
  
  // Initialize search when DOM is loaded
  document.addEventListener('DOMContentLoaded', () => {
    new SearchManager();
  });
</script>  </div> <div class="sidebar-section" data-astro-cid-5tznm7mj> <h3 data-astro-cid-5tznm7mj>Categories</h3> <div class="tag-cloud" data-astro-cid-5tznm7mj> <a href="/blog/tag/triton/" class="tag-filter" data-astro-cid-5tznm7mj>triton</a><a href="/blog/tag/GPU/" class="tag-filter" data-astro-cid-5tznm7mj>GPU</a><a href="/blog/tag/CUDA/" class="tag-filter" data-astro-cid-5tznm7mj>CUDA</a><a href="/blog/tag/compiler/" class="tag-filter" data-astro-cid-5tznm7mj>compiler</a><a href="/blog/tag/code/" class="tag-filter" data-astro-cid-5tznm7mj>code</a><a href="/blog/tag/optimization/" class="tag-filter" data-astro-cid-5tznm7mj>optimization</a> </div> </div> </div> <div class="blog-posts" data-astro-cid-5tznm7mj> <article class="blog-post-card" data-astro-cid-5tznm7mj> <header data-astro-cid-5tznm7mj> <h2 data-astro-cid-5tznm7mj> <a href="/blog/matrix-multiplication/matrix_multiplication/" data-astro-cid-5tznm7mj>Intro to Triton with Matrix Multiplication</a> </h2> <div class="post-meta" data-astro-cid-5tznm7mj> <time datetime="2025-03-18T00:00:00.000Z" data-astro-cid-5tznm7mj> March 17, 2025 </time> <span class="author" data-astro-cid-5tznm7mj>by Xiaoyou Wu</span> </div> </header> <p class="post-description" data-astro-cid-5tznm7mj>Introduction to GPU programming with Triton and build the matrix multiplication along the way</p> <div class="post-tags" data-astro-cid-5tznm7mj> <a href="/blog/tag/triton/" class="tag" data-astro-cid-5tznm7mj>triton</a><a href="/blog/tag/GPU/" class="tag" data-astro-cid-5tznm7mj>GPU</a><a href="/blog/tag/CUDA/" class="tag" data-astro-cid-5tznm7mj>CUDA</a><a href="/blog/tag/compiler/" class="tag" data-astro-cid-5tznm7mj>compiler</a><a href="/blog/tag/code/" class="tag" data-astro-cid-5tznm7mj>code</a><a href="/blog/tag/optimization/" class="tag" data-astro-cid-5tznm7mj>optimization</a> </div> <a href="/blog/matrix-multiplication/matrix_multiplication/" class="read-more" data-astro-cid-5tznm7mj>
Read more →
</a> </article> </div> </div> </section>  </main>  </main> <footer class="site-footer" data-astro-cid-37fxchfa> <div class="footer-content" data-astro-cid-37fxchfa> <p data-astro-cid-37fxchfa>&copy; 2025 Xiaoyou Wu. All rights reserved.</p> <div class="footer-links" data-astro-cid-37fxchfa> <a href="https://github.com/yourusername" target="_blank" rel="noopener noreferrer" data-astro-cid-37fxchfa>GitHub</a> <a href="https://linkedin.com/in/yourprofile" target="_blank" rel="noopener noreferrer" data-astro-cid-37fxchfa>LinkedIn</a> <a href="/resume.pdf" target="_blank" rel="noopener noreferrer" data-astro-cid-37fxchfa>Resume</a> </div> </div> </footer> <!-- KaTeX JavaScript for Math Rendering --> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInDocument()"></script> <script>
    function renderMathInDocument() {
      if (typeof window.renderMathInElement !== 'undefined') {
        window.renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError: false
        });
      }
    }
    
    document.addEventListener("DOMContentLoaded", renderMathInDocument);
  </script> <!-- Simple Mermaid Test Script --> <script type="module">
    console.log('[Simple Mermaid Test] Starting...');

    // Simple function to load and initialize Mermaid
    async function loadAndTestMermaid() {
      try {
        console.log('[Simple Mermaid Test] Loading Mermaid from CDN...');
        
        // Load Mermaid if not already loaded
        if (!window.mermaid) {
          const script = document.createElement('script');
          script.src = 'https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js';
          
          await new Promise((resolve, reject) => {
            script.onload = resolve;
            script.onerror = reject;
            document.head.appendChild(script);
          });
          
          console.log('[Simple Mermaid Test] Mermaid loaded from CDN');
        } else {
          console.log('[Simple Mermaid Test] Mermaid already available');
        }

        if (!window.mermaid) {
          throw new Error('Mermaid not available after loading');
        }

        console.log('[Simple Mermaid Test] Initializing Mermaid...');
        
        // Initialize Mermaid with simple config
        window.mermaid.initialize({
          startOnLoad: false,
          theme: 'neutral'
        });

        console.log('[Simple Mermaid Test] Finding diagram elements...');
        
        // Find all mermaid diagrams
        const diagrams = document.querySelectorAll('.mermaid-diagram');
        console.log(`[Simple Mermaid Test] Found ${diagrams.length} diagrams`);

        // Process each diagram
        for (let i = 0; i < diagrams.length; i++) {
          const diagram = diagrams[i];
          const id = diagram.id || `test-diagram-${i}`;
          const codeElement = diagram.querySelector('.mermaid-code code');
          
          console.log(`[Simple Mermaid Test] Processing diagram ${id}`);
          
          if (codeElement) {
            const code = codeElement.textContent.trim();
            console.log(`[Simple Mermaid Test] Code for ${id}:`, code.substring(0, 100) + '...');
            
            try {
              // Render the diagram
              const { svg } = await window.mermaid.render(`${id}-render`, code);
              console.log(`[Simple Mermaid Test] SVG generated for ${id}`);
              
              // Replace the loading text with the SVG
              const container = diagram.querySelector('.mermaid-container');
              if (container) {
                container.innerHTML = svg;
                console.log(`[Simple Mermaid Test] SVG inserted for ${id}`);
              } else {
                console.error(`[Simple Mermaid Test] No container found for ${id}`);
              }
              
            } catch (renderError) {
              console.error(`[Simple Mermaid Test] Render error for ${id}:`, renderError);
              
              const container = diagram.querySelector('.mermaid-container');
              if (container) {
                container.innerHTML = `<div style="color: red; padding: 1rem;">Error: ${renderError.message}</div>`;
              }
            }
          } else {
            console.error(`[Simple Mermaid Test] No code element found for ${id}`);
          }
        }

        console.log('[Simple Mermaid Test] Completed processing all diagrams');
        
      } catch (error) {
        console.error('[Simple Mermaid Test] Fatal error:', error);
      }
    }

    // Run the test when DOM is ready
    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', loadAndTestMermaid);
    } else {
      loadAndTestMermaid();
    }

    // Export for manual testing
    window.testMermaid = loadAndTestMermaid;
  </script> </body> </html>   