<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Intro to Triton with Matrix Multiplication</title><meta name="description" content="Introduction to GPU programming with Triton and build the matrix multiplication along the way"><!-- Open Graph --><meta property="og:type" content="website"><meta property="og:url" content="https://laurence-wu.github.io/blog/matrix-multiplication/"><meta property="og:title" content="Intro to Triton with Matrix Multiplication"><meta property="og:description" content="Introduction to GPU programming with Triton and build the matrix multiplication along the way"><meta property="og:image" content="./images/grouped_vs_row_major_ordering.png"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:title" content="Intro to Triton with Matrix Multiplication"><meta property="twitter:description" content="Introduction to GPU programming with Triton and build the matrix multiplication along the way"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- KaTeX CSS for Math Rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous"><!-- Book Theme CSS --><link rel="stylesheet" href="/src/styles/book.css"><style>.site-nav[data-astro-cid-pux6a34n]{padding:2.5rem 0;border-bottom:1px solid var(--border-subtle);background-color:var(--bg-paper)}.nav-container[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;align-items:center;max-width:var(--max-width-layout);margin:0 auto;padding:0 1.5rem}.site-title[data-astro-cid-pux6a34n]{font-family:var(--font-sans);font-weight:700;font-size:1.1rem;color:var(--text-primary);text-decoration:none;letter-spacing:-.02em;border-bottom:none}.site-title[data-astro-cid-pux6a34n]:hover{color:var(--text-primary);border-bottom:none}.nav-links[data-astro-cid-pux6a34n]{display:flex;gap:2rem}.nav-link[data-astro-cid-pux6a34n]{color:var(--text-secondary);font-family:var(--font-sans);font-size:.95rem;text-decoration:none;transition:color .2s;border-bottom:none}.nav-link[data-astro-cid-pux6a34n]:hover,.nav-link[data-astro-cid-pux6a34n].active{color:var(--accent)}:root{--bg-paper: #fdfbf7;--bg-card: #ffffff;--text-primary: #2c2c2c;--text-secondary: #595959;--accent: #2e59a7;--accent-hover: #1a3c7a;--border-subtle: #e6e6e6;--font-serif: "Georgia", "Times New Roman", serif;--font-sans: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;--font-mono: "Menlo", "Consolas", "Monaco", monospace;--line-height-body: 1.8;--line-height-heading: 1.3;--spacing-container: 1.5rem;--max-width-text: 68ch;--max-width-layout: 800px}*,*:before,*:after{box-sizing:border-box;margin:0;padding:0}body{background-color:var(--bg-paper);color:var(--text-primary);font-family:var(--font-serif);line-height:var(--line-height-body);font-size:18px;-webkit-font-smoothing:antialiased}h1,h2,h3,h4,h5,h6{font-family:var(--font-sans);font-weight:600;line-height:var(--line-height-heading);color:#1a1a1a;margin-top:2.5rem;margin-bottom:1rem}h1{font-size:2.5rem;letter-spacing:-.5px}h2{font-size:1.8rem;border-bottom:1px solid var(--border-subtle);padding-bottom:.5rem}h3{font-size:1.4rem}p{margin-bottom:1.5rem}a{color:var(--accent);text-decoration:none;border-bottom:1px solid transparent;transition:border-color .2s}a:hover{color:var(--accent-hover);border-bottom-color:var(--accent-hover)}.container{max-width:var(--max-width-layout);margin:0 auto;padding:0 var(--spacing-container)}.main-content{padding:4rem 0}pre{background:#f5f5f5;padding:1rem;border-radius:4px;overflow-x:auto;font-family:var(--font-mono);font-size:.9em;margin:1.5rem 0;border:1px solid var(--border-subtle)}code{font-family:var(--font-mono);background:#0000000d;padding:.2em .4em;border-radius:3px;font-size:.85em}pre code{background:none;padding:0}img{max-width:100%;height:auto;display:block;margin:2rem auto;border-radius:4px}blockquote{border-left:3px solid var(--accent);margin:2rem 0;padding-left:1.5rem;font-style:italic;color:var(--text-secondary)}.site-nav{padding:2rem 0;border-bottom:1px solid var(--border-subtle);margin-bottom:2rem}.nav-container{display:flex;justify-content:space-between;align-items:center;max-width:var(--max-width-layout);margin:0 auto;padding:0 var(--spacing-container)}.site-title{font-family:var(--font-sans);font-weight:700;font-size:1.2rem;color:var(--text-primary);text-decoration:none}.nav-links{display:flex;gap:1.5rem}.nav-link{color:var(--text-secondary);font-family:var(--font-sans);font-size:.95rem}.nav-link:hover,.nav-link.active{color:var(--accent)}.site-footer{margin-top:4rem;padding:3rem 0;border-top:1px solid var(--border-subtle);text-align:center;font-size:.9rem;color:var(--text-secondary)}
.post-header[data-astro-cid-bvzihdzo]{text-align:center;margin-bottom:3rem;padding-bottom:2rem;border-bottom:1px solid var(--border-subtle)}.post-header[data-astro-cid-bvzihdzo] h1[data-astro-cid-bvzihdzo]{font-size:2.5rem;margin-bottom:.5rem;color:var(--text-primary)}.post-meta[data-astro-cid-bvzihdzo]{color:var(--text-secondary);font-size:.95rem;font-family:var(--font-sans)}.tags[data-astro-cid-bvzihdzo]{margin-top:1rem;display:flex;justify-content:center;gap:.8rem}.tag[data-astro-cid-bvzihdzo]{color:var(--accent);font-size:.9rem;font-family:var(--font-sans);text-decoration:none}.tag[data-astro-cid-bvzihdzo]:hover{text-decoration:underline}.post-content[data-astro-cid-bvzihdzo]{max-width:var(--max-width-text);margin:0 auto;font-size:1.1rem}.post-hero-image[data-astro-cid-bvzihdzo] img[data-astro-cid-bvzihdzo]{max-height:400px;-o-object-fit:cover;object-fit:cover;width:100%;margin-bottom:3rem}.post-footer[data-astro-cid-bvzihdzo]{margin-top:4rem;padding-top:2rem;border-top:1px solid var(--border-subtle);text-align:center}.back-link[data-astro-cid-bvzihdzo]{font-family:var(--font-sans);color:var(--text-secondary)}.back-link[data-astro-cid-bvzihdzo]:hover{color:var(--accent)}
</style></head> <body> <nav class="site-nav" data-astro-cid-pux6a34n> <div class="nav-container" data-astro-cid-pux6a34n> <a href="/" class="site-title" data-astro-cid-pux6a34n>Xiaoyou Wu</a> <div class="nav-links" data-astro-cid-pux6a34n> <a href="/blog" class="nav-link active" data-astro-cid-pux6a34n>
Writing
</a> <a href="/projects" class="nav-link " data-astro-cid-pux6a34n>
Projects
</a> <a href="/about" class="nav-link " data-astro-cid-pux6a34n>
About
</a> </div> </div> </nav>  <main class="main-content">  <div class="container" data-astro-cid-bvzihdzo> <article class="blog-post" data-astro-cid-bvzihdzo> <header class="post-header" data-astro-cid-bvzihdzo> <h1 data-astro-cid-bvzihdzo>Intro to Triton with Matrix Multiplication</h1> <div class="post-meta" data-astro-cid-bvzihdzo> <time datetime="2025-03-18T00:00:00.000Z" data-astro-cid-bvzihdzo> March 18, 2025 </time> <span class="author" data-astro-cid-bvzihdzo> ‚Ä¢ Xiaoyou Wu</span>  </div> <div class="tags" data-astro-cid-bvzihdzo> <a href="/blog/tag/triton/" class="tag" data-astro-cid-bvzihdzo>#triton</a><a href="/blog/tag/GPU/" class="tag" data-astro-cid-bvzihdzo>#GPU</a><a href="/blog/tag/CUDA/" class="tag" data-astro-cid-bvzihdzo>#CUDA</a><a href="/blog/tag/compiler/" class="tag" data-astro-cid-bvzihdzo>#compiler</a><a href="/blog/tag/code/" class="tag" data-astro-cid-bvzihdzo>#code</a><a href="/blog/tag/optimization/" class="tag" data-astro-cid-bvzihdzo>#optimization</a> </div> </header>   <div class="post-content" data-astro-cid-bvzihdzo>  <p>As the most important operation in GPU computation, matrix multiplication optimization is a must learn!</p>
<p><strong>Bruh, let‚Äôs dive right in ~</strong></p>
<h2 id="from-the-architecture-to-the-idea">From the architecture to the idea</h2>
<h3 id="memory-architecture-of-gpu">Memory Architecture of GPU</h3>
<p>(research1)[<a href="https://stanfordasl.github.io/projects/">https://stanfordasl.github.io/projects/</a>]</p>
<figure class="mermaid-diagram" id="mermaid-206u3in4e">
  <div class="mermaid-container">
    Loading diagram...
  </div>
  <details class="mermaid-source">
    <summary>Source</summary>
    <pre class="mermaid-code"><code>graph TD
    subgraph "CPU / Host"
        A[Host Code] --> B{Kernel Launch}
    end

    subgraph "GPU / Device"
        C[Command Queue]
        D(Thread Blocks)
        E[Streaming Multiprocessors]
        F[L1 Cache / Shared Memory]
        G[CUDA Cores]
        H[L2 Cache]
        I[High-Bandwidth Memory]
    end

    %% --- Define Connections ---
    B -- Command --> C
    C --> D
    D --> E
    E -- Contains --> F
    E -- Contains --> G

    %% Memory Flow
    G &#x3C;--> F
    F &#x3C;--> H
    H &#x3C;--> I</code></pre>
  </details>
</figure>
<ol>
<li>Command from CPU: CPU will run a pointer to the compiled GPU function. <em>But how is it compiled, and what needs to be included? This will be discussed in the compilation part.</em></li>
<li>Then, the command will undergo an asynchronous operation using a physical memory buffer FIFO. Compile instructions will be loaded.</li>
<li>Here, the grid of blocks is simply a soft level abstraction. The information about the grid blocks will be compiled for each thread block, which is in the SMs (Streaming Multiprocessor). <strong><em><u>This is incredible, frontend and backend separation techniques, I guess it helps the hardware security.</u></em></strong></li>
<li><strong><u>VRAM</u></strong> is basically a more general external memory, and <strong><u>HBM</u></strong> is just a high-performance type.</li>
<li>Then, for the SM, it will manage warp, which manages 32 threads for executing and transporting information to other parts:</li>
</ol>
<figure class="mermaid-diagram" id="mermaid-e071s7ecj">
  <div class="mermaid-container">
    Loading diagram...
  </div>
  <details class="mermaid-source">
    <summary>Source</summary>
    <pre class="mermaid-code"><code>graph TD
    subgraph GPU Device
        subgraph Streaming Multiprocessor 1
            subgraph CUDA Core 1A
                R1[Registers]
            end
            subgraph CUDA Core 1B
                R2[Registers]
            end
            L1_1[L1 Cache / Shared Memory]
        end

        subgraph Streaming Multiprocessor 2
            subgraph CUDA Core 2A
                R3[Registers]
            end
            subgraph CUDA Core 2B
                R4[Registers]
            end
            L1_2[L1 Cache / Shared Memory]
        end

        L2[L2 Cache]
        HBM[High-Bandwidth Memory]
    end

    %% --- Data Access Paths ---
    R1 &#x3C;--> L1_1
    R2 &#x3C;--> L1_1
    R3 &#x3C;--> L1_2
    R4 &#x3C;--> L1_2
    L1_1 &#x3C;--> L2
    L1_2 &#x3C;--> L2
    L2 &#x3C;--> HBM

    style R1 fill:#f9f,stroke:#333,stroke-width:2px
    style R2 fill:#f9f,stroke:#333,stroke-width:2px
    style R3 fill:#f9f,stroke:#333,stroke-width:2px
    style R4 fill:#f9f,stroke:#333,stroke-width:2px
    style L1_1 fill:#bbf,stroke:#333,stroke-width:2px
    style L1_2 fill:#bbf,stroke:#333,stroke-width:2px
    style L2 fill:#bdf,stroke:#333,stroke-width:2px
    style HBM fill:#fb9,stroke:#333,stroke-width:2px</code></pre>
  </details>
</figure>
<p>AND there are their properties:</p>






























<table><thead><tr><th>Memory Hardware</th><th>Key Parameters</th><th>Primary Job</th></tr></thead><tbody><tr><td><strong>Registers</strong></td><td><strong>Size:</strong> Tiniest (bytes per thread)<br><strong>Speed:</strong> Instantaneous <strong>Scope:</strong> Private to <strong>one thread</strong></td><td>Holds the immediate working data for a single thread‚Äôs current instruction. The absolute fastest memory available.</td></tr><tr><td><strong>L1 Cache / Shared Memory</strong></td><td><strong>Size:</strong> Small (~100 KB per SM) <br><strong>Speed:</strong> Fastest <br><strong>Scope:</strong> Per SM</td><td>A high-speed scratchpad for threads within a <strong>block</strong> to share data and cooperate.</td></tr><tr><td><strong>L2 Cache</strong></td><td><strong>Size:</strong> Medium (a few MB) <br><strong>Speed:</strong> Fast<br><strong>Scope:</strong> Shared by <strong>all SMs</strong></td><td>A unified cache for the entire GPU, catching data requests that miss L1 to avoid slow HBM access.</td></tr><tr><td><strong>High-Bandwidth Memory (HBM) / VRAM</strong></td><td><strong>Size:</strong> Large (many GB) <strong>Speed:</strong> Slowest <strong>Scope:</strong> Global (entire GPU)</td><td>The main data storage for the GPU, holding all large datasets, textures, and models.</td></tr></tbody></table>
<h3 id="compilation-architecture">Compilation architecture</h3>
<figure class="mermaid-diagram" id="mermaid-6lqfjzqqb">
  <div class="mermaid-container">
    Loading diagram...
  </div>
  <details class="mermaid-source">
    <summary>Source</summary>
    <pre class="mermaid-code"><code>graph TD
    %% --- Node Definitions ---

    subgraph "Stage 1: Compilation"
        A["üìÑ Source File .cu\nContains both Host CPU and Device GPU code"]
        B["1. NVCC Compiler Driver"]
    end

    subgraph "Host CPU Path"
        C["Host Code C++"]
        D["Host Compiler\ne.g., GCC, Clang, MSVC"]
        E["Host Object File .o"]
    end

    subgraph "Device GPU Path"
        F["Device Code CUDA C++"]
        G["CUDA Frontend Compiler"]
        H["PTX\nParallel Thread Execution\n&#x3C;I>Intermediate Assembly&#x3C;/I>"]
        I["PTX Assembler"]
        J["SASS\nStreaming Assembler\n&#x3C;B>GPU Machine Code&#x3C;/B>"]
        K["Device Object File .o"]
    end

    subgraph "Stage 2: Linking"
        L["Linker"]
        M["‚úÖ Final Executable\nContains Host code + embedded GPU code"]
    end

    subgraph "Stage 3: Execution"
        N["Program runs on Host CPU"]
        O["CUDA Driver"]
        P{"&#x3C;B>JIT Compilation&#x3C;/B>\nJust-In-Time"}
        Q["üöÄ GPU Executes Code"]
    end

    %% --- Connection Definitions ---
    A --> B
    B -- Separates Code --> C &#x26; F
    C --> D --> E
    F --> G --> H --> I --> J --> K
    E --> L
    K --> L
    L --> M
    M --> N --> O --> P
    P -- "If embedded SASS doesn't match current GPU" --> I
    P -- "If SASS is compatible" --> Q

    %% --- Styling ---
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style L fill:#f9f,stroke:#333,stroke-width:2px
    style N fill:#f9f,stroke:#333,stroke-width:2px
    style O fill:#f9f,stroke:#333,stroke-width:2px</code></pre>
  </details>
</figure>
<ol>
<li>Command from CPU: The command will be compiled in the CPU. The kernel function should include the behavior of the Grid of Blocks and the number of threads per block.</li>
<li><strong><u>JIT (just in time)</u></strong> means it compiles the program, not before, but during the GPU‚Äôs execution.</li>
<li><strong><u>Bank conflict:</u></strong> since multiple threads can potentially want to access the same shared memory bank (a column) simultaneously, we would have to serialize them / or arrange the data properly</li>
<li><strong><u>PTX</u></strong> provides forward compatibility, and <strong><u>SASS</u></strong> provides the maximum performance.</li>
<li><strong><u>Memory coalescing</u></strong>: When a warp accesses the GPU‚Äôs main VRAM, the hardware checks the addresses they are requesting. If these addresses are close together and fall within a single, aligned memory segment, the GPU ‚Äúcoalesces‚Äù them. Instead of performing 32 small, separate memory fetches, it performs one single, large fetch that grabs all the requested data at once. Shared Memory/registers are fast enough, and L1/L2 also have this feature called locality.</li>
</ol>
<h2 id="torch-implementation-of-matrix-multiplication">Torch Implementation of Matrix Multiplication</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> matrix_multilication</span><span style="color:#E1E4E8">(x,y):</span></span>
<span class="line"><span style="color:#6A737D">  # consider matrix x and y with dimention M,N,K</span></span>
<span class="line"><span style="color:#E1E4E8">  M,N </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.shape</span></span>
<span class="line"><span style="color:#E1E4E8">  N,K </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> y.shape</span></span>
<span class="line"><span style="color:#E1E4E8">  accumulator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.zeros(M,K)</span></span>
<span class="line"><span style="color:#F97583">  for</span><span style="color:#E1E4E8"> row_x </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(M):</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> col_y </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(K):</span></span>
<span class="line"><span style="color:#E1E4E8">      dot_product </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.dot(x[row_X,:],y[:,col_y])</span></span>
<span class="line"><span style="color:#E1E4E8">      accumulator[row_x][col_y] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> dot_product</span></span>
<span class="line"><span style="color:#F97583">	return</span><span style="color:#E1E4E8"> accumulator</span></span>
<span class="line"><span style="color:#6A737D"># Basically just use</span></span>
<span class="line"><span style="color:#E1E4E8">result </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.matmul(x,y)</span></span></code></pre>
<h2 id="high-level-implementation-of-matrix-multiplication">High-Level Implementation of Matrix Multiplication</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D">#Implement the tiling in the Triton</span></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> matrix_multiplication</span><span style="color:#E1E4E8">(x,y,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K):</span></span>
<span class="line"><span style="color:#E1E4E8">  M,N </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x.shape</span></span>
<span class="line"><span style="color:#E1E4E8">  N,K </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> y.shape</span></span>
<span class="line"><span style="color:#E1E4E8">  z </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> torch.zeros(M,K)</span></span>
<span class="line"><span style="color:#F97583">  for</span><span style="color:#E1E4E8"> m </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">,M,</span><span style="color:#79B8FF">BLOCK_SIZE_M</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#F97583">  	for</span><span style="color:#E1E4E8"> k </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">,K,</span><span style="color:#79B8FF">BLOCK_SIZE_K</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">      accumulator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> zeros((</span><span style="color:#79B8FF">BLOCK_SIZE_M</span><span style="color:#E1E4E8">,</span><span style="color:#79B8FF">BLOCK_SIZE_K</span><span style="color:#E1E4E8">),</span><span style="color:#FFAB70">dtype</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">float32)</span></span>
<span class="line"><span style="color:#6A737D">      #in the SM So we have to export the ACCU to the L2 first</span></span>
<span class="line"><span style="color:#F97583">      for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">,n,</span><span style="color:#79B8FF">BLOCK_SIZE_N</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">        a </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> x[m:m</span><span style="color:#F97583">+</span><span style="color:#79B8FF">BLOCK_SIZE_M</span><span style="color:#E1E4E8">,n:n</span><span style="color:#F97583">+</span><span style="color:#79B8FF">BLOCK_SIZE_N</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">        b </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> y[n:n</span><span style="color:#F97583">+</span><span style="color:#79B8FF">BLOCK_SIZE_N</span><span style="color:#E1E4E8">,k:k</span><span style="color:#F97583">+</span><span style="color:#79B8FF">BLOCK_SIZE_K</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">        accumulator </span><span style="color:#F97583">+=</span><span style="color:#E1E4E8"> dot(a,b)</span></span>
<span class="line"><span style="color:#E1E4E8">      z[m:m</span><span style="color:#F97583">+</span><span style="color:#79B8FF">BLOCK_SIZE</span><span style="color:#E1E4E8">,k:k</span><span style="color:#F97583">+</span><span style="color:#79B8FF">BLOCK_SIZE_K</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> accumulator</span></span>
<span class="line"><span style="color:#F97583">  return</span><span style="color:#E1E4E8"> z</span></span></code></pre>
<p>Here are some useful insights:</p>
<ul>
<li>Why do we use the accumulator? Why can‚Äôt we just add to the output matrix? A: Because of the coalescing the access to the information is faster</li>
<li>Why do we do a small matrix multiplication instead of a faster dot product? A: They are essentially the same because they are just 1D arrays in the memory space, and the difference between these two algorithms won‚Äôt make much of a difference.</li>
</ul>
<h2 id="triton-kernel-for-matrix-multiplication">Triton Kernel for Matrix Multiplication</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#B392F0">@triton.jit</span></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> kernel_maxmul</span><span style="color:#E1E4E8">(</span></span>
<span class="line"><span style="color:#6A737D">        # Pointers to matrices</span></span>
<span class="line"><span style="color:#E1E4E8">        a_ptr, b_ptr, c_ptr,</span></span>
<span class="line"><span style="color:#6A737D">        # Matrix dimensions</span></span>
<span class="line"><span style="color:#E1E4E8">        M, N, K,</span></span>
<span class="line"><span style="color:#6A737D">        # The stride variables represent how much to increase the ptr by when moving by 1</span></span>
<span class="line"><span style="color:#E1E4E8">        stride_am, stride_ak,</span></span>
<span class="line"><span style="color:#E1E4E8">        stride_bk, stride_bn,</span></span>
<span class="line"><span style="color:#E1E4E8">        stride_cm, stride_cn,</span></span>
<span class="line"><span style="color:#6A737D">        # Meta-parameters</span></span>
<span class="line"><span style="color:#E1E4E8">        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  </span><span style="color:#6A737D"># for tiling</span></span>
<span class="line"><span style="color:#E1E4E8">        GROUP_SIZE_M: tl.constexpr,  </span><span style="color:#6A737D"># for super band</span></span>
<span class="line"><span style="color:#E1E4E8">        ACTIVATION: tl.constexpr  )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # This is done in a grouped ordering to promote L2 data reuse.(exactly why we use the GROUP_SIZE_M)</span></span>
<span class="line"><span style="color:#E1E4E8">    pid </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tl.program_id(</span><span style="color:#FFAB70">axis</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    num_pid_m </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tl.cdiv(M, </span><span style="color:#79B8FF">BLOCK_SIZE_M</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    num_pid_n </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tl.cdiv(N, </span><span style="color:#79B8FF">BLOCK_SIZE_N</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    num_pid_in_group </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> GROUP_SIZE_M</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> num_pid_n</span></span>
<span class="line"><span style="color:#E1E4E8">    group_id </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pid </span><span style="color:#F97583">//</span><span style="color:#E1E4E8"> num_pid_in_group</span></span>
<span class="line"><span style="color:#E1E4E8">    first_pid_m </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> group_id </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> GROUP_SIZE_M</span></span>
<span class="line"><span style="color:#E1E4E8">    group_size_m </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> min</span><span style="color:#E1E4E8">(num_pid_m </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> first_pid_m, </span><span style="color:#79B8FF">GROUP_SIZE_M</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    pid_m </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> first_pid_m </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> ((pid </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> num_pid_in_group) </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> group_size_m)</span></span>
<span class="line"><span style="color:#E1E4E8">    pid_n </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (pid </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> num_pid_in_group) </span><span style="color:#F97583">//</span><span style="color:#E1E4E8"> group_size_m</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # ------------------------------------------------------</span></span>
<span class="line"><span style="color:#6A737D">    # This helps to guide integer analysis in the backend to optimize load/store offset address calculation</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.assume(pid_m </span><span style="color:#F97583">>=</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.assume(pid_n </span><span style="color:#F97583">>=</span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.assume(stride_am </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.assume(stride_ak </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.assume(stride_bn </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.assume(stride_bk </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.assume(stride_cm </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.assume(stride_cn </span><span style="color:#F97583">></span><span style="color:#79B8FF"> 0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # ----------------------------------------------------------</span></span>
<span class="line"><span style="color:#6A737D">    # Create pointers for the first blocks of A and B.</span></span>
<span class="line"><span style="color:#E1E4E8">    offs_am </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (pid_m </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> BLOCK_SIZE_M</span><span style="color:#F97583"> +</span><span style="color:#E1E4E8"> tl.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">BLOCK_SIZE_M</span><span style="color:#E1E4E8">)) </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> M</span></span>
<span class="line"><span style="color:#E1E4E8">    offs_bn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (pid_n </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> BLOCK_SIZE_N</span><span style="color:#F97583"> +</span><span style="color:#E1E4E8"> tl.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">BLOCK_SIZE_N</span><span style="color:#E1E4E8">)) </span><span style="color:#F97583">%</span><span style="color:#E1E4E8"> N</span></span>
<span class="line"><span style="color:#E1E4E8">    offs_k </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tl.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">BLOCK_SIZE_K</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    a_ptrs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> a_ptr </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> (offs_am[:, </span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> stride_am </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> offs_k[</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, :] </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> stride_ak)</span></span>
<span class="line"><span style="color:#E1E4E8">    b_ptrs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> b_ptr </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> (offs_k[:, </span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> stride_bk </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> offs_bn[</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, :] </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> stride_bn)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # -----------------------------------------------------------</span></span>
<span class="line"><span style="color:#6A737D">    # Iterate to compute a block of the C matrix.</span></span>
<span class="line"><span style="color:#6A737D">    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block</span></span>
<span class="line"><span style="color:#6A737D">    # of fp32 values for higher accuracy.</span></span>
<span class="line"><span style="color:#6A737D">    # `accumulator` will be converted back to fp16 after the loop.</span></span>
<span class="line"><span style="color:#E1E4E8">    accumulator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tl.zeros((</span><span style="color:#79B8FF">BLOCK_SIZE_M</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">BLOCK_SIZE_N</span><span style="color:#E1E4E8">), </span><span style="color:#FFAB70">dtype</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">tl.float32)</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> k </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, tl.cdiv(K, </span><span style="color:#79B8FF">BLOCK_SIZE_K</span><span style="color:#E1E4E8">)):</span></span>
<span class="line"><span style="color:#6A737D">        # Load the next block of A and B, generate a mask by checking the K dimension.</span></span>
<span class="line"><span style="color:#6A737D">        # If it is out of bounds, set it to 0.</span></span>
<span class="line"><span style="color:#E1E4E8">        a </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tl.load(a_ptrs, </span><span style="color:#FFAB70">mask</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">offs_k[</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, :] </span><span style="color:#F97583">&#x3C;</span><span style="color:#E1E4E8"> K </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> k </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> BLOCK_SIZE_K</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">other</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">        b </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tl.load(b_ptrs, </span><span style="color:#FFAB70">mask</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">offs_k[:, </span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">&#x3C;</span><span style="color:#E1E4E8"> K </span><span style="color:#F97583">-</span><span style="color:#E1E4E8"> k </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> BLOCK_SIZE_K</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">other</span><span style="color:#F97583">=</span><span style="color:#79B8FF">0.0</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D">        # We accumulate along the K dimension.</span></span>
<span class="line"><span style="color:#E1E4E8">        accumulator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> tl.dot(a, b, accumulator)</span></span>
<span class="line"><span style="color:#6A737D">        # Advance the ptrs to the next K block.</span></span>
<span class="line"><span style="color:#E1E4E8">        a_ptrs </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> BLOCK_SIZE_K</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> stride_ak</span></span>
<span class="line"><span style="color:#E1E4E8">        b_ptrs </span><span style="color:#F97583">+=</span><span style="color:#79B8FF"> BLOCK_SIZE_K</span><span style="color:#F97583"> *</span><span style="color:#E1E4E8"> stride_bk</span></span>
<span class="line"><span style="color:#6A737D">    # You can fuse arbitrary activation functions here</span></span>
<span class="line"><span style="color:#6A737D">    # while the accumulator is still in FP32!</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#79B8FF"> ACTIVATION</span><span style="color:#F97583"> ==</span><span style="color:#9ECBFF"> "leaky_relu"</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">        accumulator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> leaky_relu(accumulator)</span></span>
<span class="line"><span style="color:#E1E4E8">    c </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> accumulator.to(tl.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # -----------------------------------------------------------</span></span>
<span class="line"><span style="color:#6A737D">    # Write back the block of the output matrix C with masks.</span></span>
<span class="line"><span style="color:#E1E4E8">    offs_cm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pid_m </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> BLOCK_SIZE_M</span><span style="color:#F97583"> +</span><span style="color:#E1E4E8"> tl.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">BLOCK_SIZE_M</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    offs_cn </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pid_n </span><span style="color:#F97583">*</span><span style="color:#79B8FF"> BLOCK_SIZE_N</span><span style="color:#F97583"> +</span><span style="color:#E1E4E8"> tl.arange(</span><span style="color:#79B8FF">0</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">BLOCK_SIZE_N</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    c_ptrs </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> c_ptr </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> stride_cm </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> offs_cm[:, </span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">+</span><span style="color:#E1E4E8"> stride_cn </span><span style="color:#F97583">*</span><span style="color:#E1E4E8"> offs_cn[</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, :]</span></span>
<span class="line"><span style="color:#E1E4E8">    c_mask </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> (offs_cm[:, </span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">&#x3C;</span><span style="color:#E1E4E8"> M) </span><span style="color:#F97583">&#x26;</span><span style="color:#E1E4E8"> (offs_cn[</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">, :] </span><span style="color:#F97583">&#x3C;</span><span style="color:#E1E4E8"> N)</span></span>
<span class="line"><span style="color:#E1E4E8">    tl.store(c_ptrs, c, </span><span style="color:#FFAB70">mask</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">c_mask)</span></span>
<span class="line"></span></code></pre>
<ul>
<li>We used a novel parameter: GROUP_SIZE_M, which was meant to be the height of a super-block with several BLOCK_SIZE_M tall. The following picture shows this idea better. Let me use the quote from the original tutorial <u><em>‚ÄúIn the following matmul where each matrix is 9 blocks by 9 blocks, we can see that if we compute the output in row-major ordering, we need to load 90 blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped ordering, we only need to load 54 blocks.‚Äù</em></u></li>
</ul>
<h2 id=""><img alt="grouped_vs_row_major_ordering" loading="lazy" decoding="async" fetchpriority="auto" width="1346" height="958" src="/_astro/grouped_vs_row_major_ordering.zGGkymFD_ZMOyLA.webp" ></h2>
<h2 id="conclusion">Conclusion</h2>
<p>GPU performance is dictated by its memory hierarchy: extremely fast but small on-chip memory (Registers, L1 Cache/Shared Memory) and large but slow off-chip memory (VRAM/HBM). The primary optimization goal is to minimize traffic to slow VRAM by maximizing data reuse in the fast caches.</p>
<p>The key software strategy is <strong>tiling</strong> (or blocking), where large matrices are broken into smaller blocks that fit into fast on-chip memory. Computations are performed on these blocks using a local <strong>accumulator</strong> to sum results, ensuring only one final, slow write to VRAM per block. This algorithm must also account for hardware features, ensuring memory access is <strong>coalesced</strong> to maximize VRAM bandwidth and that it avoids <strong>bank conflicts</strong> in shared memory.</p>
<p>Block Tiling also works ! And it fosters the calculation speed.</p>  </div> </article> <div class="post-footer" data-astro-cid-bvzihdzo> <a href="/blog" class="back-link" data-astro-cid-bvzihdzo>‚Üê Back to Blog</a> </div> </div>  </main> <footer class="site-footer"> <div class="container"> <p>&copy; 2025 Xiaoyou Wu. All rights reserved.</p> <div style="margin-top: 1rem;"> <a href="https://github.com/yourusername" target="_blank" style="margin: 0 10px;">GitHub</a> <a href="https://linkedin.com/in/yourprofile" target="_blank" style="margin: 0 10px;">LinkedIn</a> </div> </div> </footer> <!-- KaTeX --> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInDocument()"></script> <script>
    function renderMathInDocument() {
      if (typeof window.renderMathInElement !== 'undefined') {
        window.renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError: false
        });
      }
    }
    document.addEventListener("DOMContentLoaded", renderMathInDocument);
  </script> </body> </html>