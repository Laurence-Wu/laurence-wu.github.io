<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Quantization Aware Training Implementation Guide</title><meta name="description" content="Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies"><!-- Open Graph --><meta property="og:type" content="website"><meta property="og:url" content="https://laurence-wu.github.io/blog/qat-implementation/"><meta property="og:title" content="Quantization Aware Training Implementation Guide"><meta property="og:description" content="Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies"><meta property="og:image" content="/assets/profile-image.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:title" content="Quantization Aware Training Implementation Guide"><meta property="twitter:description" content="Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- KaTeX CSS for Math Rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous"><!-- Book Theme CSS --><link rel="stylesheet" href="/src/styles/book.css"><style>.site-nav[data-astro-cid-pux6a34n]{padding:2.5rem 0;border-bottom:1px solid var(--border-subtle);background-color:var(--bg-paper)}.nav-container[data-astro-cid-pux6a34n]{display:flex;justify-content:space-between;align-items:center;max-width:var(--max-width-layout);margin:0 auto;padding:0 1.5rem}.site-title[data-astro-cid-pux6a34n]{font-family:var(--font-sans);font-weight:700;font-size:1.1rem;color:var(--text-primary);text-decoration:none;letter-spacing:-.02em;border-bottom:none}.site-title[data-astro-cid-pux6a34n]:hover{color:var(--text-primary);border-bottom:none}.nav-links[data-astro-cid-pux6a34n]{display:flex;gap:2rem}.nav-link[data-astro-cid-pux6a34n]{color:var(--text-secondary);font-family:var(--font-sans);font-size:.95rem;text-decoration:none;transition:color .2s;border-bottom:none}.nav-link[data-astro-cid-pux6a34n]:hover,.nav-link[data-astro-cid-pux6a34n].active{color:var(--accent)}:root{--bg-paper: #fdfbf7;--bg-card: #ffffff;--text-primary: #2c2c2c;--text-secondary: #595959;--accent: #2e59a7;--accent-hover: #1a3c7a;--border-subtle: #e6e6e6;--font-serif: "Georgia", "Times New Roman", serif;--font-sans: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;--font-mono: "Menlo", "Consolas", "Monaco", monospace;--line-height-body: 1.8;--line-height-heading: 1.3;--spacing-container: 1.5rem;--max-width-text: 68ch;--max-width-layout: 800px}*,*:before,*:after{box-sizing:border-box;margin:0;padding:0}body{background-color:var(--bg-paper);color:var(--text-primary);font-family:var(--font-serif);line-height:var(--line-height-body);font-size:18px;-webkit-font-smoothing:antialiased}h1,h2,h3,h4,h5,h6{font-family:var(--font-sans);font-weight:600;line-height:var(--line-height-heading);color:#1a1a1a;margin-top:2.5rem;margin-bottom:1rem}h1{font-size:2.5rem;letter-spacing:-.5px}h2{font-size:1.8rem;border-bottom:1px solid var(--border-subtle);padding-bottom:.5rem}h3{font-size:1.4rem}p{margin-bottom:1.5rem}a{color:var(--accent);text-decoration:none;border-bottom:1px solid transparent;transition:border-color .2s}a:hover{color:var(--accent-hover);border-bottom-color:var(--accent-hover)}.container{max-width:var(--max-width-layout);margin:0 auto;padding:0 var(--spacing-container)}.main-content{padding:4rem 0}pre{background:#f5f5f5;padding:1rem;border-radius:4px;overflow-x:auto;font-family:var(--font-mono);font-size:.9em;margin:1.5rem 0;border:1px solid var(--border-subtle)}code{font-family:var(--font-mono);background:#0000000d;padding:.2em .4em;border-radius:3px;font-size:.85em}pre code{background:none;padding:0}img{max-width:100%;height:auto;display:block;margin:2rem auto;border-radius:4px}blockquote{border-left:3px solid var(--accent);margin:2rem 0;padding-left:1.5rem;font-style:italic;color:var(--text-secondary)}.site-nav{padding:2rem 0;border-bottom:1px solid var(--border-subtle);margin-bottom:2rem}.nav-container{display:flex;justify-content:space-between;align-items:center;max-width:var(--max-width-layout);margin:0 auto;padding:0 var(--spacing-container)}.site-title{font-family:var(--font-sans);font-weight:700;font-size:1.2rem;color:var(--text-primary);text-decoration:none}.nav-links{display:flex;gap:1.5rem}.nav-link{color:var(--text-secondary);font-family:var(--font-sans);font-size:.95rem}.nav-link:hover,.nav-link.active{color:var(--accent)}.site-footer{margin-top:4rem;padding:3rem 0;border-top:1px solid var(--border-subtle);text-align:center;font-size:.9rem;color:var(--text-secondary)}
.post-header[data-astro-cid-bvzihdzo]{text-align:center;margin-bottom:3rem;padding-bottom:2rem;border-bottom:1px solid var(--border-subtle)}.post-header[data-astro-cid-bvzihdzo] h1[data-astro-cid-bvzihdzo]{font-size:2.5rem;margin-bottom:.5rem;color:var(--text-primary)}.post-meta[data-astro-cid-bvzihdzo]{color:var(--text-secondary);font-size:.95rem;font-family:var(--font-sans)}.tags[data-astro-cid-bvzihdzo]{margin-top:1rem;display:flex;justify-content:center;gap:.8rem}.tag[data-astro-cid-bvzihdzo]{color:var(--accent);font-size:.9rem;font-family:var(--font-sans);text-decoration:none}.tag[data-astro-cid-bvzihdzo]:hover{text-decoration:underline}.post-content[data-astro-cid-bvzihdzo]{max-width:var(--max-width-text);margin:0 auto;font-size:1.1rem}.post-hero-image[data-astro-cid-bvzihdzo] img[data-astro-cid-bvzihdzo]{max-height:400px;-o-object-fit:cover;object-fit:cover;width:100%;margin-bottom:3rem}.post-footer[data-astro-cid-bvzihdzo]{margin-top:4rem;padding-top:2rem;border-top:1px solid var(--border-subtle);text-align:center}.back-link[data-astro-cid-bvzihdzo]{font-family:var(--font-sans);color:var(--text-secondary)}.back-link[data-astro-cid-bvzihdzo]:hover{color:var(--accent)}
</style></head> <body> <nav class="site-nav" data-astro-cid-pux6a34n> <div class="nav-container" data-astro-cid-pux6a34n> <a href="/" class="site-title" data-astro-cid-pux6a34n>Xiaoyou Wu</a> <div class="nav-links" data-astro-cid-pux6a34n> <a href="/blog" class="nav-link active" data-astro-cid-pux6a34n>
Writing
</a> <a href="/projects" class="nav-link " data-astro-cid-pux6a34n>
Projects
</a> <a href="/about" class="nav-link " data-astro-cid-pux6a34n>
About
</a> </div> </div> </nav>  <main class="main-content">  <div class="container" data-astro-cid-bvzihdzo> <article class="blog-post" data-astro-cid-bvzihdzo> <header class="post-header" data-astro-cid-bvzihdzo> <h1 data-astro-cid-bvzihdzo>Quantization Aware Training Implementation Guide</h1> <div class="post-meta" data-astro-cid-bvzihdzo> <time datetime="2025-01-15T00:00:00.000Z" data-astro-cid-bvzihdzo> January 15, 2025 </time> <span class="author" data-astro-cid-bvzihdzo> ‚Ä¢ Xiaoyou Wu</span>  </div> <div class="tags" data-astro-cid-bvzihdzo> <a href="/blog/tag/machine-learning/" class="tag" data-astro-cid-bvzihdzo>#machine-learning</a><a href="/blog/tag/quantization/" class="tag" data-astro-cid-bvzihdzo>#quantization</a><a href="/blog/tag/QAT/" class="tag" data-astro-cid-bvzihdzo>#QAT</a><a href="/blog/tag/LoRA/" class="tag" data-astro-cid-bvzihdzo>#LoRA</a><a href="/blog/tag/GPU/" class="tag" data-astro-cid-bvzihdzo>#GPU</a><a href="/blog/tag/optimization/" class="tag" data-astro-cid-bvzihdzo>#optimization</a><a href="/blog/tag/pytorch/" class="tag" data-astro-cid-bvzihdzo>#pytorch</a> </div> </header>   <div class="post-content" data-astro-cid-bvzihdzo>  <h1 id="quantization-aware-training">Quantization Aware Training</h1>
<h1 id="basic-information">Basic information</h1>
<h3 id="lora">LoRA:</h3>
<ul>
<li>Old method to improve domain specific fine-tuning faces disadvantages like sequential processing bottleneck. So LoRA changes the feed forward layer to self.linear(x) + (x @ self.lora_A @ self.lora_B) *
self.scaling .</li>
</ul>
<p><strong>PRT (Precision Range Test)</strong></p>
<ul>
<li>Start from the lowest bit and detect the predetermined threshold to record the B_min. Then the B_max should be determined by the max precision you will be need to experiment with.</li>
</ul>
<h2 id="implementation-details">Implementation details</h2>
<h3 id="loading-weights">Loading weights</h3>
<p>For model from the transformer, you should remember to import the weight of the projection layer</p>
<h3 id="initializing-the-model">Initializing the model</h3>
<p>Use apply function in torch to optimize the initialization of different layers. In the current case, you should initialize transformer layer with its final projection layer and also two layers of ffn.</p>
<h3 id="inheritance-of--nnmodule-and-torchautogradfunction">Inheritance of  nn.Module and torch.autograd.Function</h3>
<h1 id="appendix-on-torch-and-transformer-usage">APPENDIX ON torch and transformer usage</h1>
<h2 id="important-functions-in-torch">Important functions in torch</h2>
<ol>
<li>torch.save: uses the pickle for operations. For tensors, its raw data, size information, gradient requirements. For models, it mainly store the state_dict which is an orderedDict that maps each layer or params name to its tensor values.</li>
<li>torch.amp.GradScaler(‚Äòcuda‚Äô): Automatic Mixed Precision</li>
<li>torch.nn.module: its handy to inherit this class for your customized model. You can just do model(input_params) to call the ‚Äú_<em>call</em>_‚Äù function inherited to perform ffn.</li>
<li>For dataset objects imported with the load_dataset from the datasets library. Its handy to call the .feature property  or you can call the _<em>dict_</em> method to understand the structure.</li>
<li>return_tensor = ‚Äúpt‚Äù adds a dimension so remember to do the [0] for the tensor.</li>
<li>While loading information from a dataset, pay attention to the dataset padding token and eos token, if their choice is the same, you should change the padding token to be something else</li>
</ol>
<h2 id="important-functions-in-transformer">Important functions in transformer</h2>
<ol>
<li>GPT2Config return an json with all those configurations and then could be utilized by other tuning methods.</li>
<li>model.eval() turn on the evaluation mode and disable those dropout,</li>
</ol>
<h1 id="appendix-on-gpu-ram-usage">APPENDIX ON GPU RAM usage</h1>
<h1 id="gpu-memory-hierarchy--parameter-impact-guide">GPU Memory Hierarchy &#x26; Parameter Impact Guide</h1>
<h2 id="memory-hierarchy-overview">Memory Hierarchy Overview</h2>








































<table><thead><tr><th>Memory Type</th><th>Size (A100)</th><th>Bandwidth</th><th>Latency</th><th>What‚Äôs Stored</th></tr></thead><tbody><tr><td><strong>Registers</strong></td><td>256 KB/SM</td><td>~19 TB/s</td><td>1 cycle</td><td>Active thread variables, loop counters</td></tr><tr><td><strong>L1 Cache/SMEM</strong></td><td>192 KB/SM</td><td>~19 TB/s</td><td>~30 cycles</td><td>Shared memory, frequently accessed data</td></tr><tr><td><strong>L2 Cache</strong></td><td>40 MB</td><td>~4 TB/s</td><td>~200 cycles</td><td>Recently accessed data from HBM</td></tr><tr><td><strong>HBM (VRAM)</strong></td><td>40-80 GB</td><td>~2 TB/s</td><td>~400 cycles</td><td>Model weights, activations, optimizer states</td></tr></tbody></table>
<h2 id="parameter-impact-on-memory-usage">Parameter Impact on Memory Usage</h2>




































































<table><thead><tr><th>Parameter</th><th>HBM Usage</th><th>L1/SMEM Usage</th><th>Register Usage</th><th>Impact Description</th></tr></thead><tbody><tr><td><strong>batch_size</strong></td><td>High üî¥</td><td>Medium üü°</td><td>Low üü¢</td><td>Multiplies activation memory linearly</td></tr><tr><td><strong>model_size</strong></td><td>High üî¥</td><td>Low üü¢</td><td>Low üü¢</td><td>Weights stored entirely in HBM</td></tr><tr><td><strong>sequence_length</strong></td><td>High üî¥</td><td>Medium üü°</td><td>Low üü¢</td><td>Quadratic for attention (seq_len¬≤)</td></tr><tr><td><strong>hidden_dim</strong></td><td>High üî¥</td><td>Medium üü°</td><td>Low üü¢</td><td>Affects weight matrices &#x26; activations</td></tr><tr><td><strong>num_layers</strong></td><td>High üî¥</td><td>Low üü¢</td><td>Low üü¢</td><td>Linear increase in weights</td></tr><tr><td><strong>precision (FP32/16/8)</strong></td><td>High üî¥</td><td>Medium üü°</td><td>Medium üü°</td><td>Halves memory per precision drop</td></tr><tr><td><strong>gradient_accumulation</strong></td><td>Low üü¢</td><td>Low üü¢</td><td>Low üü¢</td><td>Reduces batch memory requirement</td></tr><tr><td><strong>optimizer (SGD/Adam)</strong></td><td>High üî¥</td><td>Low üü¢</td><td>Low üü¢</td><td>Adam uses 3x memory (m, v states)</td></tr></tbody></table>
<h2 id="detailed-hbm-storage-breakdown">Detailed HBM Storage Breakdown</h2>






















































<table><thead><tr><th>Component</th><th>Formula</th><th>FP32 Memory</th><th>FP16 Memory</th><th>Stored Location</th></tr></thead><tbody><tr><td><strong>Model Weights</strong></td><td><code>num_params √ó precision</code></td><td>4 bytes/param</td><td>2 bytes/param</td><td>HBM</td></tr><tr><td><strong>Gradients</strong></td><td><code>num_params √ó precision</code></td><td>4 bytes/param</td><td>2 bytes/param</td><td>HBM</td></tr><tr><td><strong>Adam Optimizer</strong></td><td><code>2 √ó num_params √ó FP32</code></td><td>8 bytes/param</td><td>8 bytes/param*</td><td>HBM</td></tr><tr><td><strong>Activations</strong></td><td><code>batch √ó seq_len √ó hidden √ó layers</code></td><td>Variable</td><td>Variable/2</td><td>HBM</td></tr><tr><td><strong>KV Cache (LLMs)</strong></td><td><code>batch √ó heads √ó seq_len √ó dim √ó layers √ó 2</code></td><td>Large</td><td>Large/2</td><td>HBM</td></tr><tr><td><strong>Temp Buffers</strong></td><td><code>workspace for ops</code></td><td>~1-2 GB</td><td>~0.5-1 GB</td><td>HBM</td></tr></tbody></table>
<p>*Adam states typically stay FP32 even in mixed precision</p>
<h2 id="kernel-level-memory-usage">Kernel-Level Memory Usage</h2>















































<table><thead><tr><th>Operation</th><th>Register Pressure</th><th>L1/SMEM Usage</th><th>HBM Access Pattern</th></tr></thead><tbody><tr><td><strong>GEMM (MatMul)</strong></td><td>High üî¥</td><td>High üî¥</td><td>Tiled access</td></tr><tr><td><strong>Element-wise</strong></td><td>Medium üü°</td><td>Low üü¢</td><td>Sequential streaming</td></tr><tr><td><strong>Softmax</strong></td><td>Medium üü°</td><td>Medium üü°</td><td>Row-wise access</td></tr><tr><td><strong>LayerNorm</strong></td><td>Medium üü°</td><td>Medium üü°</td><td>Channel-wise access</td></tr><tr><td><strong>Attention</strong></td><td>High üî¥</td><td>High üî¥</td><td>Complex tiling</td></tr><tr><td><strong>Conv2D</strong></td><td>High üî¥</td><td>High üî¥</td><td>Im2col or tiled</td></tr></tbody></table>
<h2 id="optimization-strategies-by-memory-type">Optimization Strategies by Memory Type</h2>






























<table><thead><tr><th>Memory Type</th><th>Optimization Strategy</th><th>Impact</th></tr></thead><tbody><tr><td><strong>HBM</strong></td><td>Gradient checkpointing, model sharding, mixed precision</td><td>Reduce total storage</td></tr><tr><td><strong>L2 Cache</strong></td><td>Increase arithmetic intensity, kernel fusion</td><td>Reduce HBM traffic</td></tr><tr><td><strong>L1/SMEM</strong></td><td>Tile size tuning, shared memory allocation</td><td>Better data reuse</td></tr><tr><td><strong>Registers</strong></td><td>Loop unrolling, reduce live variables</td><td>Higher throughput</td></tr></tbody></table>
<h2 id="practical-example-gpt-2-medium-345m-parameters">Practical Example: GPT-2 Medium (345M Parameters)</h2>
<h3 id="memory-breakdown">Memory Breakdown</h3>








































<table><thead><tr><th>Component</th><th>Calculation</th><th>Memory Usage</th></tr></thead><tbody><tr><td><strong>Parameters</strong></td><td>345M params √ó 4 bytes</td><td>1.4 GB (FP32)</td></tr><tr><td><strong>Gradients</strong></td><td>345M params √ó 4 bytes</td><td>1.4 GB (FP32)</td></tr><tr><td><strong>Adam States</strong></td><td>345M √ó 2 √ó 4 bytes</td><td>2.8 GB (FP32)</td></tr><tr><td><strong>Activations</strong></td><td>batch=8, seq=1024, ~20 layers</td><td>~4 GB</td></tr><tr><td><strong>Total Training</strong></td><td>Sum of above</td><td><strong>~9.6 GB</strong></td></tr><tr><td><strong>Inference Only</strong></td><td>Parameters only</td><td><strong>~1.4 GB</strong></td></tr></tbody></table>
<h3 id="memory-usage-by-precision">Memory Usage by Precision</h3>





































<table><thead><tr><th>Precision</th><th>Weights</th><th>Gradients</th><th>Adam</th><th>Activations</th><th>Total Training</th></tr></thead><tbody><tr><td><strong>FP32</strong></td><td>1.4 GB</td><td>1.4 GB</td><td>2.8 GB</td><td>4 GB</td><td>9.6 GB</td></tr><tr><td><strong>FP16 Mixed</strong></td><td>0.7 GB</td><td>0.7 GB</td><td>2.8 GB</td><td>2 GB</td><td>6.2 GB</td></tr><tr><td><strong>INT8</strong></td><td>0.35 GB</td><td>N/A</td><td>N/A</td><td>1 GB</td><td>1.35 GB (Inference)</td></tr></tbody></table>
<h2 id="memory-calculation-formulas">Memory Calculation Formulas</h2>
<h3 id="training-memory">Training Memory</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Total_Memory = Model_Weights + Gradients + Optimizer_States + Activations + Temp_Buffers</span></span></code></pre>
<h3 id="model-weights">Model Weights</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Model_Memory = num_parameters √ó bytes_per_param</span></span></code></pre>
<h3 id="activation-memory-transformer">Activation Memory (Transformer)</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Activation_Memory = batch_size √ó seq_length √ó hidden_dim √ó num_layers √ó </span></span>
<span class="line"><span>                   (attention_heads + mlp_ratio + norm_layers)</span></span></code></pre>
<h3 id="attention-memory">Attention Memory</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Attention_Memory = batch_size √ó num_heads √ó seq_length¬≤ √ó head_dim √ó num_layers</span></span></code></pre>
<h2 id="common-memory-bottlenecks">Common Memory Bottlenecks</h2>



































<table><thead><tr><th>Bottleneck</th><th>Symptoms</th><th>Solution</th></tr></thead><tbody><tr><td><strong>OOM on forward pass</strong></td><td>Crashes during model(input)</td><td>Reduce batch size or model size</td></tr><tr><td><strong>OOM on backward pass</strong></td><td>Crashes during loss.backward()</td><td>Enable gradient checkpointing</td></tr><tr><td><strong>OOM on optimizer step</strong></td><td>Crashes during optimizer.step()</td><td>Use gradient accumulation or efficient optimizer</td></tr><tr><td><strong>Slow training</strong></td><td>Low GPU utilization</td><td>Increase batch size or arithmetic intensity</td></tr><tr><td><strong>Memory fragmentation</strong></td><td>OOM with available memory</td><td>Clear cache: <code>torch.cuda.empty_cache()</code></td></tr></tbody></table>  </div> </article> <div class="post-footer" data-astro-cid-bvzihdzo> <a href="/blog" class="back-link" data-astro-cid-bvzihdzo>‚Üê Back to Blog</a> </div> </div>  </main> <footer class="site-footer"> <div class="container"> <p>&copy; 2025 Xiaoyou Wu. All rights reserved.</p> <div style="margin-top: 1rem;"> <a href="https://github.com/yourusername" target="_blank" style="margin: 0 10px;">GitHub</a> <a href="https://linkedin.com/in/yourprofile" target="_blank" style="margin: 0 10px;">LinkedIn</a> </div> </div> </footer> <!-- KaTeX --> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInDocument()"></script> <script>
    function renderMathInDocument() {
      if (typeof window.renderMathInElement !== 'undefined') {
        window.renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError: false
        });
      }
    }
    document.addEventListener("DOMContentLoaded", renderMathInDocument);
  </script> </body> </html>