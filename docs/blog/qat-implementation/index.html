<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Quantization Aware Training Implementation Guide</title><meta name="description" content="Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://laurence-wu.github.io/blog/qat-implementation/"><meta property="og:title" content="Quantization Aware Training Implementation Guide"><meta property="og:description" content="Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies"><meta property="og:image" content="/assets/profile-image.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://laurence-wu.github.io/blog/qat-implementation/"><meta property="twitter:title" content="Quantization Aware Training Implementation Guide"><meta property="twitter:description" content="Implementation details and best practices for Quantization Aware Training (QAT) with LoRA, including GPU memory optimization strategies"><meta property="twitter:image" content="/assets/profile-image.jpg"><!-- KaTeX CSS for Math Rendering --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous"><!-- Google Fonts - Preserved Typography System --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;500;600;700&family=Montserrat:wght@300;400;500;600;700&family=Cormorant+Garamond:wght@300;400;500;600;700&display=swap" rel="stylesheet"><!-- Fira Code for code blocks --><link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap" rel="stylesheet"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Global CSS Import --><link rel="stylesheet" href="/_astro/_slug_.CaX6V1Dz.css">
<link rel="stylesheet" href="/_astro/index.BRcr8Vd7.css">
<link rel="stylesheet" href="/_astro/_slug_.CDIrPqxW.css"></head> <body data-astro-cid-37fxchfa> <nav class="navbar" data-astro-cid-pux6a34n> <div class="navbar-brand" data-astro-cid-pux6a34n>Xiaoyou Wu</div> <a href="/" class="navbar-link " data-astro-cid-pux6a34n>
Home
</a> <a href="/blog" class="navbar-link active" data-astro-cid-pux6a34n>
Blog
</a> <a href="/projects" class="navbar-link " data-astro-cid-pux6a34n>
Projects
</a> <a href="/about" class="navbar-link " data-astro-cid-pux6a34n>
About
</a> </nav>  <script type="module">function r(){const e=document.querySelector(".navbar");window.scrollY>50?e?.classList.add("scrolled"):e?.classList.remove("scrolled")}window.addEventListener("scroll",r);document.querySelector(".navbar")?.addEventListener("contextmenu",e=>{e.preventDefault()});document.querySelector(".navbar")?.addEventListener("dragstart",e=>{e.preventDefault()});</script> <div class="animated-background full" aria-hidden="true" data-astro-cid-5a2bynky></div>  <main class="main-content" data-astro-cid-37fxchfa>  <main class="blog-post" data-astro-cid-bvzihdzo> <article data-astro-cid-bvzihdzo> <header class="post-header" data-astro-cid-bvzihdzo> <h1 data-astro-cid-bvzihdzo>Quantization Aware Training Implementation Guide</h1> <div class="post-meta" data-astro-cid-bvzihdzo> <time datetime="2025-01-15T00:00:00.000Z" data-astro-cid-bvzihdzo> January 14, 2025 </time> <span class="author" data-astro-cid-bvzihdzo>by Xiaoyou Wu</span>  </div> <div class="tags" data-astro-cid-bvzihdzo> <a href="/blog/tag/machine-learning/" class="tag" data-astro-cid-bvzihdzo>machine-learning</a><a href="/blog/tag/quantization/" class="tag" data-astro-cid-bvzihdzo>quantization</a><a href="/blog/tag/QAT/" class="tag" data-astro-cid-bvzihdzo>QAT</a><a href="/blog/tag/LoRA/" class="tag" data-astro-cid-bvzihdzo>LoRA</a><a href="/blog/tag/GPU/" class="tag" data-astro-cid-bvzihdzo>GPU</a><a href="/blog/tag/optimization/" class="tag" data-astro-cid-bvzihdzo>optimization</a><a href="/blog/tag/pytorch/" class="tag" data-astro-cid-bvzihdzo>pytorch</a> </div> </header> <div id="table-of-contents" class="toc-container" data-astro-cid-xvrfupwn> <div class="toc-header" data-astro-cid-xvrfupwn> <div class="toc-title-area" data-astro-cid-xvrfupwn> <span class="drag-handle" title="Drag to move" data-astro-cid-xvrfupwn>â‹®â‹®</span> <h3 data-astro-cid-xvrfupwn>Table of Contents</h3> </div> <button id="toc-toggle" class="toc-toggle" aria-label="Toggle table of contents" data-astro-cid-xvrfupwn> <span class="toggle-icon" data-astro-cid-xvrfupwn>â†</span> </button> </div> <nav class="toc-nav" aria-label="Table of contents" data-astro-cid-xvrfupwn> <ul id="toc-list" class="toc-list" data-astro-cid-xvrfupwn> <!-- TOC items will be populated by JavaScript --> </ul> </nav> </div>  <script type="module" src="/_astro/TableOfContents.astro_astro_type_script_index_0_lang.ZAa9MuU5.js"></script> <div class="content-with-sidebar" data-astro-cid-bvzihdzo> <div class="post-content main-content" data-astro-cid-bvzihdzo>  <h1 id="quantization-aware-training">Quantization Aware Training</h1>
<h1 id="basic-information">Basic information</h1>
<h3 id="lora">LoRA:</h3>
<ul>
<li>Old method to improve domain specific fine-tuning faces disadvantages like sequential processing bottleneck. So LoRA changes the feed forward layer to self.linear(x) + (x @ self.lora_A @ self.lora_B) *
self.scaling .</li>
</ul>
<p><strong>PRT (Precision Range Test)</strong></p>
<ul>
<li>Start from the lowest bit and detect the predetermined threshold to record the B_min. Then the B_max should be determined by the max precision you will be need to experiment with.</li>
</ul>
<h2 id="implementation-details">Implementation details</h2>
<h3 id="loading-weights">Loading weights</h3>
<p>For model from the transformer, you should remember to import the weight of the projection layer</p>
<h3 id="initializing-the-model">Initializing the model</h3>
<p>Use apply function in torch to optimize the initialization of different layers. In the current case, you should initialize transformer layer with its final projection layer and also two layers of ffn.</p>
<h3 id="inheritance-of--nnmodule-and-torchautogradfunction">Inheritance of  nn.Module and torch.autograd.Function</h3>
<h1 id="appendix-on-torch-and-transformer-usage">APPENDIX ON torch and transformer usage</h1>
<h2 id="important-functions-in-torch">Important functions in torch</h2>
<ol>
<li>torch.save: uses the pickle for operations. For tensors, its raw data, size information, gradient requirements. For models, it mainly store the state_dict which is an orderedDict that maps each layer or params name to its tensor values.</li>
<li>torch.amp.GradScaler(â€˜cudaâ€™): Automatic Mixed Precision</li>
<li>torch.nn.module: its handy to inherit this class for your customized model. You can just do model(input_params) to call the â€œ_<em>call</em>_â€ function inherited to perform ffn.</li>
<li>For dataset objects imported with the load_dataset from the datasets library. Its handy to call the .feature property  or you can call the _<em>dict_</em> method to understand the structure.</li>
<li>return_tensor = â€œptâ€ adds a dimension so remember to do the [0] for the tensor.</li>
<li>While loading information from a dataset, pay attention to the dataset padding token and eos token, if their choice is the same, you should change the padding token to be something else</li>
</ol>
<h2 id="important-functions-in-transformer">Important functions in transformer</h2>
<ol>
<li>GPT2Config return an json with all those configurations and then could be utilized by other tuning methods.</li>
<li>model.eval() turn on the evaluation mode and disable those dropout,</li>
</ol>
<h1 id="appendix-on-gpu-ram-usage">APPENDIX ON GPU RAM usage</h1>
<h1 id="gpu-memory-hierarchy--parameter-impact-guide">GPU Memory Hierarchy &#x26; Parameter Impact Guide</h1>
<h2 id="memory-hierarchy-overview">Memory Hierarchy Overview</h2>








































<table><thead><tr><th>Memory Type</th><th>Size (A100)</th><th>Bandwidth</th><th>Latency</th><th>Whatâ€™s Stored</th></tr></thead><tbody><tr><td><strong>Registers</strong></td><td>256 KB/SM</td><td>~19 TB/s</td><td>1 cycle</td><td>Active thread variables, loop counters</td></tr><tr><td><strong>L1 Cache/SMEM</strong></td><td>192 KB/SM</td><td>~19 TB/s</td><td>~30 cycles</td><td>Shared memory, frequently accessed data</td></tr><tr><td><strong>L2 Cache</strong></td><td>40 MB</td><td>~4 TB/s</td><td>~200 cycles</td><td>Recently accessed data from HBM</td></tr><tr><td><strong>HBM (VRAM)</strong></td><td>40-80 GB</td><td>~2 TB/s</td><td>~400 cycles</td><td>Model weights, activations, optimizer states</td></tr></tbody></table>
<h2 id="parameter-impact-on-memory-usage">Parameter Impact on Memory Usage</h2>




































































<table><thead><tr><th>Parameter</th><th>HBM Usage</th><th>L1/SMEM Usage</th><th>Register Usage</th><th>Impact Description</th></tr></thead><tbody><tr><td><strong>batch_size</strong></td><td>High ğŸ”´</td><td>Medium ğŸŸ¡</td><td>Low ğŸŸ¢</td><td>Multiplies activation memory linearly</td></tr><tr><td><strong>model_size</strong></td><td>High ğŸ”´</td><td>Low ğŸŸ¢</td><td>Low ğŸŸ¢</td><td>Weights stored entirely in HBM</td></tr><tr><td><strong>sequence_length</strong></td><td>High ğŸ”´</td><td>Medium ğŸŸ¡</td><td>Low ğŸŸ¢</td><td>Quadratic for attention (seq_lenÂ²)</td></tr><tr><td><strong>hidden_dim</strong></td><td>High ğŸ”´</td><td>Medium ğŸŸ¡</td><td>Low ğŸŸ¢</td><td>Affects weight matrices &#x26; activations</td></tr><tr><td><strong>num_layers</strong></td><td>High ğŸ”´</td><td>Low ğŸŸ¢</td><td>Low ğŸŸ¢</td><td>Linear increase in weights</td></tr><tr><td><strong>precision (FP32/16/8)</strong></td><td>High ğŸ”´</td><td>Medium ğŸŸ¡</td><td>Medium ğŸŸ¡</td><td>Halves memory per precision drop</td></tr><tr><td><strong>gradient_accumulation</strong></td><td>Low ğŸŸ¢</td><td>Low ğŸŸ¢</td><td>Low ğŸŸ¢</td><td>Reduces batch memory requirement</td></tr><tr><td><strong>optimizer (SGD/Adam)</strong></td><td>High ğŸ”´</td><td>Low ğŸŸ¢</td><td>Low ğŸŸ¢</td><td>Adam uses 3x memory (m, v states)</td></tr></tbody></table>
<h2 id="detailed-hbm-storage-breakdown">Detailed HBM Storage Breakdown</h2>






















































<table><thead><tr><th>Component</th><th>Formula</th><th>FP32 Memory</th><th>FP16 Memory</th><th>Stored Location</th></tr></thead><tbody><tr><td><strong>Model Weights</strong></td><td><code>num_params Ã— precision</code></td><td>4 bytes/param</td><td>2 bytes/param</td><td>HBM</td></tr><tr><td><strong>Gradients</strong></td><td><code>num_params Ã— precision</code></td><td>4 bytes/param</td><td>2 bytes/param</td><td>HBM</td></tr><tr><td><strong>Adam Optimizer</strong></td><td><code>2 Ã— num_params Ã— FP32</code></td><td>8 bytes/param</td><td>8 bytes/param*</td><td>HBM</td></tr><tr><td><strong>Activations</strong></td><td><code>batch Ã— seq_len Ã— hidden Ã— layers</code></td><td>Variable</td><td>Variable/2</td><td>HBM</td></tr><tr><td><strong>KV Cache (LLMs)</strong></td><td><code>batch Ã— heads Ã— seq_len Ã— dim Ã— layers Ã— 2</code></td><td>Large</td><td>Large/2</td><td>HBM</td></tr><tr><td><strong>Temp Buffers</strong></td><td><code>workspace for ops</code></td><td>~1-2 GB</td><td>~0.5-1 GB</td><td>HBM</td></tr></tbody></table>
<p>*Adam states typically stay FP32 even in mixed precision</p>
<h2 id="kernel-level-memory-usage">Kernel-Level Memory Usage</h2>















































<table><thead><tr><th>Operation</th><th>Register Pressure</th><th>L1/SMEM Usage</th><th>HBM Access Pattern</th></tr></thead><tbody><tr><td><strong>GEMM (MatMul)</strong></td><td>High ğŸ”´</td><td>High ğŸ”´</td><td>Tiled access</td></tr><tr><td><strong>Element-wise</strong></td><td>Medium ğŸŸ¡</td><td>Low ğŸŸ¢</td><td>Sequential streaming</td></tr><tr><td><strong>Softmax</strong></td><td>Medium ğŸŸ¡</td><td>Medium ğŸŸ¡</td><td>Row-wise access</td></tr><tr><td><strong>LayerNorm</strong></td><td>Medium ğŸŸ¡</td><td>Medium ğŸŸ¡</td><td>Channel-wise access</td></tr><tr><td><strong>Attention</strong></td><td>High ğŸ”´</td><td>High ğŸ”´</td><td>Complex tiling</td></tr><tr><td><strong>Conv2D</strong></td><td>High ğŸ”´</td><td>High ğŸ”´</td><td>Im2col or tiled</td></tr></tbody></table>
<h2 id="optimization-strategies-by-memory-type">Optimization Strategies by Memory Type</h2>






























<table><thead><tr><th>Memory Type</th><th>Optimization Strategy</th><th>Impact</th></tr></thead><tbody><tr><td><strong>HBM</strong></td><td>Gradient checkpointing, model sharding, mixed precision</td><td>Reduce total storage</td></tr><tr><td><strong>L2 Cache</strong></td><td>Increase arithmetic intensity, kernel fusion</td><td>Reduce HBM traffic</td></tr><tr><td><strong>L1/SMEM</strong></td><td>Tile size tuning, shared memory allocation</td><td>Better data reuse</td></tr><tr><td><strong>Registers</strong></td><td>Loop unrolling, reduce live variables</td><td>Higher throughput</td></tr></tbody></table>
<h2 id="practical-example-gpt-2-medium-345m-parameters">Practical Example: GPT-2 Medium (345M Parameters)</h2>
<h3 id="memory-breakdown">Memory Breakdown</h3>








































<table><thead><tr><th>Component</th><th>Calculation</th><th>Memory Usage</th></tr></thead><tbody><tr><td><strong>Parameters</strong></td><td>345M params Ã— 4 bytes</td><td>1.4 GB (FP32)</td></tr><tr><td><strong>Gradients</strong></td><td>345M params Ã— 4 bytes</td><td>1.4 GB (FP32)</td></tr><tr><td><strong>Adam States</strong></td><td>345M Ã— 2 Ã— 4 bytes</td><td>2.8 GB (FP32)</td></tr><tr><td><strong>Activations</strong></td><td>batch=8, seq=1024, ~20 layers</td><td>~4 GB</td></tr><tr><td><strong>Total Training</strong></td><td>Sum of above</td><td><strong>~9.6 GB</strong></td></tr><tr><td><strong>Inference Only</strong></td><td>Parameters only</td><td><strong>~1.4 GB</strong></td></tr></tbody></table>
<h3 id="memory-usage-by-precision">Memory Usage by Precision</h3>





































<table><thead><tr><th>Precision</th><th>Weights</th><th>Gradients</th><th>Adam</th><th>Activations</th><th>Total Training</th></tr></thead><tbody><tr><td><strong>FP32</strong></td><td>1.4 GB</td><td>1.4 GB</td><td>2.8 GB</td><td>4 GB</td><td>9.6 GB</td></tr><tr><td><strong>FP16 Mixed</strong></td><td>0.7 GB</td><td>0.7 GB</td><td>2.8 GB</td><td>2 GB</td><td>6.2 GB</td></tr><tr><td><strong>INT8</strong></td><td>0.35 GB</td><td>N/A</td><td>N/A</td><td>1 GB</td><td>1.35 GB (Inference)</td></tr></tbody></table>
<h2 id="memory-calculation-formulas">Memory Calculation Formulas</h2>
<h3 id="training-memory">Training Memory</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Total_Memory = Model_Weights + Gradients + Optimizer_States + Activations + Temp_Buffers</span></span></code></pre>
<h3 id="model-weights">Model Weights</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Model_Memory = num_parameters Ã— bytes_per_param</span></span></code></pre>
<h3 id="activation-memory-transformer">Activation Memory (Transformer)</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Activation_Memory = batch_size Ã— seq_length Ã— hidden_dim Ã— num_layers Ã— </span></span>
<span class="line"><span>                   (attention_heads + mlp_ratio + norm_layers)</span></span></code></pre>
<h3 id="attention-memory">Attention Memory</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Attention_Memory = batch_size Ã— num_heads Ã— seq_lengthÂ² Ã— head_dim Ã— num_layers</span></span></code></pre>
<h2 id="common-memory-bottlenecks">Common Memory Bottlenecks</h2>



































<table><thead><tr><th>Bottleneck</th><th>Symptoms</th><th>Solution</th></tr></thead><tbody><tr><td><strong>OOM on forward pass</strong></td><td>Crashes during model(input)</td><td>Reduce batch size or model size</td></tr><tr><td><strong>OOM on backward pass</strong></td><td>Crashes during loss.backward()</td><td>Enable gradient checkpointing</td></tr><tr><td><strong>OOM on optimizer step</strong></td><td>Crashes during optimizer.step()</td><td>Use gradient accumulation or efficient optimizer</td></tr><tr><td><strong>Slow training</strong></td><td>Low GPU utilization</td><td>Increase batch size or arithmetic intensity</td></tr><tr><td><strong>Memory fragmentation</strong></td><td>OOM with available memory</td><td>Clear cache: <code>torch.cuda.empty_cache()</code></td></tr></tbody></table>  </div>  </div> </article> </main> <footer class="blog-footer" data-astro-cid-bvzihdzo> <div class="container" data-astro-cid-bvzihdzo> <a href="/blog" class="back-link" data-astro-cid-bvzihdzo>â† Back to Blog</a> </div> </footer>  </main> <footer class="site-footer" data-astro-cid-37fxchfa> <div class="footer-content" data-astro-cid-37fxchfa> <p data-astro-cid-37fxchfa>&copy; 2025 Xiaoyou Wu. All rights reserved.</p> <div class="footer-links" data-astro-cid-37fxchfa> <a href="https://github.com/yourusername" target="_blank" rel="noopener noreferrer" data-astro-cid-37fxchfa>GitHub</a> <a href="https://linkedin.com/in/yourprofile" target="_blank" rel="noopener noreferrer" data-astro-cid-37fxchfa>LinkedIn</a> <a href="/resume.pdf" target="_blank" rel="noopener noreferrer" data-astro-cid-37fxchfa>Resume</a> </div> </div> </footer> <!-- KaTeX JavaScript for Math Rendering --> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInDocument()"></script> <script>
    function renderMathInDocument() {
      if (typeof window.renderMathInElement !== 'undefined') {
        window.renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError: false
        });
      }
    }
    
    document.addEventListener("DOMContentLoaded", renderMathInDocument);
  </script> <!-- Simple Mermaid Test Script --> <script type="module">
    console.log('[Simple Mermaid Test] Starting...');

    // Simple function to load and initialize Mermaid
    async function loadAndTestMermaid() {
      try {
        console.log('[Simple Mermaid Test] Loading Mermaid from CDN...');
        
        // Load Mermaid if not already loaded
        if (!window.mermaid) {
          const script = document.createElement('script');
          script.src = 'https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js';
          
          await new Promise((resolve, reject) => {
            script.onload = resolve;
            script.onerror = reject;
            document.head.appendChild(script);
          });
          
          console.log('[Simple Mermaid Test] Mermaid loaded from CDN');
        } else {
          console.log('[Simple Mermaid Test] Mermaid already available');
        }

        if (!window.mermaid) {
          throw new Error('Mermaid not available after loading');
        }

        console.log('[Simple Mermaid Test] Initializing Mermaid...');
        
        // Initialize Mermaid with simple config
        window.mermaid.initialize({
          startOnLoad: false,
          theme: 'neutral'
        });

        console.log('[Simple Mermaid Test] Finding diagram elements...');
        
        // Find all mermaid diagrams
        const diagrams = document.querySelectorAll('.mermaid-diagram');
        console.log(`[Simple Mermaid Test] Found ${diagrams.length} diagrams`);

        // Process each diagram
        for (let i = 0; i < diagrams.length; i++) {
          const diagram = diagrams[i];
          const id = diagram.id || `test-diagram-${i}`;
          const codeElement = diagram.querySelector('.mermaid-code code');
          
          console.log(`[Simple Mermaid Test] Processing diagram ${id}`);
          
          if (codeElement) {
            const code = codeElement.textContent.trim();
            console.log(`[Simple Mermaid Test] Code for ${id}:`, code.substring(0, 100) + '...');
            
            try {
              // Render the diagram
              const { svg } = await window.mermaid.render(`${id}-render`, code);
              console.log(`[Simple Mermaid Test] SVG generated for ${id}`);
              
              // Replace the loading text with the SVG
              const container = diagram.querySelector('.mermaid-container');
              if (container) {
                container.innerHTML = svg;
                console.log(`[Simple Mermaid Test] SVG inserted for ${id}`);
              } else {
                console.error(`[Simple Mermaid Test] No container found for ${id}`);
              }
              
            } catch (renderError) {
              console.error(`[Simple Mermaid Test] Render error for ${id}:`, renderError);
              
              const container = diagram.querySelector('.mermaid-container');
              if (container) {
                container.innerHTML = `<div style="color: red; padding: 1rem;">Error: ${renderError.message}</div>`;
              }
            }
          } else {
            console.error(`[Simple Mermaid Test] No code element found for ${id}`);
          }
        }

        console.log('[Simple Mermaid Test] Completed processing all diagrams');
        
      } catch (error) {
        console.error('[Simple Mermaid Test] Fatal error:', error);
      }
    }

    // Run the test when DOM is ready
    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', loadAndTestMermaid);
    } else {
      loadAndTestMermaid();
    }

    // Export for manual testing
    window.testMermaid = loadAndTestMermaid;
  </script> </body> </html>  